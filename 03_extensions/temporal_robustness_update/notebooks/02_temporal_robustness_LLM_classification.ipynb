{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "To evaluate large language model (LLM) performance at scale, we leveraged batch inference APIs from all three model providers included in the experiments—OpenAI, Mistral, and Anthropic—across all design decision variations.\n",
    "For each provider, a systematic procedure was implemented to generate, upload, and execute batch requests for both base and fine-tuned models, adhering to the respective provider’s API documentation."
   ],
   "id": "730efb46c3f2200c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "5ad8f49c7051a0fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "from anthropic import Anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "e9da44f648cc492d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, and Variables",
   "id": "e8b57b9ff80eae6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "API_KEY_DIR = os.path.expanduser(os.getenv(\"API_KEY_DIR\", 'PATH')) # insert path to .txt with API Key\n",
    "PROMPT_DIR = os.path.join(DEMO_PATH, 'LLM_API', 'prompt_templates')\n",
    "LLM_API = os.path.join(DEMO_PATH,'LLM_API')\n",
    "VAL_PATH   = os.path.join(DEMO_PATH,'training_validation_data', 'demo_product_reviews_validation_real_1000.csv')\n",
    "\n",
    "OPENAI_BATCH_DIR = os.path.join(LLM_API, 'OpenAI_batches', 'raw')\n",
    "MISTRAL_BATCH_DIR = os.path.join(LLM_API, 'Mistral_batches', 'raw')\n",
    "ANTHROPIC_BATCH_DIR = os.path.join(LLM_API, 'Anthropic_batches')\n",
    "\n",
    "OPENAI_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'OpenAI_batches', '02_results')\n",
    "MISTRAL_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Mistral_batches', '02_results')\n",
    "ANTHROPIC_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Anthropic_batches', '02_results')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 94032\n",
    "\n",
    "# Max lines per batch (just redundancy, as batch per model size shouldn't exceed this)\n",
    "MAX_LINES_PER_BATCH = 50000\n",
    "\n",
    "# Load API keys\n",
    "with open(os.path.join(API_KEY_DIR, \"anthropic_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: ANTHROPIC_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"mistral_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: MISTRAL_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"openai_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: OPENAI_API_KEY = f.read().strip()\n",
    "\n",
    "# Define API-relevant URLs and clients\n",
    "MISTRAL_CLIENT =  Mistral(api_key=MISTRAL_API_KEY)\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "ANTHROPIC_CLIENT = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "### Models for API-requests\n",
    "# OpenAI GPT models (default set)\n",
    "GPT_MODELS = [\n",
    "    \"gpt-5.2\",\n",
    "    \"gpt-5-mini\",\n",
    "    \"gpt-5-nano\",\n",
    "]\n",
    "\n",
    "# Keep the variable around in case reasoning models / models that don't accept temperature input get updated\n",
    "GPT_REASONING_MODELS = set()\n",
    "\n",
    "# Fine-tuned OpenAI set removed - only possible until 4.1-series models\n",
    "\n",
    "# Mistral models (default set)\n",
    "MISTRAL_MODELS = [\n",
    "    \"mistral-large-2512\",\n",
    "]\n",
    "# Mistral models (fine-tuned set -> run 4-2 notebook before)\n",
    "MISTRAL_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"MISTRAL_fine-tuned.txt\")\n",
    "\n",
    "# Anthropic Claude models (default set)\n",
    "CLAUDE_MODELS = [\n",
    "    \"claude-opus-4-6\",\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    \"claude-haiku-4-5-20251001\",\n",
    "]   # note reasoning models can easily get very expensive\n",
    "###\n",
    "\n",
    "# Load prompt template\n",
    "DEFAULT_PROMPT_TEMPLATE = Path(PROMPT_DIR, \"reviews_prompt_default.txt\").read_text(encoding=\"utf-8\").strip()\n",
    "DEFAULT_PROMPT_NAME = \"default\"\n",
    "\n",
    "# Other settings\n",
    "DEFAULT_TEMPERATURE = 0.0\n",
    "GPT5_MINI_NANO_TEMPERATURE = 1.0   # setting 0 not possible\n",
    "REPEATED_RUNS = 3"
   ],
   "id": "f36a47c54a96be45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "df = pd.read_csv(VAL_PATH)"
   ],
   "id": "753b56a08691a1b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI Batches\n",
    "We generate JSONL batch files for **default** and **fine-tuned** GPT models, upload them, and create 24 h batch-jobs via the OpenAI API."
   ],
   "id": "8ccc17a5392d0567"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per GPT model. Run either Default oder Fine-Tuned Models."
   ],
   "id": "16aee42d8ac2836a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "a1b1bb78ff0c24a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in GPT_MODELS:\n",
    "    entries = []\n",
    "\n",
    "    # Decide the single temperature for this model\n",
    "    # (gpt-5-mini / gpt-5-nano only allow 0.5 per OpenAI API spec)\n",
    "    if model in {\"gpt-5-mini\", \"gpt-5-nano\"}:\n",
    "        temp = GPT5_MINI_NANO_TEMPERATURE\n",
    "    elif model in GPT_REASONING_MODELS:\n",
    "        temp = None  # reasoning models ignore temperature\n",
    "    else:\n",
    "        temp = DEFAULT_TEMPERATURE\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            # Compose user prompt\n",
    "            review_text = (row.get('review_text_tagged', '') or '').strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp review text: {review_text}\"\n",
    "            # Unique ID encodes model/prompt/temp/run/row.id\n",
    "            custom_id = (\n",
    "                f\"02__\"\n",
    "                f\"{model}__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int((temp or 0) * 10)}__run{run}__review_id{row['review_id']}\"\n",
    "            )\n",
    "            # Unique ID encodes model/prompt/temp/run/row.id\n",
    "            custom_id = (\n",
    "                f\"{model}__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int((temp or 0) * 10)}__run{run}__id{row['id']}\"\n",
    "            )\n",
    "\n",
    "            body = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_completion_tokens\": 1000,\n",
    "                \"seed\": SEED,\n",
    "            }\n",
    "\n",
    "            if temp is not None:\n",
    "                body[\"temperature\"] = temp\n",
    "            if model in GPT_REASONING_MODELS:\n",
    "                body[\"reasoning_effort\"] = \"low\"\n",
    "\n",
    "            entries.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": body,\n",
    "            })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        model_safe = model.replace(\"/\", \"-\")\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"02_openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(OPENAI_BATCH_DIR, batch_fname)\n",
    "\n",
    "        with open(batch_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {batch_path} (lines {start + 1}–{end})\")"
   ],
   "id": "c9640a9cd558565b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fine-Tuned Models\n",
    "Note: Before running this code, run notebook 3-2 to generate fine-tuned models via fine-tuning API."
   ],
   "id": "2e13173564d403ed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to OpenAI and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "2509eda64c727370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(OPENAI_BATCH_DIR, \"02_openai_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded_file_ids = []\n",
    "for path in batch_paths:\n",
    "    print(f\"Uploading {path}...\")\n",
    "    with open(path, 'rb') as f:\n",
    "        file_resp = OPENAI_CLIENT.files.create(\n",
    "            file=f,\n",
    "            purpose='batch'\n",
    "        )\n",
    "    print(f\"Uploaded: id={file_resp.id}, filename={file_resp.filename}, bytes={file_resp.bytes}\")\n",
    "    uploaded_file_ids.append(file_resp.id)\n",
    "\n",
    "print(\"All batch files uploaded.\\n\")"
   ],
   "id": "cb5a348208f56347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a 24 h chat-completion batch job for each uploaded file"
   ],
   "id": "ecbcf57f7ee3abd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Batch for each uploaded file\n",
    "for file_id in uploaded_file_ids:\n",
    "    batch = OPENAI_CLIENT.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"eval job for {file_id}\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created batch {batch.id} for {file_id}, status: {batch.status}\")"
   ],
   "id": "e38e61616ea89440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mistral Batches\n",
    "We generate JSONL batch files for default and fine-tuned Mistral models, upload them, and create batch-jobs via the Mistral API."
   ],
   "id": "a8a09d5b57a8bf43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per Mistral model. Run either Default or Fine-Tuned Models."
   ],
   "id": "ff0280ad83091447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "e94a56337b86fc78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in MISTRAL_MODELS:\n",
    "    entries = []\n",
    "\n",
    "    # Single temp for Mistral (=0)\n",
    "    temp = DEFAULT_TEMPERATURE\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            # Compose user prompt\n",
    "            review_text = (row.get('review_text_tagged', '') or '').strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp review text: {review_text}\"\n",
    "            # Unique ID encodes model/prompt/temp/run/row.id\n",
    "            custom_id = (\n",
    "                f\"{model}__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int((temp or 0) * 10)}__run{run}__review_id{row['review_id']}\"\n",
    "            )\n",
    "\n",
    "            body = {\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": temp,\n",
    "                \"random_seed\": SEED,\n",
    "            }\n",
    "\n",
    "            entries.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"body\": body,\n",
    "            })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"02_mistral_batch_{model.replace('/', '-')}_{suffix}.jsonl\"\n",
    "        path = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {path}\")"
   ],
   "id": "ca5970ed467acea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to Mistral and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "8be7d84213aeb213"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(MISTRAL_BATCH_DIR, \"02_mistral_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded = []\n",
    "for path in batch_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    model = fname.split(\"_\")[3]\n",
    "\n",
    "    print(f\"Uploading {fname} (model={model})…\")\n",
    "    with open(path, 'rb') as f:\n",
    "        up = MISTRAL_CLIENT.files.upload(\n",
    "            file={\"file_name\": fname, \"content\": f},\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    print(f\"Uploaded: id={up.id}\")\n",
    "    uploaded.append({\"model\": model, \"file_id\": up.id})\n",
    "\n",
    "print(\"All Mistral batch files uploaded.\\n\")"
   ],
   "id": "e3a1aec05ffaddbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a batch job for each uploaded file."
   ],
   "id": "3311b674c2bc7d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for item in uploaded:\n",
    "    file_id = item[\"file_id\"]\n",
    "    model = item[\"model\"]\n",
    "\n",
    "    # Parse model name for fine-tuned models\n",
    "    if model.startswith(\"ft-\"):\n",
    "        parts = model.split(\"-\") # parts = [\"ft\", \"mistral\", \"small\", \"latest\", \"d1ef7e20\", \"20250527\", \"314de464\", \"equal_500\"]\n",
    "\n",
    "        prefix     = parts[0]                        # \"ft\"\n",
    "        model_name = \"-\".join(parts[1:4])            # \"mistral-small-latest\"\n",
    "        rev        = parts[4]                        # \"d1ef7e20\"\n",
    "        date       = parts[5]                        # \"20250527\"\n",
    "        sha        = parts[6]                        # \"314de464\"\n",
    "\n",
    "        real_model = f\"{prefix}:{model_name}:{rev}:{date}:{sha}\"\n",
    "        # \"ft:mistral-small-latest:d1ef7e20:20250527:314de464\"\n",
    "    else:\n",
    "        real_model = model\n",
    "\n",
    "    # Submit batch job to Mistral API\n",
    "    job = MISTRAL_CLIENT.batch.jobs.create(\n",
    "        input_files=[file_id],\n",
    "        model=real_model,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        metadata={\"description\": f\"classification batch for {real_model}\"}\n",
    "    )\n",
    "    print(f\"Created batch job {job.id} for file {file_id} (model={real_model}), status={job.status}\")"
   ],
   "id": "57d40fa23299dfb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Anthropic Batches\n",
    "We create batch requests for Claude models and process JSONL results into CSV format."
   ],
   "id": "c434387bb42d2e80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Requests\n",
    "We build batch requests for each Claude model with various prompts and temperature settings."
   ],
   "id": "a37637b387b43779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shorten model names as custom_id max is 64 characters\n",
    "def short_model_name(full_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts full Claude model names to shortened versions for custom_id usage.\n",
    "    # e.g. \"claude-3-7-sonnet-20250219\" → [\"claude\",\"3\",\"7\",\"sonnet\",\"20250219\"]\n",
    "    \"\"\"\n",
    "    parts = full_model.split(\"-\")\n",
    "    # Drop the first (\"claude\") and last (date) parts, then re-join\n",
    "    return \"-\".join(parts[1:-1])"
   ],
   "id": "72513c6c6f273617",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for model in CLAUDE_MODELS:\n",
    "    short_model = short_model_name(model)\n",
    "    batch_requests = []\n",
    "\n",
    "    temp = DEFAULT_TEMPERATURE  # single temp\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            # Compose user prompt\n",
    "            review_text = (row.get(\"review_text_tagged\", \"\") or \"\").strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp review text: {review_text}\"\n",
    "\n",
    "            # Unique ID encodes model/prompt/temp/run/row.id\n",
    "            raw_id = (\n",
    "                f\"{short_model}_{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"_t{int((temp or 0) * 10)}_r{run}_review_i{row['review_id']}\"\n",
    "            )\n",
    "            custom_id = raw_id[:64] if len(raw_id) <= 64 else raw_id[:50]\n",
    "\n",
    "            params = MessageCreateParamsNonStreaming(\n",
    "                model=model,\n",
    "                max_tokens=1000,\n",
    "                temperature=temp,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            batch_requests.append(Request(custom_id=custom_id, params=params))\n",
    "\n",
    "    message_batch = ANTHROPIC_CLIENT.messages.batches.create(requests=batch_requests)\n",
    "    print(\n",
    "        f\"Created batch for {model} with {len(batch_requests)} requests: \"\n",
    "        f\"{message_batch.id} (status={message_batch.processing_status})\"\n",
    "    )"
   ],
   "id": "d7f0f8b8b516af0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process results\n",
    "We process JSONL result files and convert them to CSV format for analysis. Note: Download batch files from each provider's developer platform before."
   ],
   "id": "9057656827634055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation dataset and prepare ID columns for matching with batch results\n",
    "df = pd.read_csv(VAL_PATH, dtype={'review_id': str})\n",
    "df['review_id'] = df['review_id'].astype(str)\n",
    "df['id'] = df['id'].astype(str)"
   ],
   "id": "80455e4a9ccf4fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper Functions\n",
    "Utility functions for ID normalization and record parsing across different LLM providers."
   ],
   "id": "ca0fe059b14033f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper: normalize ID string by removing 'id' prefix",
   "id": "6fad967e16c4e168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_id_str(id_part: str) -> str:\n",
    "    \"\"\"Normalize id strings from custom_id into raw numeric/string ids.\"\"\"\n",
    "    if id_part is None:\n",
    "        return \"\"\n",
    "    s = str(id_part).strip()\n",
    "\n",
    "    # Remove common prefixes at the *start* only\n",
    "    s = re.sub(r'^(review[_-]?id|reviewid|review_|id)\\s*', '', s, flags=re.IGNORECASE)\n",
    "\n",
    "    # Keep only remaining trimmed value\n",
    "    return s.strip()"
   ],
   "id": "8bf0319443a1dd5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Provider-Specific Parsers\n",
    "Each LLM provider has different JSONL response formats requiring specialized parsing."
   ],
   "id": "6eaaeebae4d8ed55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_openai_record(record):\n",
    "    \"\"\"Parse OpenAI batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    custom_id = record.get('custom_id', '')\n",
    "    parts     = custom_id.split('__')\n",
    "\n",
    "    if custom_id.startswith(\"02__\"):\n",
    "        if len(parts) < 6:\n",
    "            return None\n",
    "        model, prompt_type, temp, run, id_part = parts[-5:]\n",
    "    else:\n",
    "        if len(parts) < 5:\n",
    "            return None\n",
    "        model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "    orig_id = record.get('id', '')\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, orig_id, col, content\n",
    "\n",
    "def parse_mistral_record(record):\n",
    "    \"\"\"Parse Mistral batch response record\"\"\"\n",
    "    # Same as parse_openai_record\n",
    "    return parse_openai_record(record)\n",
    "\n",
    "def parse_anthropic_record(record):\n",
    "    \"\"\"Parse Anthropic batch response record\"\"\"\n",
    "    # Split custom_id into components (single underscore separator)\n",
    "    custom_id = record.get('custom_id', '')\n",
    "    parts     = custom_id.split('_')\n",
    "\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "\n",
    "    # Extract components from different positions due to Anthropic format\n",
    "    id_part = parts[-1]\n",
    "    run = parts[-2]\n",
    "    temp = parts[-3]\n",
    "    model = parts[0]\n",
    "    prompt_type = '_'.join(parts[1:-3])\n",
    "    id_str = normalize_id_str(id_part.lstrip('review_i'))\n",
    "    orig_id = record.get('id', '')\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    content = ''\n",
    "    try:\n",
    "        content = record.get('result', {}).get('message', {})['content'][0]['text']\n",
    "    except Exception:\n",
    "        # If content missing or malformed, leave empty\n",
    "        pass\n",
    "    return id_str, orig_id, col, content\n",
    "\n",
    "def process_file(path, parser):\n",
    "    \"\"\"Process a single JSONL file using the specified parser\"\"\"\n",
    "    mapping = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed JSON lines\n",
    "                continue\n",
    "            # Parse record using provider-specific parser\n",
    "            parsed = parser(rec)\n",
    "            if parsed:\n",
    "                id_str, orig_id, col, content = parsed\n",
    "                if col not in mapping:\n",
    "                    mapping[col] = {}\n",
    "                # Group by column, then by ID\n",
    "                mapping[col][id_str] = (content, orig_id)\n",
    "    return mapping\n",
    "\n",
    "def process_results_parallel(directory, parser, df, workers=8):\n",
    "    \"\"\"Process all JSONL files in a directory using parallel execution.\"\"\"\n",
    "    # Find all JSONL files in directory and subdirectories\n",
    "    files = list(Path(directory).rglob('*.jsonl'))\n",
    "\n",
    "    combined = {}\n",
    "\n",
    "    # Process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all file processing tasks\n",
    "        futures = {executor.submit(process_file, f, parser): f for f in files}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            file_map = future.result()\n",
    "             # Merge results from each file\n",
    "            for col, id_map in file_map.items():\n",
    "                if col not in combined:\n",
    "                    combined[col] = {}\n",
    "                combined[col].update(id_map)\n",
    "\n",
    "    # Add new columns to dataframe\n",
    "    for col, id_map in combined.items():\n",
    "        def lookup(row):\n",
    "            row_id = str(row['id']).strip()\n",
    "            row_review_id = str(row['review_id']).strip()\n",
    "            # For regular: try review_id, then id\n",
    "            if row_review_id in id_map:\n",
    "                return id_map[row_review_id][0]\n",
    "            if row_id in id_map:\n",
    "                return id_map[row_id][0]\n",
    "            return ''\n",
    "        df[col] = df.apply(lookup, axis=1)\n",
    "    return df.copy()\n"
   ],
   "id": "281215772d4ea7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process All Provider Results\n",
    "Execute processing for each LLM provider in sequence."
   ],
   "id": "6a9cfc7b5d681508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = process_results_parallel(OPENAI_BATCH_RESULTS_DIR, parse_openai_record, df)\n",
    "df = process_results_parallel(MISTRAL_BATCH_RESULTS_DIR, parse_mistral_record, df)\n",
    "df = process_results_parallel(ANTHROPIC_BATCH_RESULTS_DIR, parse_anthropic_record, df)"
   ],
   "id": "b4d05371150991d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save final output\n",
    "df.to_csv(os.path.join(DEMO_PATH, 'output_data','02_validation_with_model_preds_LLM_temp.csv'), index=False)"
   ],
   "id": "b02481924e211f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean Results\n",
    "Standardize raw model outputs into consistent multi-label strings.\n",
    "This includes string cleanup, numeric type handling, and fallback for invalid predictions."
   ],
   "id": "6f85736880b4e3f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Prep and Helper Function",
   "id": "527b0af345bbff8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Columns to Exclude from Cleaning\n",
    "EXCLUDE_COLS = [\n",
    "    'review_id', 'user_id', 'title', 'body', 'label', 'id', 'app_id',\n",
    "    'review_text_plain', 'review_text_tagged', 'split_labels','sorted_labels','multi_hot'\n",
    "]\n",
    "\n",
    "def extract_pred_group(text):\n",
    "    \"\"\"\n",
    "    Normalize prediction output into semicolon-separated label string.\n",
    "    Returns '9' if no valid label is found (fallback class).\n",
    "\n",
    "    - Accepts int, float, or string input.\n",
    "    - Parses predictions like '2;5', '(3 ; 6)', etc.\n",
    "    - Ignores non-digit content or out-of-range values (labels ∈ [0, 8]).\n",
    "    \"\"\"\n",
    "    # Handle numeric types first\n",
    "    if isinstance(text, float):\n",
    "        # If it's NaN, treat as no valid label\n",
    "        if pd.isna(text):\n",
    "            return '9'\n",
    "        # If it's a float that represents an int (e.g. 2.0), convert to int string\n",
    "        if text.is_integer() and 0 <= int(text) <= 8:\n",
    "            return str(int(text))\n",
    "        # If not in range or not an int, treat as no valid label\n",
    "        return '9'\n",
    "    if isinstance(text, int):\n",
    "        if 0 <= text <= 8:\n",
    "            return str(text)\n",
    "        return '9'\n",
    "\n",
    "    # Handle string or other types\n",
    "    text = str(text)\n",
    "    pattern = r'\\(?\\s*([0-8](?:\\s*;\\s*[0-8])*)\\s*\\)?'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return '9'\n",
    "    last = matches[-1]\n",
    "    cleaned_digits = [d for d in re.split(r'\\s*;\\s*', last.strip('; ')) if d.isdigit() and 0 <= int(d) <= 8]\n",
    "    if not cleaned_digits:\n",
    "        return '9'\n",
    "    return ';'.join(cleaned_digits)"
   ],
   "id": "b97981e48e91f1af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model Output File and Apply Cleaning",
   "id": "38397bb02fa7a2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(DEMO_PATH, 'output_data', '02_validation_with_model_preds_LLM_temp.csv'))"
   ],
   "id": "2dc67f8ad631928b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify prediction columns (exclude metadata & raw input)\n",
    "model_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "# Apply standardization function to all model columns\n",
    "for col in model_cols:\n",
    "    df[col] = df[col].apply(extract_pred_group)\n",
    "\n",
    "# Save cleaned results\n",
    "save_path = os.path.join(DEMO_PATH, 'output_data','02_validation_with_model_preds_LLM_cleaned.csv')\n",
    "df.to_csv(save_path, index=False)\n",
    "print(f\"Saved cleaned file to {save_path}\")"
   ],
   "id": "53f441aec320e11",
   "outputs": [],
   "execution_count": null
  }
   ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
