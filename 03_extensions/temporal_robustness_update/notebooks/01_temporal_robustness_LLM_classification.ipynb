{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "We adjusted the `4-1 notebook` to support small-scale batch API inference for selected frontier models in the default setup.\n",
    "For each provider, a systematic procedure was implemented to generate, upload, and execute batch requests, adhering to the respective provider’s API documentation.\n",
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "29612dd5ae6b9f8b"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "from anthropic import Anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, and Variables",
   "id": "6022b2e716b6f9e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "API_KEY_DIR = os.path.expanduser(os.getenv(\"API_KEY_DIR\", 'F:\\RobinNO\\API_keys')) # insert path to .txt with API Key\n",
    "PROMPT_DIR = os.path.join(DEMO_PATH, 'LLM_API', 'prompt_templates')\n",
    "LLM_API = os.path.join(DEMO_PATH,'LLM_API')\n",
    "VAL_PATH = os.path.join(DEMO_PATH, 'training_validation_data', 'demo_app_updates_validation_real_1000.csv')\n",
    "\n",
    "OPENAI_BATCH_DIR = os.path.join(LLM_API, 'OpenAI_batches', 'raw')\n",
    "MISTRAL_BATCH_DIR = os.path.join(LLM_API, 'Mistral_batches', 'raw')\n",
    "ANTHROPIC_BATCH_DIR = os.path.join(LLM_API, 'Anthropic_batches')\n",
    "\n",
    "OPENAI_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'OpenAI_batches', '01_results')\n",
    "MISTRAL_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Mistral_batches', '01_results')\n",
    "ANTHROPIC_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Anthropic_batches', '01_results')\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 94032\n",
    "\n",
    "# Max lines per batch (just redundancy, as batch per model size shouldn't exceed this)\n",
    "MAX_LINES_PER_BATCH = 50000\n",
    "\n",
    "# Load API keys\n",
    "with open(os.path.join(API_KEY_DIR, \"anthropic_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: ANTHROPIC_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"mistral_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: MISTRAL_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"openai_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: OPENAI_API_KEY = f.read().strip()\n",
    "\n",
    "# Define API-relevant URLs and clients\n",
    "MISTRAL_CLIENT =  Mistral(api_key=MISTRAL_API_KEY)\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "ANTHROPIC_CLIENT = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "### Models for API-requests\n",
    "# OpenAI GPT models (default set)\n",
    "GPT_MODELS = [\n",
    "    \"gpt-5.2\",\n",
    "    \"gpt-5-mini\",\n",
    "    \"gpt-5-nano\",\n",
    "]\n",
    "\n",
    "# Keep the variable around in case reasoning models / models that don't accept temperature input get updated\n",
    "GPT_REASONING_MODELS = set()\n",
    "\n",
    "# Fine-tuned OpenAI set removed - only possible until 4.1-series models\n",
    "\n",
    "# Mistral models (default set)\n",
    "MISTRAL_MODELS = [\n",
    "    \"mistral-large-2512\",\n",
    "]\n",
    "# Mistral models (fine-tuned set -> run 4-2 notebook before)\n",
    "MISTRAL_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"MISTRAL_fine-tuned.txt\")\n",
    "\n",
    "# Anthropic Claude models (default set)\n",
    "CLAUDE_MODELS = [\n",
    "    \"claude-opus-4-6\",\n",
    "    \"claude-sonnet-4-5-20250929\",\n",
    "    \"claude-haiku-4-5-20251001\",\n",
    "]   # note reasoning models can easily get very expensive\n",
    "###\n",
    "\n",
    "# Load prompt template\n",
    "DEFAULT_PROMPT_TEMPLATE = Path(PROMPT_DIR, \"updates_prompt_default.txt\").read_text(encoding=\"utf-8\").strip()\n",
    "DEFAULT_PROMPT_NAME = \"default\"\n",
    "\n",
    "# Other settings\n",
    "DEFAULT_TEMPERATURE = 0.0\n",
    "GPT5_MINI_NANO_TEMPERATURE = 1.0   # setting 0 not possible\n",
    "REPEATED_RUNS = 3"
   ],
   "id": "5acd7154f6db21b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "df = pd.read_csv(VAL_PATH)"
   ],
   "id": "e73a4a2627e5b113",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI Batches\n",
    "We generate JSONL batch files for **default** and **fine-tuned** GPT models, upload them, and create 24 h batch-jobs via the OpenAI API.\n",
    "### Batch Creation\n",
    "We build one JSONL batch per GPT model. Run either Default oder Fine-Tuned Models.\n",
    "#### Default Models"
   ],
   "id": "a7fbdeacd3d458bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in GPT_MODELS:\n",
    "    entries = []\n",
    "\n",
    "    # Decide the single temperature for this model\n",
    "    # (gpt-5-mini / gpt-5-nano only allow 0.5 per OpenAI API spec)\n",
    "    if model in {\"gpt-5-mini\", \"gpt-5-nano\"}:\n",
    "        temp = GPT5_MINI_NANO_TEMPERATURE\n",
    "    elif model in GPT_REASONING_MODELS:\n",
    "        temp = None  # reasoning models ignore temperature\n",
    "    else:\n",
    "        temp = DEFAULT_TEMPERATURE\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp update text: {update_text}\"\n",
    "\n",
    "            # Unique ID encodes model/prompt/temp/run/row.id\n",
    "            custom_id = (\n",
    "                f\"{model}__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int((temp or 0) * 10)}__run{run}__id{row['id']}\"\n",
    "            )\n",
    "\n",
    "            body = {\n",
    "                \"model\": model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_completion_tokens\": 1000,\n",
    "                \"seed\": SEED,\n",
    "            }\n",
    "\n",
    "            if temp is not None:\n",
    "                body[\"temperature\"] = temp\n",
    "            if model in GPT_REASONING_MODELS:\n",
    "                body[\"reasoning_effort\"] = \"low\"\n",
    "\n",
    "            entries.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": body,\n",
    "            })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        model_safe = model.replace(\"/\", \"-\")\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"01_openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(OPENAI_BATCH_DIR, batch_fname)\n",
    "\n",
    "        with open(batch_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {batch_path} (lines {start + 1}–{end})\")"
   ],
   "id": "accfbea090d74ff6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to OpenAI and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "f23933f92472b549"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(OPENAI_BATCH_DIR, \"01_openai_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded_file_ids = []\n",
    "for path in batch_paths:\n",
    "    print(f\"Uploading {path}...\")\n",
    "    with open(path, 'rb') as f:\n",
    "        file_resp = OPENAI_CLIENT.files.create(\n",
    "            file=f,\n",
    "            purpose='batch'\n",
    "        )\n",
    "    print(f\"Uploaded: id={file_resp.id}, filename={file_resp.filename}, bytes={file_resp.bytes}\")\n",
    "    uploaded_file_ids.append(file_resp.id)\n",
    "\n",
    "print(\"All batch files uploaded.\\n\")"
   ],
   "id": "84ee28c5ce0ccf8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a 24 h chat-completion batch job for each uploaded file"
   ],
   "id": "a2b6945d0dd786cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Batch for each uploaded file\n",
    "for file_id in uploaded_file_ids:\n",
    "    batch = OPENAI_CLIENT.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"eval job for {file_id}\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created batch {batch.id} for {file_id}, status: {batch.status}\")"
   ],
   "id": "3771251132180297",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mistral Batches\n",
    "We generate JSONL batch files for default and fine-tuned Mistral models, upload them, and create batch-jobs via the Mistral API."
   ],
   "id": "377f588d6036521"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per Mistral model. Run either Default or Fine-Tuned Models."
   ],
   "id": "f4f7b2d5e6c5c9e3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "642377b903d672c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for model in MISTRAL_MODELS:\n",
    "    entries = []\n",
    "\n",
    "    # Single temp for Mistral (=0)\n",
    "    temp = DEFAULT_TEMPERATURE\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp update text: {update_text}\"\n",
    "\n",
    "            # Unique ID encodes model/prompt/temp/run/row.id\n",
    "            custom_id = (\n",
    "                f\"{model}__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int((temp or 0) * 10)}__run{run}__id{row['id']}\"\n",
    "            )\n",
    "\n",
    "            body = {\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"temperature\": temp,\n",
    "                \"random_seed\": SEED,\n",
    "            }\n",
    "\n",
    "            entries.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"body\": body,\n",
    "            })\n",
    "\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"mistral_batch_{model.replace('/', '-')}_{suffix}.jsonl\"\n",
    "        path = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {path}\")\n"
   ],
   "id": "ba81e1e6b84c1f25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to Mistral and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "15767324c00c5acf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(MISTRAL_BATCH_DIR, \"01_mistral_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded = []\n",
    "for path in batch_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    model = fname.split(\"_\")[2]\n",
    "\n",
    "    print(f\"Uploading {fname} (model={model})…\")\n",
    "    with open(path, 'rb') as f:\n",
    "        up = MISTRAL_CLIENT.files.upload(\n",
    "            file={\"file_name\": fname, \"content\": f},\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    print(f\"Uploaded: id={up.id}\")\n",
    "    uploaded.append({\"model\": model, \"file_id\": up.id})\n",
    "\n",
    "print(\"All Mistral batch files uploaded.\\n\")"
   ],
   "id": "326783afae537407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a batch job for each uploaded file."
   ],
   "id": "547941308fc94090"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a batch job for each uploaded file\n",
    "for item in uploaded:\n",
    "    file_id = item[\"file_id\"]\n",
    "    model = item[\"model\"]\n",
    "    # Parse model name for fine-tuned models\n",
    "    if model.startswith(\"ft-\"):\n",
    "        parts = model.split(\"-\")\n",
    "        # parts = [\"ft\",\"mistral\",\"small\",\"latest\",\"d1ef7e20\",\"20250527\",\"314de464\",\"equal_500\"]\n",
    "\n",
    "        prefix = parts[0]\n",
    "        model_name = \"-\".join(parts[1:4])   # \"mistral-small-latest\"\n",
    "        rev = parts[4]              # \"d1ef7e20\"\n",
    "        date = parts[5]              # \"20250527\"\n",
    "        sha = parts[6]              # \"314de464\"\n",
    "\n",
    "        real_model = f\"{prefix}:{model_name}:{rev}:{date}:{sha}\"\n",
    "        # \"ft:mistral-small-latest:d1ef7e20:20250527:314de464\"\n",
    "    else:\n",
    "        real_model = model\n",
    "\n",
    "    # Submit batch job to Mistral API\n",
    "    job = MISTRAL_CLIENT.batch.jobs.create(\n",
    "        input_files=[file_id],\n",
    "        model=real_model,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        metadata={\"description\": f\"classification batch for {real_model}\"}\n",
    "    )\n",
    "    print(f\"Created batch job {job.id} for file {file_id} (model={real_model}), status={job.status}\")"
   ],
   "id": "c63d52da208fdf3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Anthropic Batches\n",
    "We create batch requests for Claude models and process JSONL results into CSV format."
   ],
   "id": "355319d27cbb104c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Requests\n",
    "We build batch requests for each Claude model with various prompts and temperature settings."
   ],
   "id": "6c6edec183ab5fce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shorten model names as custom_id max is 64 characters\n",
    "def short_model_name(full_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts full Claude model names to shortened versions for custom_id usage.\n",
    "    # e.g. \"claude-3-7-sonnet-20250219\" → [\"claude\",\"3\",\"7\",\"sonnet\",\"20250219\"]\n",
    "    \"\"\"\n",
    "    parts = full_model.split(\"-\")\n",
    "    # Drop the first (\"claude\") and last (date) parts, then re-join\n",
    "    return \"-\".join(parts[1:-1])"
   ],
   "id": "c8303a3bb822d236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for model in CLAUDE_MODELS:\n",
    "    short_model = short_model_name(model)\n",
    "    batch_requests = []\n",
    "\n",
    "    temp = DEFAULT_TEMPERATURE  # single temp\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp update text: {update_text}\"\n",
    "\n",
    "            raw_id = (\n",
    "                f\"{short_model}_{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"_t{int((temp or 0) * 10)}_r{run}_i{row['id']}\"\n",
    "            )\n",
    "            custom_id = raw_id[:64] if len(raw_id) <= 64 else raw_id[:50]\n",
    "\n",
    "            params = MessageCreateParamsNonStreaming(\n",
    "                model=model,\n",
    "                max_tokens=1000,\n",
    "                temperature=temp,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            )\n",
    "            batch_requests.append(Request(custom_id=custom_id, params=params))\n",
    "\n",
    "    message_batch = ANTHROPIC_CLIENT.messages.batches.create(requests=batch_requests)\n",
    "    print(\n",
    "        f\"Created batch for {model} with {len(batch_requests)} requests: \"\n",
    "        f\"{message_batch.id} (status={message_batch.processing_status})\"\n",
    "    )"
   ],
   "id": "ce908a6437753746",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process results\n",
    "We process JSONL result files and convert them to CSV format for analysis. Note: Download batch files from each provider's developer platform before."
   ],
   "id": "31e6bc00681ddd7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation dataset and prepare ID columns for matching with batch results\n",
    "df = pd.read_csv(VAL_PATH, dtype={'id': str})\n",
    "df['id'] = df['id'].str.strip()"
   ],
   "id": "449b6d6b87e1c1b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper Functions\n",
    "Utility functions for ID normalization and record parsing across different LLM providers."
   ],
   "id": "8ecd62d96b8d014d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper: normalize ID string by removing 'id' prefix",
   "id": "1cd57a6cdb556606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_id_str(id_part):\n",
    "    \"\"\"Clean ID strings by removing 'id' prefix and whitespace\"\"\"\n",
    "    return id_part.replace('id', '').strip()"
   ],
   "id": "b0204688ba699b7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Provider-Specific Parsers\n",
    "Each LLM provider has different JSONL response formats requiring specialized parsing."
   ],
   "id": "644b1bb429f5abf1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_openai_record(record):\n",
    "    \"\"\"Parse OpenAI batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    parts = record.get('custom_id', '').split('__')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, col, content\n",
    "\n",
    "def parse_mistral_record(record):\n",
    "    \"\"\"Parse Mistral batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    parts = record.get('custom_id', '').split('__')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, col, content\n",
    "\n",
    "def parse_anthropic_record(record):\n",
    "    \"\"\"Parse Anthropic batch response record\"\"\"\n",
    "    # Split custom_id into components (single underscore separator)\n",
    "    parts = record.get('custom_id', '').split('_')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "\n",
    "    # Extract components from different positions due to Anthropic format\n",
    "    id_part = parts[-1]\n",
    "    run = parts[-2]\n",
    "    temp = parts[-3]\n",
    "    model = parts[0]\n",
    "    prompt_type = '_'.join(parts[1:-3])\n",
    "    id_str = normalize_id_str(id_part.lstrip('i'))\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    msg = record.get('result', {}).get('message', {})\n",
    "    content = ''\n",
    "    try:\n",
    "        content = msg['content'][0]['text']\n",
    "    except Exception:\n",
    "        # If content missing or malformed, leave empty\n",
    "        pass\n",
    "    return id_str, col, content"
   ],
   "id": "be12b5ef2d495926",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### File Processing Functions\n",
    "Core functions for processing JSONL files and parallel execution."
   ],
   "id": "b003508badef14c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_file(path, parser):\n",
    "    \"\"\"Process a single JSONL file using the specified parser\"\"\"\n",
    "    mapping = {}\n",
    "    print(f\"Processing {path.name}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed JSON lines\n",
    "                continue\n",
    "            # Parse record using provider-specific parser\n",
    "            parsed = parser(rec)\n",
    "            if parsed:\n",
    "                id_str, col, content = parsed\n",
    "                # Group by column, then by ID\n",
    "                mapping.setdefault(col, {})[id_str] = content\n",
    "\n",
    "    print(f\"  Mapped {len(mapping)} columns from {path.name}\")\n",
    "    return mapping\n",
    "\n",
    "def process_results_parallel(directory, parser, df, workers=10):\n",
    "    \"\"\"Process all JSONL files in a directory using parallel execution.\"\"\"\n",
    "    # Find all JSONL files in directory and subdirectories\n",
    "    files = list(Path(directory).rglob('*.jsonl'))\n",
    "    print(f\"Found {len(files)} files in {directory}\")\n",
    "\n",
    "    combined = {}\n",
    "\n",
    "    # Process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all file processing tasks\n",
    "        futures = {executor.submit(process_file, f, parser): f for f in files}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            file_map = future.result()\n",
    "            # Merge results from each file\n",
    "            for col, id_map in file_map.items():\n",
    "                combined.setdefault(col, {}).update(id_map)\n",
    "\n",
    "    # Add new columns to dataframe\n",
    "    for col, id_map in combined.items():\n",
    "        df[col] = df['id'].map(id_map)\n",
    "\n",
    "    return df.copy()"
   ],
   "id": "ed3af2e531e229d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process All Provider Results\n",
    "Execute processing for each LLM provider in sequence."
   ],
   "id": "4f6fc2879f900ebe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = process_results_parallel(OPENAI_BATCH_RESULTS_DIR, parse_openai_record, df)\n",
    "df = process_results_parallel(MISTRAL_BATCH_RESULTS_DIR, parse_mistral_record, df)\n",
    "df = process_results_parallel(ANTHROPIC_BATCH_RESULTS_DIR, parse_anthropic_record, df)"
   ],
   "id": "d34a382cf0732daa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save final output\n",
    "df.to_csv(os.path.join(DEMO_PATH, 'output_data', '01_validation_with_model_preds_LLM_temp.csv'), index=False)"
   ],
   "id": "aafe4bc39540500e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean Results\n",
    "Standardize raw model outputs into valid single-label strings (1–7).\n",
    "Cleans and validates outputs from LLMs or API responses, ensuring only acceptable class labels are retained."
   ],
   "id": "a366960a9c972d70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Prep and Helper Function",
   "id": "862ad772a6831445"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Columns to exclude from label cleaning\n",
    "EXCLUDE_COLS = [\n",
    "    'release_date', 'version_display', 'whats_new', 'body', 'update_classification', 'id', 'app_id', 'previous_version', 'previous_release_date', 'previous_id'\n",
    "]\n",
    "\n",
    "def extract_single_label(text):\n",
    "    \"\"\"\n",
    "    Normalize prediction output into a single class label string (1–7).\n",
    "    Returns '0' as a fallback for invalid predictions.\n",
    "\n",
    "    Handles:\n",
    "    - Integers or floats like 3, 3.0\n",
    "    - Strings like '3', ' (3) ', or even '3 ; 4' (keeps only last valid single label)\n",
    "    \"\"\"\n",
    "    if isinstance(text, float):\n",
    "        if pd.isna(text):\n",
    "            return '0'  # fallback\n",
    "        if text.is_integer() and 1 <= int(text) <= 7:\n",
    "            return str(int(text))\n",
    "        return '0'\n",
    "\n",
    "    if isinstance(text, int):\n",
    "        return str(text) if 1 <= text <= 7 else '0'\n",
    "\n",
    "    # Parse strings and try to extract digits in range 1–7\n",
    "    text = str(text)\n",
    "    pattern = r'\\(?\\s*([1-7](?:\\s*;\\s*[1-7])*)\\s*\\)?'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return '0'\n",
    "    last = matches[-1]\n",
    "    cleaned = [d for d in re.split(r'\\s*;\\s*', last.strip('; ')) if d.isdigit() and 1 <= int(d) <= 7]\n",
    "    return cleaned[-1] if cleaned else '0'"
   ],
   "id": "e7f1e314881baee1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model Output File and Apply Cleaning",
   "id": "e7b09be28b8ad32e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(DEMO_PATH, 'output_data', '01_validation_with_model_preds_LLM_temp.csv'))"
   ],
   "id": "4c3ea713289ded75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify prediction columns (exclude inputs/metadata)\n",
    "model_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "# Apply label cleaning function to each model column\n",
    "for col in model_cols:\n",
    "    df[col] = df[col].apply(extract_single_label)\n",
    "\n",
    "# Save cleaned predictions to file\n",
    "save_path = os.path.join(DEMO_PATH, 'output_data', '01_validation_with_model_preds_LLM_temp_cleaned.csv')\n",
    "df.to_csv(save_path, index=False)"
   ],
   "id": "402d2047a8a069d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.columns",
   "id": "76076b2dc92d2959",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
