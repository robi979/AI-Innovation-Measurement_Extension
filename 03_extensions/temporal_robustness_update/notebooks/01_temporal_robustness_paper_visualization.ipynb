{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "In this notebook, we compute the same evaluation metrics as in the demonstration studies for a small set of LLM prediction columns (frontier models) and create Table F9 for the Online Appendix."
   ],
   "id": "891156cf192acc87"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "884d8f7323ef5718"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score, precision_score, recall_score, accuracy_score, cohen_kappa_score\n",
    ")\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "39cbe3c7f1e218d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Paths",
   "id": "670abcc651ad968a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DEMO_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "OUT_DIR   = os.path.join(DEMO_PATH, \"output_data\")\n",
    "LLM_PATH  = os.path.join(OUT_DIR, \"01_validation_with_model_preds_LLM_temp_cleaned.csv\")\n",
    "VISUAL_DIR  = os.path.join(DEMO_PATH, \"paper_visuals\")"
   ],
   "id": "9a33cf87ef489930",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "712d38fabe32fd4b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n",
    "In this section, we define regular expressions and helper functions for parsing model output columns. This includes extracting metadata standardizing column names and processing LLM results. These routines are used to prepare the results for Table F9 for the Online Appendix."
   ],
   "id": "b60d565a799a1f0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRUTH_COL  = \"update_classification\"\n",
    "LABELS_1_7 = list(range(1, 8))\n",
    "\n",
    "RUN_COL_RE = re.compile(r\"^(?P<prefix>.+?)__(?:run|r)(?P<run>\\d+)$\")"
   ],
   "id": "33159035eb8870df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def innovation_dummy(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Innovation = labels 1 or 2 (<=2).\"\"\"\n",
    "    s = s.astype(\"Int64\")\n",
    "    return (s <= 2).astype(\"Int64\")\n",
    "\n",
    "\n",
    "def compute_binary_metrics(y_true: pd.Series, y_pred: pd.Series) -> dict:\n",
    "    \"\"\"Binary metrics: F1, precision, recall, accuracy.\"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    return dict(\n",
    "        f1=f1_score(y_true, y_pred, average=\"binary\", zero_division=0),\n",
    "        precision=precision_score(y_true, y_pred, average=\"binary\", zero_division=0),\n",
    "        recall=recall_score(y_true, y_pred, average=\"binary\", zero_division=0),\n",
    "        accuracy=accuracy_score(y_true, y_pred),\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_multiclass_metrics(y_true: pd.Series, y_pred: pd.Series, labels=LABELS_1_7) -> dict:\n",
    "    \"\"\"\n",
    "    Multiclass metrics:\n",
    "    - per-class F1: f1_1..f1_7\n",
    "    - macro F1\n",
    "    - weighted F1\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "\n",
    "    per_class = f1_score(y_true, y_pred, labels=labels, average=None, zero_division=0)\n",
    "    out = {f\"f1_{lab}\": float(per_class[i]) for i, lab in enumerate(labels)}\n",
    "    out[\"f1_macro\"] = float(f1_score(y_true, y_pred, labels=labels, average=\"macro\", zero_division=0))\n",
    "    out[\"f1_weighted\"] = float(f1_score(y_true, y_pred, labels=labels, average=\"weighted\", zero_division=0))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _consistency_rate(preds_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Fraction of items where all runs agree exactly.\"\"\"\n",
    "    if preds_df.shape[1] < 2 or preds_df.shape[0] == 0:\n",
    "        return np.nan\n",
    "    return float((preds_df.nunique(axis=1) == 1).mean())\n",
    "\n",
    "\n",
    "def _mean_cohen_kappa(preds_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Mean pairwise Cohen's kappa across runs.\"\"\"\n",
    "    cols = list(preds_df.columns)\n",
    "    if len(cols) < 2 or preds_df.shape[0] == 0:\n",
    "        return np.nan\n",
    "    ks = []\n",
    "    for i in range(len(cols)):\n",
    "        for j in range(i + 1, len(cols)):\n",
    "            ks.append(cohen_kappa_score(preds_df[cols[i]], preds_df[cols[j]]))\n",
    "    return float(np.mean(ks)) if ks else np.nan\n",
    "\n",
    "\n",
    "def _fleiss_from_preds(preds_df: pd.DataFrame, labels: list) -> float:\n",
    "    \"\"\"\n",
    "    Fleiss' kappa:\n",
    "    Build counts per item of how many runs chose each label.\n",
    "    \"\"\"\n",
    "    arr = preds_df.to_numpy()\n",
    "    countmat = np.array([[(row == lab).sum() for lab in labels] for row in arr])\n",
    "    return float(fleiss_kappa(countmat))\n",
    "\n",
    "\n",
    "def parse_prefix(prefix: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse prefix like: model__prompt__tX\n",
    "    Returns model, provider, prompt_type, temperature.\n",
    "    \"\"\"\n",
    "    parts = prefix.split(\"__\")\n",
    "    model_raw   = parts[0] if len(parts) >= 1 else prefix\n",
    "    prompt_type = parts[1] if len(parts) >= 2 else None\n",
    "    tpart       = parts[2] if len(parts) >= 3 else None\n",
    "\n",
    "    temperature = None\n",
    "    if isinstance(tpart, str) and tpart.startswith(\"t\"):\n",
    "        try:\n",
    "            temperature = int(tpart[1:]) / 10.0  # t0 -> 0.0, t10 -> 1.0\n",
    "        except Exception:\n",
    "            temperature = None\n",
    "\n",
    "    m = model_raw.lower()\n",
    "    if m.startswith(\"gpt\") or m.startswith(\"o\"):\n",
    "        provider = \"OpenAI\"\n",
    "    elif \"mistral\" in m or \"ministral\" in m:\n",
    "        provider = \"Mistral\"\n",
    "    elif any(x in m for x in [\"haiku\", \"opus\", \"sonnet\"]):\n",
    "        provider = \"Anthropic\"\n",
    "    else:\n",
    "        provider = None\n",
    "\n",
    "    return dict(model=model_raw, provider=provider, prompt_type=prompt_type, temperature=temperature)\n",
    "\n",
    "\n",
    "def _run_num(colname: str) -> int:\n",
    "    m = RUN_COL_RE.match(colname)\n",
    "    return int(m.group(\"run\")) if m else 10**9"
   ],
   "id": "feb59d5ba0a6e126",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main function for compiling results DataFrame for all models",
   "id": "f53b2322fc50db7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_llm_results(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Build a tidy DataFrame summarizing all multilabel metrics, consistency, and model metadata for all predictors. Used for generating result tables.\n",
    "    \"\"\"\n",
    "    # Group run columns by prefix\n",
    "    run_cols = [c for c in df.columns if RUN_COL_RE.match(c)]\n",
    "    prefix_to_cols = {}\n",
    "    for c in run_cols:\n",
    "        prefix = RUN_COL_RE.match(c).group(\"prefix\")\n",
    "        prefix_to_cols.setdefault(prefix, []).append(c)\n",
    "\n",
    "    # Ground truth\n",
    "    y_true_cls = df[TRUTH_COL].astype(\"Int64\")\n",
    "    y_true_innov = innovation_dummy(y_true_cls)\n",
    "\n",
    "    innovation_records = []\n",
    "    classification_records = []\n",
    "\n",
    "    for prefix, cols in prefix_to_cols.items():\n",
    "        cols_sorted = sorted(cols, key=_run_num)\n",
    "\n",
    "        # choose representative run\n",
    "        run1 = next((c for c in cols_sorted if c.endswith(\"__run1\") or c.endswith(\"__r1\")), cols_sorted[0])\n",
    "\n",
    "        meta = parse_prefix(prefix)\n",
    "\n",
    "        # representative predictions\n",
    "        y_pred_run1_cls = df[run1].astype(\"Int64\")\n",
    "        valid = y_true_cls.notna() & y_pred_run1_cls.notna()\n",
    "\n",
    "        # --- Innovation metrics (binary) ---\n",
    "        innov_metrics = dict(f1=np.nan, precision=np.nan, recall=np.nan, accuracy=np.nan)\n",
    "        if valid.any():\n",
    "            innov_metrics = compute_binary_metrics(\n",
    "                y_true_innov[valid],\n",
    "                innovation_dummy(y_pred_run1_cls[valid]),\n",
    "            )\n",
    "\n",
    "        # agreement on innovation dummy (all runs)\n",
    "        preds_innov = pd.DataFrame({c: innovation_dummy(df[c].astype(\"Int64\")) for c in cols_sorted})\n",
    "        preds_innov = preds_innov.dropna(axis=0, how=\"any\")\n",
    "\n",
    "        innov_consistency = _consistency_rate(preds_innov)\n",
    "        innov_mean_ck = _mean_cohen_kappa(preds_innov)\n",
    "        innov_fleiss = np.nan\n",
    "        if preds_innov.shape[1] >= 2 and preds_innov.shape[0] > 0:\n",
    "            try:\n",
    "                innov_fleiss = _fleiss_from_preds(preds_innov, labels=[0, 1])\n",
    "            except Exception:\n",
    "                innov_fleiss = np.nan\n",
    "\n",
    "        innovation_records.append({\n",
    "            **meta,\n",
    "            \"prefix\": prefix,\n",
    "            \"run_used\": run1,\n",
    "            \"n_runs_available\": len(cols_sorted),\n",
    "            **innov_metrics,\n",
    "            \"consistency_all_runs\": innov_consistency,\n",
    "            \"mean_cohen_kappa\": innov_mean_ck,\n",
    "            \"fleiss_kappa\": innov_fleiss,\n",
    "        })\n",
    "\n",
    "        # --- Classification metrics (multiclass) ---\n",
    "        cls_metrics = {f\"f1_{lab}\": np.nan for lab in LABELS_1_7}\n",
    "        cls_metrics.update(dict(f1_macro=np.nan, f1_weighted=np.nan))\n",
    "        if valid.any():\n",
    "            cls_metrics = compute_multiclass_metrics(\n",
    "                y_true_cls[valid],\n",
    "                y_pred_run1_cls[valid],\n",
    "                labels=LABELS_1_7\n",
    "            )\n",
    "\n",
    "        # agreement on multiclass (all runs)\n",
    "        preds_cls = pd.DataFrame({c: df[c].astype(\"Int64\") for c in cols_sorted})\n",
    "        preds_cls = preds_cls.dropna(axis=0, how=\"any\")\n",
    "\n",
    "        cls_consistency = _consistency_rate(preds_cls)\n",
    "        cls_mean_ck = _mean_cohen_kappa(preds_cls)\n",
    "        cls_fleiss = np.nan\n",
    "        if preds_cls.shape[1] >= 2 and preds_cls.shape[0] > 0:\n",
    "            try:\n",
    "                cls_fleiss = _fleiss_from_preds(preds_cls, labels=LABELS_1_7)\n",
    "            except Exception:\n",
    "                cls_fleiss = np.nan\n",
    "\n",
    "        classification_records.append({\n",
    "            **meta,\n",
    "            \"prefix\": prefix,\n",
    "            \"run_used\": run1,\n",
    "            \"n_runs_available\": len(cols_sorted),\n",
    "            **cls_metrics,\n",
    "            \"consistency_all_runs\": cls_consistency,\n",
    "            \"mean_cohen_kappa\": cls_mean_ck,\n",
    "            \"fleiss_kappa\": cls_fleiss,\n",
    "        })\n",
    "\n",
    "    innovation_df = pd.DataFrame(innovation_records).sort_values(\n",
    "        [\"provider\", \"model\", \"temperature\"], na_position=\"last\"\n",
    "    )\n",
    "    classification_df = pd.DataFrame(classification_records).sort_values(\n",
    "        [\"provider\", \"model\", \"temperature\"], na_position=\"last\"\n",
    "    )\n",
    "    return innovation_df, classification_df"
   ],
   "id": "62ec3591aa63932e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data + Compile Outputs",
   "id": "dc74785ac048fb5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load prediction files\n",
    "df = pd.read_csv(LLM_PATH, low_memory=False)\n",
    "\n",
    "# Compile results for all LLMs\n",
    "innovation_df, classification_df = build_llm_results(df)\n",
    "\n",
    "# Save\n",
    "innovation_out_path = os.path.join(OUT_DIR, \"performance_updates_innovation_frontier.csv\")\n",
    "classification_out_path = os.path.join(OUT_DIR, \"performance_updates_label-specific_frontier.csv\")\n",
    "\n",
    "innovation_df.to_csv(innovation_out_path, index=False)\n",
    "classification_df.to_csv(classification_out_path, index=False)"
   ],
   "id": "914db7da2a4b0600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tables\n",
    "In this section, we generate all Table E9 for the online appendix, based on the processed results."
   ],
   "id": "da315f5fc96697b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output and Helper Function",
   "id": "110c0d7d1be35b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Table Output Columns\n",
    "BINARY_COLS = [\n",
    "    \"model\", \"provider\", \"prompt_type\", \"temperature\", \"run_used\", \"n_runs_available\",\n",
    "    \"f1\", \"precision\", \"recall\", \"accuracy\",\n",
    "    \"fleiss_kappa\", \"mean_cohen_kappa\", \"consistency_all_runs\",\n",
    "]\n",
    "\n",
    "MULTICLASS_COLS = [\n",
    "    \"model\", \"provider\", \"prompt_type\", \"temperature\", \"run_used\", \"n_runs_available\",\n",
    "    \"f1_1\", \"f1_2\", \"f1_3\", \"f1_4\", \"f1_5\", \"f1_6\", \"f1_7\",\n",
    "    \"f1_macro\", \"f1_weighted\",\n",
    "    \"fleiss_kappa\", \"mean_cohen_kappa\", \"consistency_all_runs\",\n",
    "]\n",
    "\n",
    "# Function to construct tables based on filters and models\n",
    "def make_table(df, columns, **filters):\n",
    "    mask = pd.Series(True, index=df.index)\n",
    "    for col, val in filters.items():\n",
    "        if isinstance(val, (list, tuple, set)):\n",
    "            mask &= df[col].isin(val) | df[col].isna()\n",
    "        else:\n",
    "            mask &= (df[col] == val) | df[col].isna()\n",
    "    cols_keep = [c for c in columns if c in df.columns]\n",
    "    return df.loc[mask, cols_keep].copy()"
   ],
   "id": "d40233660e061987",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Build and export all tables",
   "id": "9ebcf746377e283f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_innov = make_table(innovation_df, BINARY_COLS)\n",
    "table_cls = make_table(classification_df, MULTICLASS_COLS)\n",
    "\n",
    "table_innov.to_excel(os.path.join(VISUAL_DIR, \"table_F9-1_01.xlsx\"), index=False)\n",
    "table_cls.to_excel(os.path.join(VISUAL_DIR, \"table_F9-2_01.xlsx\"), index=False)"
   ],
   "id": "18aa92770647e3e9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
