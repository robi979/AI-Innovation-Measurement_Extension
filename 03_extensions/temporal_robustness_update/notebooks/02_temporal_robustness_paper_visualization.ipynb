{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "In this notebook, we compute the same evaluation metrics as in demonstration study 2\n",
    "for a small set of LLM prediction columns (frontier models only)."
   ],
   "id": "12a2100e230c5918"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "acfde9533e2ba5b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ],
   "id": "2f435d00d564acb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Paths",
   "id": "1b4980ddb195756c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DEMO_PATH  = os.path.abspath(os.path.join(\"..\"))\n",
    "OUT_DIR    = os.path.join(DEMO_PATH, \"output_data\")\n",
    "VISUAL_DIR = os.path.join(DEMO_PATH, \"paper_visuals\")\n",
    "LLM_PATH   = os.path.join(OUT_DIR, \"02_validation_with_model_preds_LLM_cleaned.csv\")"
   ],
   "id": "7bb06b2836e66335",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n",
    "In this section, we define regular expressions and helper functions for parsing model output columns. This includes extracting metadata standardizing column names and processing LLM results. These routines are used to prepare the results tables for the online appendix."
   ],
   "id": "5cba78a3c560789e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "NUM_LABELS = 9\n",
    "RUN_COL_RE = re.compile(r\"^(?P<prefix>.+?)__(?:run|r)(?P<run>\\d+)$\")"
   ],
   "id": "889d874639c2775a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def standardize_llm_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Standardizes LLM column names by:\n",
    "    - removing trailing '__review'\n",
    "    - fixing underscores before temp tokens: '_t0' -> '__t0'\n",
    "    \"\"\"\n",
    "    def _fix(c):\n",
    "        c = re.sub(r'__review$', '', c)\n",
    "        return re.sub(r'(?<!_)_(t\\d+)', r'__\\1', c)\n",
    "\n",
    "    out = df.copy()\n",
    "    out.columns = [_fix(c) for c in out.columns]\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_model_pred(val, num_labels=NUM_LABELS):\n",
    "    \"\"\"\n",
    "    Parse prediction into a multi-hot vector of length num_labels.\n",
    "    Handles empty, 'nan', or '9' as all-negative.\n",
    "    Expected format: \"0;3;7\" (semicolon-separated label ids).\n",
    "    \"\"\"\n",
    "    val = str(val).strip()\n",
    "    if val in (\"\", \"9\", \"nan\", \"None\"):\n",
    "        return [0] * num_labels\n",
    "\n",
    "    vec = [0] * num_labels\n",
    "    for tok in val.replace(\" \", \"\").split(\";\"):\n",
    "        if tok.isdigit():\n",
    "            k = int(tok)\n",
    "            if 0 <= k < num_labels:\n",
    "                vec[k] = 1\n",
    "    return vec\n",
    "\n",
    "\n",
    "def parse_multi_hot(val, num_labels=NUM_LABELS):\n",
    "    \"\"\"\n",
    "    Parse ground-truth multi_hot.\n",
    "    - if list/array: use it\n",
    "    - if str: extract digits (fallback)\n",
    "    \"\"\"\n",
    "    if isinstance(val, (list, np.ndarray)):\n",
    "        arr = [int(x) for x in val]\n",
    "        return (arr + [0] * num_labels)[:num_labels]\n",
    "\n",
    "    if isinstance(val, str):\n",
    "        digs = [int(x) for x in re.findall(r\"\\d\", val)]\n",
    "        return (digs + [0] * num_labels)[:num_labels]\n",
    "\n",
    "    return [0] * num_labels\n",
    "\n",
    "\n",
    "def multilabel_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:\n",
    "    \"\"\"\n",
    "    y_true/y_pred are (n_samples, num_labels) binary matrices.\n",
    "    Returns per-label F1 plus macro/micro/weighted, subset accuracy, hamming loss.\n",
    "    \"\"\"\n",
    "    per_label = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    out = {f\"f1_{i}\": float(per_label[i]) for i in range(y_true.shape[1])}\n",
    "\n",
    "    out[\"f1_macro\"] = float(f1_score(y_true, y_pred, average=\"macro\", zero_division=0))\n",
    "    out[\"f1_micro\"] = float(f1_score(y_true, y_pred, average=\"micro\", zero_division=0))\n",
    "    out[\"f1_weighted\"] = float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0))\n",
    "\n",
    "    out[\"subset_acc\"] = float(np.mean((y_true == y_pred).all(axis=1)))\n",
    "    out[\"hamming_loss\"] = float(np.mean(y_true != y_pred))\n",
    "    return out\n",
    "\n",
    "\n",
    "def consistency_across_runs(df: pd.DataFrame, cols: list) -> float:\n",
    "    \"\"\"\n",
    "    Fraction of samples where ALL runs agree exactly on the multi-hot vector.\n",
    "    Returns NaN if fewer than 2 runs.\n",
    "    \"\"\"\n",
    "    if len(cols) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    mats = [np.vstack(df[c].apply(parse_model_pred).values) for c in cols]  # list of (n,L)\n",
    "    arr = np.stack(mats, axis=1)  # (n, runs, L)\n",
    "\n",
    "    return float(np.mean([np.all(arr[i] == arr[i][0]) for i in range(arr.shape[0])]))\n",
    "\n",
    "\n",
    "def parse_prefix(prefix: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse prefix like: model__prompt__tX\n",
    "    Returns model, provider, prompt_type, temperature.\n",
    "    \"\"\"\n",
    "    parts = prefix.split(\"__\")\n",
    "    model_raw   = parts[0] if len(parts) >= 1 else prefix\n",
    "    prompt_type = parts[1] if len(parts) >= 2 else None\n",
    "    tpart       = parts[2] if len(parts) >= 3 else None\n",
    "\n",
    "    temperature = None\n",
    "    if isinstance(tpart, str) and tpart.startswith(\"t\"):\n",
    "        try:\n",
    "            temperature = int(tpart[1:]) / 10.0\n",
    "        except Exception:\n",
    "            temperature = None\n",
    "\n",
    "    m = model_raw.lower()\n",
    "    if m.startswith(\"gpt\") or m.startswith(\"o\"):\n",
    "        provider = \"OpenAI\"\n",
    "    elif \"mistral\" in m or \"ministral\" in m:\n",
    "        provider = \"Mistral\"\n",
    "    elif any(x in m for x in [\"haiku\", \"opus\", \"sonnet\"]):\n",
    "        provider = \"Anthropic\"\n",
    "    else:\n",
    "        provider = None\n",
    "\n",
    "    return dict(model=model_raw, provider=provider, prompt_type=prompt_type, temperature=temperature)\n",
    "\n",
    "\n",
    "def _run_num(colname: str) -> int:\n",
    "    m = RUN_COL_RE.match(colname)\n",
    "    return int(m.group(\"run\")) if m else 10**9\n"
   ],
   "id": "9f7eb333292df030",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Main function for compiling results DataFrame for all models",
   "id": "54a04276f4a2039a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def build_llm_multilabel_results(df: pd.DataFrame, truth_col=\"multi_hot\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Builds a tidy results table for LLM multilabel predictions.\n",
    "    Uses a representative run (run1 if present else smallest run) for metrics,\n",
    "    and uses all runs for consistency.\n",
    "    \"\"\"\n",
    "    run_cols = [c for c in df.columns if RUN_COL_RE.match(c)]\n",
    "    prefix_to_cols = {}\n",
    "    for c in run_cols:\n",
    "        prefix = RUN_COL_RE.match(c).group(\"prefix\")\n",
    "        prefix_to_cols.setdefault(prefix, []).append(c)\n",
    "\n",
    "    # Ground truth\n",
    "    y_true = np.vstack(df[truth_col].apply(parse_multi_hot).values).astype(int)\n",
    "\n",
    "    records = []\n",
    "    for prefix, cols in prefix_to_cols.items():\n",
    "        cols_sorted = sorted(cols, key=_run_num)\n",
    "        run1 = next((c for c in cols_sorted if c.endswith(\"__run1\") or c.endswith(\"__r1\")), cols_sorted[0])\n",
    "\n",
    "        meta = parse_prefix(prefix)\n",
    "\n",
    "        # Representative predictions\n",
    "        y_pred = np.vstack(df[run1].apply(parse_model_pred).values).astype(int)\n",
    "\n",
    "        rec = {\n",
    "            **meta,\n",
    "            \"prefix\": prefix,\n",
    "            \"run_used\": run1,\n",
    "            \"n_runs_available\": len(cols_sorted),\n",
    "            **multilabel_metrics(y_true, y_pred),\n",
    "            \"consistency_all_runs\": consistency_across_runs(df, cols_sorted),\n",
    "        }\n",
    "        records.append(rec)\n",
    "\n",
    "    out = pd.DataFrame.from_records(records).sort_values(\n",
    "        [\"provider\", \"model\", \"prompt_type\", \"temperature\"], na_position=\"last\"\n",
    "    )\n",
    "    return out\n"
   ],
   "id": "e81f617bfeb06319",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Data + Compile Outputs\n",
    "Loads the LLM file, standardizes column names, computes results, and writes CSV output."
   ],
   "id": "a519c1cacf44f3b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_raw = pd.read_csv(LLM_PATH, low_memory=False)\n",
    "df = standardize_llm_columns(df_raw)\n",
    "\n",
    "# If your truth column is not named \"multi_hot\", change truth_col=... below.\n",
    "results_df = build_llm_multilabel_results(df, truth_col=\"multi_hot\")\n",
    "\n",
    "out_csv = os.path.join(OUT_DIR, \"performance_reviews_label-specific_frontier.csv\")\n",
    "results_df.to_csv(out_csv, index=False)"
   ],
   "id": "73c37db2c660f065",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tables\n",
    "In this section, we generate all Table E9 for the online appendix, based on the processed results."
   ],
   "id": "2a6933e2c578b3c1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Output",
   "id": "a02001653f2ff857"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BASE_COLS = [\"model\", \"provider\", \"prompt_type\", \"temperature\", \"run_used\", \"n_runs_available\"]\n",
    "F1_COLS   = [f\"f1_{i}\" for i in range(NUM_LABELS)]\n",
    "SUM_COLS  = [\"f1_macro\", \"f1_micro\", \"f1_weighted\", \"subset_acc\", \"hamming_loss\", \"consistency_all_runs\"]\n",
    "\n",
    "TABLE_COLS = BASE_COLS + F1_COLS + SUM_COLS\n",
    "TABLE_COLS = [c for c in TABLE_COLS if c in results_df.columns]\n",
    "\n",
    "table_all = results_df[TABLE_COLS].copy()\n",
    "\n",
    "out_xlsx = os.path.join(VISUAL_DIR, \"table_F9-2_02.xlsx\")\n",
    "table_all.to_excel(out_xlsx, index=False)"
   ],
   "id": "89a151752f741cec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "8655ce408af46154"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5ed40a823c0ca170"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
