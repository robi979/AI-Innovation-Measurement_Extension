{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "In this notebook, we compute the same evaluation metrics as in the demonstration studies for a small set of LLM prediction columns (frontier models) and create Figure E6 for the Online Appendix."
   ],
   "id": "1b60e3243bda92a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Imports",
   "id": "31651fa0203da811"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score"
   ],
   "id": "b04435d2c16c20ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Paths",
   "id": "1b2bb44cb0daa024"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DEMO_PATH = os.path.abspath(os.path.join(\"..\"))\n",
    "OUT_DIR = os.path.join(DEMO_PATH, \"output_data\")\n",
    "VISUAL_DIR = os.path.join(DEMO_PATH, \"paper_visuals\")\n",
    "\n",
    "NLP_10K_PATH = os.path.join(OUT_DIR, \"10000_validation_with_model_preds_NLP.csv\")\n",
    "NLP_20K_PATH = os.path.join(OUT_DIR, \"20000_validation_with_model_preds_NLP.csv\")\n",
    "LLM_20K_PATH = os.path.join(OUT_DIR, \"20000_ft_validation_with_model_preds_LLM_cleaned.csv\")"
   ],
   "id": "e201c45623016e79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pre-Processing\n",
    "In this section, we define regular expressions and helper functions for parsing model output columns. This includes extracting metadata standardizing column names and processing model predictions. These routines are used to prepare the Figure E6 for the Online Appendix."
   ],
   "id": "5ecd29e1f8ded9ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "TRUTH_COL = \"update_classification\"\n",
    "LABELS_1_7 = list(range(1, 8))\n",
    "\n",
    "RUN1_RE = re.compile(r\"__(run1|r1)$\")"
   ],
   "id": "85c5b9baf97f2d7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Helper Functions\n",
    "def macro_f1_multiclass(y_true: pd.Series, y_pred: pd.Series, labels=LABELS_1_7) -> float:\n",
    "    \"\"\"\n",
    "    Macro F1 for 7-class classification (labels 1..7).\n",
    "    Drops NA rows.\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(\"Int64\")\n",
    "    y_pred = y_pred.astype(\"Int64\")\n",
    "    valid = y_true.notna() & y_pred.notna()\n",
    "    return float(\n",
    "        f1_score(y_true[valid].astype(int), y_pred[valid].astype(int),\n",
    "                 labels=labels, average=\"macro\", zero_division=0)\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Predictions",
   "id": "d0c4a9ddc490ffbb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### NLP 10k Training Sample",
   "id": "41f3f0f47f46fffb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_10k_nlp = pd.read_csv(NLP_10K_PATH, low_memory=False)\n",
    "y_true_10k_nlp = df_10k_nlp[TRUTH_COL]\n",
    "\n",
    "# model columns (0..6 -> shift to 1..7)\n",
    "NLP_COLS = {\n",
    "    \"NaiveBayes\": \"tfidf__for__ml__NaiveBayes_pred\",\n",
    "    \"SVM\": \"tfidf__for__ml__SVM_pred\",\n",
    "    \"XGBoost\": \"tfidf__for__ml__XGBoost_pred\",\n",
    "    \"bert\": \"bert_for_ml_pred\",\n",
    "    \"xlnet\": \"xlnet_for_ml_pred\",\n",
    "}\n",
    "\n",
    "macro10k_nlp = {}\n",
    "for key, col in NLP_COLS.items():\n",
    "    y_pred_10k_nlp = pd.to_numeric(df_10k_nlp[col], errors=\"coerce\") + 1\n",
    "    macro10k_nlp[key] = macro_f1_multiclass(y_true_10k_nlp, y_pred_10k_nlp)"
   ],
   "id": "421d7a0a57ec797a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### NLP 20k Training Sample",
   "id": "a30f9b7d48262a91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_20k_nlp = pd.read_csv(NLP_20K_PATH, low_memory=False)\n",
    "y_true_20k_nlp = df_20k_nlp[TRUTH_COL]\n",
    "\n",
    "# same schema as 10k NLP\n",
    "NLP_COLS = {\n",
    "    \"NaiveBayes\": \"tfidf__for__ml__NaiveBayes_pred\",\n",
    "    \"SVM\": \"tfidf__for__ml__SVM_pred\",\n",
    "    \"XGBoost\": \"tfidf__for__ml__XGBoost_pred\",\n",
    "    \"bert\": \"bert_for_ml_pred\",\n",
    "    \"xlnet\": \"xlnet_for_ml_pred\",\n",
    "}\n",
    "\n",
    "macro20k_nlp = {}\n",
    "for key, col in NLP_COLS.items():\n",
    "    y_pred_20k_nlp = pd.to_numeric(df_20k_nlp[col], errors=\"coerce\") +1\n",
    "    macro20k_nlp[key] = macro_f1_multiclass(y_true_20k_nlp, y_pred_20k_nlp)"
   ],
   "id": "398279837521bdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### LLM 20k Training Sample",
   "id": "715ff8e3bb1f4c40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_20k_llm = pd.read_csv(LLM_20K_PATH, low_memory=False)\n",
    "y_true_20k_llm = df_20k_llm[TRUTH_COL]\n",
    "\n",
    "run1_cols = [c for c in df_20k_llm.columns if RUN1_RE.search(c)]\n",
    "\n",
    "\n",
    "def pick_first_contains(substr: str):\n",
    "    hits = [c for c in run1_cols if substr in c]\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "LLM_RUN1_COLS = {\n",
    "    \"gpt_4_1_nano\": pick_first_contains(\"ft_gpt_4_1_nano\"),\n",
    "    \"ministral_8b\": pick_first_contains(\"ft_ministral_8b\"),\n",
    "}\n",
    "\n",
    "macro20k_llm = {}\n",
    "for key, col in LLM_RUN1_COLS.items():\n",
    "    if col is None:\n",
    "        continue\n",
    "    y_pred_20k_llm = pd.to_numeric(df_20k_llm[col], errors=\"coerce\")  # <-- NO +1\n",
    "    macro20k_llm[key] = macro_f1_multiclass(y_true_20k_llm, y_pred_20k_llm)"
   ],
   "id": "18a5aebb4f15688b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Figure",
   "id": "4a5b852be1d58ef0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Assemble Plot Data",
   "id": "f0b5c50e9838ef59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_by_size = {\n",
    "    \"2k\": [                                             # hard-coded 2k baselines (macro-F1; from main article)\n",
    "        (\"NaiveBayes\", 0.524, \"Na誰ve Bayes\"),\n",
    "        (\"SVM\", 0.506, \"SVM\"),\n",
    "        (\"XGBoost\", 0.501, \"XGBoost\"),\n",
    "        (\"xlnet\", 0.547, \"XLNet\"),\n",
    "        (\"bert\", 0.530, \"BERT\"),\n",
    "        (\"ministral_8b\", 0.538, \"Ministral 8B\"),\n",
    "        (\"gpt_4_1_nano\", 0.710, \"GPT-4.1 Nano\"),\n",
    "    ],\n",
    "    \"10k\": [\n",
    "        (\"NaiveBayes\", macro10k_nlp[\"NaiveBayes\"], \"Na誰ve Bayes\"),\n",
    "        (\"SVM\", macro10k_nlp[\"SVM\"], \"SVM\"),\n",
    "        (\"XGBoost\", macro10k_nlp[\"XGBoost\"], \"XGBoost\"),\n",
    "        (\"xlnet\", macro10k_nlp[\"xlnet\"], \"XLNet\"),\n",
    "        (\"bert\", macro10k_nlp[\"bert\"], \"BERT\"),\n",
    "    ],\n",
    "    \"20k\": [\n",
    "        # NLP @ 20k\n",
    "        (\"NaiveBayes\", macro20k_nlp[\"NaiveBayes\"], \"Na誰ve Bayes\"),\n",
    "        (\"SVM\", macro20k_nlp[\"SVM\"], \"SVM\"),\n",
    "        (\"XGBoost\", macro20k_nlp[\"XGBoost\"], \"XGBoost\"),\n",
    "        (\"xlnet\", macro20k_nlp[\"xlnet\"], \"XLNet\"),\n",
    "        (\"bert\", macro20k_nlp[\"bert\"], \"BERT\"),\n",
    "        # LLM @ 20k (run1)\n",
    "        (\"ministral_8b\", macro20k_llm.get(\"ministral_8b\", np.nan), \"Ministral 8B\"),\n",
    "        (\"gpt_4_1_nano\", macro20k_llm.get(\"gpt_4_1_nano\", np.nan), \"GPT-4.1 Nano\"),\n",
    "    ],\n",
    "}"
   ],
   "id": "375d91828c2fd088",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Create Figure: Macro Avg. F1 vs. Scaled Training Size",
   "id": "9964fb0fb504b5f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.rc(\"font\", family=\"Times New Roman\")\n",
    "\n",
    "STYLE_LLM = {\n",
    "    \"gpt_4_1_nano\": {\"color\": \"#6BAED6\", \"marker\": \"^\", \"label\": \"GPT-4.1 Nano\"},\n",
    "    \"ministral_8b\": {\"color\": \"#74C476\", \"marker\": \"p\", \"label\": \"Ministral 8B\"},\n",
    "}\n",
    "STYLE_NON = {\n",
    "    \"NaiveBayes\": {\"color\": \"#FF6347\", \"marker\": \"D\", \"label\": \"Na誰ve Bayes\"},\n",
    "    \"SVM\": {\"color\": \"#FA8072\", \"marker\": \"v\", \"label\": \"SVM\"},\n",
    "    \"XGBoost\": {\"color\": \"#FF4500\", \"marker\": \"p\", \"label\": \"XGBoost\"},\n",
    "    \"bert\": {\"color\": \"#2F4F4F\", \"marker\": \"X\", \"label\": \"BERT\"},\n",
    "    \"xlnet\": {\"color\": \"#D3D3D3\", \"marker\": \"D\", \"label\": \"XLNet\"},\n",
    "}\n",
    "\n",
    "STYLE = {\n",
    "    **STYLE_NON,\n",
    "    **STYLE_LLM,\n",
    "}\n",
    "\n",
    "sizes = list(data_by_size.keys())\n",
    "n_groups = len(sizes)\n",
    "\n",
    "bar_width = 0.08\n",
    "inner_gap = 0.02\n",
    "group_gap = 0.4\n",
    "\n",
    "max_bars = max(len(v) for v in data_by_size.values())\n",
    "group_span = max_bars * (bar_width + inner_gap) + group_gap\n",
    "group_centers = np.arange(n_groups) * group_span\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "handles_done = {}\n",
    "\n",
    "for gi, size in enumerate(sizes):\n",
    "    models_for_size = data_by_size[size]\n",
    "    n_bars = len(models_for_size)\n",
    "\n",
    "    total_width = n_bars * (bar_width + inner_gap)\n",
    "    start_x = group_centers[gi] - total_width / 2.0\n",
    "\n",
    "    for bi, (model_key, macro_f1, nice_label) in enumerate(models_for_size):\n",
    "        x = start_x + bi * (bar_width + inner_gap)\n",
    "\n",
    "        bar = ax.bar(\n",
    "            x, macro_f1,\n",
    "            width=bar_width,\n",
    "            color=STYLE[model_key][\"color\"],\n",
    "            edgecolor=\"black\"\n",
    "        )\n",
    "\n",
    "        if model_key not in handles_done:\n",
    "            handles_done[model_key] = (bar[0], nice_label)\n",
    "\n",
    "# optional teacher/reference line\n",
    "teacher_f1 = 0.744\n",
    "ax.axhline(teacher_f1, color=\"black\", linestyle=\"--\", linewidth=1)\n",
    "ax.text(\n",
    "    group_centers[0] + group_gap * 0.2,\n",
    "    teacher_f1,\n",
    "    \"Teacher Model (F1 = 0.744)\",\n",
    "    va=\"bottom\", ha=\"left\", fontsize=9\n",
    ")\n",
    "\n",
    "ax.set_xticks(group_centers)\n",
    "ax.set_xticklabels(sizes)\n",
    "ax.set_xlabel(\"Training-Data Size (N)\")\n",
    "ax.set_ylabel(\"Macro Avg. F1-Score\")\n",
    "ax.set_title(\"Teacher-Student Setup: Macro Avg. F1-Score vs. Scaled Training Size\")\n",
    "\n",
    "legend_order = [\"NaiveBayes\", \"SVM\", \"XGBoost\", \"xlnet\", \"bert\", \"ministral_8b\", \"gpt_4_1_nano\"]\n",
    "legend_handles = [handles_done[k][0] for k in legend_order if k in handles_done]\n",
    "legend_labels = [handles_done[k][1] for k in legend_order if k in handles_done]\n",
    "\n",
    "ax.legend(\n",
    "    legend_handles, legend_labels,\n",
    "    title=\"Student Model\",\n",
    "    loc=\"lower left\",\n",
    "    bbox_to_anchor=(1.02, 0.02),\n",
    "    borderaxespad=0.5\n",
    ")\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.8, 1])\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "out_png = os.path.join(VISUAL_DIR, \"figure_E6.png\")\n",
    "plt.savefig(out_png, dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ],
   "id": "b9b602d207ff1df1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
