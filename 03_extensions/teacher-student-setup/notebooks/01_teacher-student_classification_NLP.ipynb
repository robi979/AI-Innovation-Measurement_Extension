{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "We use a high-performing framework specification as a teacher (GPT-4o-mini) to generate labels for progressively larger sets (n = 10,000 & n = 20,000) of additional release notes. We then train and tune a set of student models, including classical ML baselines and smaller transformer-based models, on these pseudo-labeled datasets and evaluate them on the same expert-labeled holdout validation set (n =1,000) used throughout the study. To ensure compatibility with scikit-learn, labels were zero-indexed."
   ],
   "id": "b4877c816a63cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "c87e5bfbf8894bfb"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from glove_vectorizer import GloveVectorizer\n",
    "from skorch import NeuralNetClassifier\n",
    "from textcnn import TorchTokenizer, TextCNN\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from optuna.samplers import TPESampler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbe1a8ec6fe38fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, Variables, and Classifier Instances",
   "id": "3d1fdee2e39d9926"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"...\"))\n",
    "\n",
    "# Define relevant paths \n",
    "TRAIN_GLOB =os.path.join(DEMO_PATH,'training_validation_data', 'sample_10000_openai_for_ml.csv') # adjust sample size file if needed\n",
    "VAL_PATH   = os.path.join(DEMO_PATH,'training_validation_data', 'demo_app_updates_validation_real_1000.csv')\n",
    "OUTPUT_DIR = os.path.join(DEMO_PATH,'output_data')\n",
    "TABLE_DIR  = os.path.join(DEMO_PATH,'tables')\n",
    "FIG_DIR    = os.path.join(DEMO_PATH,'figures')\n",
    "MODELS_DIR = os.path.join(DEMO_PATH,'models')\n",
    "\n",
    "# Define parallelism structure (note: INNER_JOBS * OUT_JOBS < max cores)\n",
    "INNER_JOBS = 5\n",
    "OUTER_JOBS = 10\n",
    "\n",
    "# CV & reproducibility\n",
    "RANDOM_STATE = 94032\n",
    "CV     = StratifiedKFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "SCORER = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "# Define classifier instances\n",
    "CLASSIFIERS = {\n",
    "    \"NaiveBayes\":        MultinomialNB(),\n",
    "    \"SVM\":               SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    \"XGBoost\":           XGBClassifier(random_state=RANDOM_STATE, n_jobs=INNER_JOBS, eval_metric=\"mlogloss\"),\n",
    "}\n",
    "\n",
    "# Define hyperparameter ranges for each classifier\n",
    "CLASSIFIER_PARAM_TEMPLATES = {\n",
    "    # NaiveBayes hyperparameters:\n",
    "    \"NaiveBayes\": {\n",
    "        \"clf__alpha\"    : uniform(1e-6, 1.0),           # Additive smoothing (1e-6-1).  Controls how aggressively rare terms are down-weighted.\n",
    "        \"clf__fit_prior\": [True, False],                # Learn class priors or assume uniform.  Testing both handles potential label imbalance.\n",
    "    },\n",
    "\n",
    "    # SVM hyperparameters:\n",
    "    \"SVM\": {\n",
    "        \"clf__C\"     : uniform(0.1, 10),                # Soft-margin cost (0.1-10).  Balances margin width vs. mis-classification tolerance.\n",
    "        \"clf__kernel\": [\"linear\", \"rbf\", \"poly\"],       # Kernels: linear (fast for TF-IDF), RBF & poly for non-linear patterns.\n",
    "        \"clf__gamma\" : [\"scale\", \"auto\"],               # Kernel coefficient heuristics.  Both common; effect only for RBF/poly.\n",
    "        \"clf__degree\": randint(2, 6),                   # Polynomial degree (2-5).  Higher degree leads to more complex decision surface (poly kernel only).\n",
    "    },\n",
    "\n",
    "    # XGBoost hyperparameters:\n",
    "    \"XGBoost\": {\n",
    "        \"clf__n_estimators\"     : randint(50, 501),     # Boosting rounds (50-500).  More rounds improve fit but risk overfitting & longer training.\n",
    "        \"clf__max_depth\"        : randint(3, 11),       # Tree depth (3-10).  Shallow trees reduce overfitting on sparse high-dim. text features.\n",
    "        \"clf__learning_rate\"    : uniform(0.01, 0.3),   # Shrinkage (0.01-0.3).  Lower LR needs more trees but can yield better generalisation.\n",
    "        \"clf__subsample\"        : uniform(0.5, 0.5),    # Row subsampling centred at 0.5.  Encourages diversity among trees; mitigates overfitting.\n",
    "        \"clf__colsample_bytree\" : uniform(0.5, 0.5),    # Column subsampling (~50 %).  Helpful with large TF-IDF vocabularies to speed up training.\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd1ebd689defbcfd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classical ML models",
   "id": "b9ad2fe3c827e77"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF Pipeline\n",
    "To identify the optimal model configuration for update classification, we systematically tuned feature representations and classifier hyperparameters using an automated and reproducible pipeline via the scikit-learn (Pedregosa et al., 2011) and XGBoost  (Chen & Guestrin, 2016) libraries.\n",
    "We used scikit-learn’s `RandomizedSearchCV` with 50 randomly sampled parameter sets per classifier for hyperparameter optimization. Feature representations and classifiers were tuned jointly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "578198b1b9889188"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define hyperparameter search space for the TF-IDF vectorizer\n",
    "TFIDF_PARAMS = {\n",
    "    \"tfidf__max_features\" : randint(2000, 10001),      # Vocabulary size (2 k – 10 k).  Controls richness of feature set vs. sparsity and runtime.\n",
    "    \"tfidf__ngram_range\"  : [(1, 1), (1, 2), (1, 3)],  # Unigrams, bigrams or trigrams.  Enables both single words and frequent phrases as features.\n",
    "    \"tfidf__min_df\"       : randint(1, 5),             # Min-doc frequency (1–4).  Prunes extremely rare tokens that add noise or inflate vocab.\n",
    "    \"tfidf__max_df\"       : uniform(0.6, 0.4),         # Max-doc frequency (0.6–1.0).  Filters very common terms that carry little discriminative power.\n",
    "}\n",
    "\n",
    "# Combine TF-IDF and model-specific hyperparameters for joint optimization in RandomizedSearchCV\n",
    "PARAM_DISTS = {\n",
    "    model_name: { **TFIDF_PARAMS, **CLASSIFIER_PARAM_TEMPLATES[model_name] }\n",
    "    for model_name in CLASSIFIERS\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d86d0eb437c29233",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Evaluation\n",
    "\n",
    "The following function `run_search_tfidf` handles model training, hyperparameter search, and evaluation on a fixed validation set. Cross-validation results are visualized."
   ],
   "id": "9e76661d1ac11bfc"
  },
  {
   "cell_type": "code",
   "source": [
    "def run_search_tfidf(clf_name, train_path, df_val):\n",
    "    \"\"\"\n",
    "    Trains and tunes a model pipeline with TF-IDF and the specified classifier.\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.splitext(os.path.basename(train_path))[0]\n",
    "    fig_sub = os.path.join(FIG_DIR, clf_name)\n",
    "    os.makedirs(fig_sub, exist_ok=True)\n",
    "\n",
    "    # Load training data\n",
    "    df_train = pd.read_csv(train_path)\n",
    "    y_train = df_train['update_classification'] - 1\n",
    "    X_train = df_train['whats_new']\n",
    "\n",
    "    # Extract target and features from the preloaded validation DataFrame\n",
    "    y_val = df_val['update_classification'] - 1\n",
    "    X_val = df_val['whats_new']\n",
    "\n",
    "    # Build sklearn pipeline: TF-IDF vectorizer followed by the selected classifier\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", CLASSIFIERS[clf_name]),\n",
    "    ])\n",
    "\n",
    "    # Perform randomized search over TF-IDF and classifier parameters using cross-validation\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=PARAM_DISTS[clf_name],\n",
    "        n_iter=20,\n",
    "        scoring=SCORER,\n",
    "        cv=CV,\n",
    "        n_jobs=OUTER_JOBS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best CV score and optimal parameter combination\n",
    "    best_cv = rs.best_score_\n",
    "    best_params = {k: (v.item() if hasattr(v, \"item\") else v)\n",
    "                   for k, v in rs.best_params_.items()}\n",
    "    \n",
    "    # Evaluate best model on the hold-out validation set\n",
    "    y_pred = rs.predict(X_val)\n",
    "\n",
    "    # Generate classification report and summarize key hold-out metrics\n",
    "    report = classification_report(\n",
    "        y_val, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    hold_macro = report['macro avg']['f1-score']\n",
    "    hold_wgtd = report['weighted avg']['f1-score']\n",
    "    hold_acc = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    # Compile summary dictionary with performance metrics and best parameters\n",
    "    summary = {\n",
    "        'dataset': dataset_name,\n",
    "        'model': clf_name,\n",
    "        'best_cv_macro_f1': best_cv,\n",
    "        'best_params': json.dumps(best_params),\n",
    "        'holdout_macro_f1': hold_macro,\n",
    "        'holdout_weighted_f1': hold_wgtd,\n",
    "        'holdout_accuracy': hold_acc\n",
    "    }\n",
    "    for label, m in report.items():\n",
    "        if label.isdigit():\n",
    "            summary[f'label_{label}_f1'] = m['f1-score']\n",
    "\n",
    "    # Plot histogram of mean CV scores from the search results\n",
    "    cv_df = pd.DataFrame(rs.cv_results_)\n",
    "    cv_df['dataset'] = dataset_name\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.hist(cv_df['mean_test_score'], bins=10, color='grey', edgecolor='white')\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    ax.set_xlabel('Mean (CV) Macro Avg. F1-Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(fig_sub, f\"20000_tfidf_{dataset_name}_histogram.jpg\"),\n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    return summary, cv_df, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eabda08981cfaa22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execution\n",
    "We loop over all available training splits and classifiers, optimizing each configuration. Results and predictions are saved for further analysis.\n"
   ],
   "id": "14e00515d369ed5a"
  },
  {
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "df_val = pd.read_csv(VAL_PATH)\n",
    "\n",
    "# Initialize containers for results and predictions\n",
    "all_summaries = []\n",
    "cv_results_by_model = {name: [] for name in CLASSIFIERS}\n",
    "model_preds = {}\n",
    "\n",
    "# Loop through different training files and classifiers\n",
    "for train_csv in glob.glob(TRAIN_GLOB):\n",
    "    base = os.path.splitext(os.path.basename(train_csv))[0]\n",
    "    parts = base.split('_')  # parts == [\"demo\",\"app\",\"updates\",\"train\",\"real\",\"2000\"]\n",
    "    split_type = parts[-2]   # \"real\" or \"equal\"\n",
    "    size       = parts[-1]   # \"2000\", etc.\n",
    "    for name in CLASSIFIERS:\n",
    "        print(f\"Running {name} on {train_csv}\")\n",
    "        summary, cv_df, y_pred = run_search_tfidf(name, train_csv, df_val)\n",
    "        all_summaries.append(summary)\n",
    "        cv_results_by_model[name].append(cv_df)\n",
    "        # Store predictions for this model and dataset combination\n",
    "        key = f\"tfidf__{split_type}__{size}__{name}\"\n",
    "        model_preds[key] = y_pred\n",
    "\n",
    "# Combine all summary metrics into a single DataFrame and export to CSV\n",
    "combined_df = pd.DataFrame(all_summaries)\n",
    "combined_df.to_csv(\n",
    "    os.path.join(TABLE_DIR, '20000_tfidf_all_models_all_datasets_summary.csv'),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Append model predictions to the validation set and export results\n",
    "val_with_preds = df_val.copy()\n",
    "for key, preds in model_preds.items():\n",
    "    colname = f\"{key}_pred\"\n",
    "    val_with_preds[colname] = preds\n",
    "val_with_preds.to_csv(os.path.join(OUTPUT_DIR, '20000_validation_with_model_preds_NLP.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "97d58dbf20e1bde1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Trained Language Models\n",
    "We fine-tune four transformer checkpoints (`BERT` & `XLNet`) on each train split.\n",
    "Hyper-parameters are tuned with **Optuna** (`NUM_TRIALS = 1`) and the best model is evaluated on the houldout validation set. To keep this extension efficient compared to the specification of the main article, we limit optuna trials to 1."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14bea2f9bea84491"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hugging Face model checkpoints\n",
    "MODEL_CHECKPOINTS = {\n",
    "    \"xlnet\"   : \"xlnet-base-cased\",\n",
    "    \"bert\"    : \"bert-base-cased\",\n",
    "}\n",
    "\n",
    "# Base TrainingArguments shared by all fine-tuning runs\n",
    "BASE_ARGS = {\n",
    "    \"eval_strategy\"           : \"epoch\",   # Evaluate once per epoch for clear learning curves.\n",
    "    \"save_strategy\"           : \"epoch\",   # Save a checkpoint after every epoch for rollback.\n",
    "    \"load_best_model_at_end\"  : True,      # Restore best epoch automatically.\n",
    "    \"metric_for_best_model\"   : \"f1_macro\",# Macro-F1 chosen to weight classes equally.\n",
    "    \"greater_is_better\"       : True,      # Higher F1 = better.\n",
    "    \"seed\"                    : RANDOM_STATE,\n",
    "    \"logging_steps\"           : 50,        # Frequent logging; low overhead on modern GPUs.\n",
    "    \"save_total_limit\"        : 1,         # Keep only the best checkpoint → disk-friendly.\n",
    "    \"disable_tqdm\"            : True,      # Cleaner notebook output.\n",
    "    \"report_to\"               : [],        # Disable WandB/MLflow unless explicitly enabled.\n",
    "}\n",
    "\n",
    "# Optuna trials per checkpoint\n",
    "NUM_TRIALS = 1"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38504da64ed5ab91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Helper: Load CSVs as HuggingFace Datasets\n",
    "Zero-bases the labels so they align with `transformers` expectations."
   ],
   "id": "5c637d9fa06365a"
  },
  {
   "cell_type": "code",
   "source": [
    "def load_datasets(train_csv, val_csv):\n",
    "    \"\"\"Return Hugging Face Datasets for train / validation splits.\"\"\"\n",
    "    df_train = pd.read_csv(train_csv)\n",
    "    df_val = pd.read_csv(val_csv)\n",
    "    df_train['label'] = df_train['update_classification'] - 1\n",
    "    df_val['label'] = df_val['update_classification'] - 1\n",
    "    return (\n",
    "        Dataset.from_pandas(df_train[['whats_new','label']]),\n",
    "        Dataset.from_pandas(df_val[['whats_new','label']]),\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9928395b6ae47991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Hyperparameter Search\n",
    "`run_search_transformer` performs an Optuna search, saves the best model, and returns metrics plus predictions."
   ],
   "id": "952e15e34417bc56"
  },
  {
   "cell_type": "code",
   "source": [
    "def run_search_transformer(model_key, checkpoint, train_csv, val_csv):\n",
    "    \"\"\"\n",
    "    Fine-tune a transformer checkpoint, tune HPs with Optuna, evaluate on hold-out.\n",
    "    Returns summary dict, Optuna trials DataFrame, and validation predictions.\n",
    "    \"\"\"\n",
    "    split_name = os.path.basename(train_csv).rsplit('.', 1)[0]\n",
    "    train_ds, val_ds = load_datasets(train_csv, val_csv)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "    # Tokenization\n",
    "    def preprocess(batch):\n",
    "        toks = tok(batch[\"whats_new\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "        toks[\"labels\"] = batch[\"label\"]\n",
    "        return toks\n",
    "\n",
    "    train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "    val_ds   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    # Optuna setup\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    study   = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "\n",
    "    def objective(trial):\n",
    "        # Reset PyTorch RNGs for reproducibility\n",
    "        torch.manual_seed(RANDOM_STATE)\n",
    "        torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "        # Set hyperparameters\n",
    "        lr     = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)  # Learning rate (1e-5–5e-5).  Standard fine-tune range; log-scale search.\n",
    "        bs     = trial.suggest_categorical(\"batch_size\", [8, 16])            # Batch size 8/16.  16 fits 12–16 GB GPUs; 8 for memory-constrained cases.\n",
    "        epochs = trial.suggest_int(\"num_train_epochs\", 3, 5)                 # Training epochs (3–5).  Balances convergence vs. over-fitting.\n",
    "        wd     = trial.suggest_float(\"weight_decay\", 0.0, 0.01)              # L2 weight decay (0–0.01).  Light regularisation for stability.\n",
    "\n",
    "\n",
    "        trial_dir = os.path.join(MODELS_DIR, f\"{model_key}_{split_name}\", f\"trial_{trial.number}\")\n",
    "        os.makedirs(trial_dir, exist_ok=True)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=trial_dir,\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=wd,\n",
    "            **BASE_ARGS,\n",
    "        )\n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=len(set(train_ds[\"labels\"])))\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            compute_metrics=lambda p: {\n",
    "                \"f1_macro\": f1_score(p.label_ids, np.argmax(p.predictions, axis=1), average=\"macro\"),\n",
    "                \"f1_weighted\": f1_score(p.label_ids, p.predictions.argmax(-1), average=\"weighted\"),\n",
    "                \"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1)),\n",
    "            },\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        trial.set_user_attr(\"best_ckpt\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "        # Clean up GPU mem\n",
    "        del trainer, model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return metrics[\"eval_f1_macro\"]\n",
    "\n",
    "    # Run Optuna\n",
    "    study.optimize(objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "    # Save all trial results + best flag\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_df[\"is_best\"] = trials_df[\"number\"] == study.best_trial.number\n",
    "    trials_out = os.path.join(TABLE_DIR, f\"{model_key}_{split_name}_optuna_trials.csv\")\n",
    "    trials_df.to_csv(trials_out, index=False)\n",
    "    print(f\"Saved {len(trials_df)} trials to {trials_out} (best trial = {study.best_trial.number})\")\n",
    "\n",
    "    # Reload & save best model centrally\n",
    "    best = study.best_trial\n",
    "    best_ckpt = best.user_attrs[\"best_ckpt\"]\n",
    "    master_dir = os.path.join(MODELS_DIR, f\"{model_key}_{split_name}_best\")\n",
    "    os.makedirs(master_dir, exist_ok=True)\n",
    "\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(best_ckpt)\n",
    "    best_tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    best_model.save_pretrained(master_dir)\n",
    "    best_tok.save_pretrained(master_dir)\n",
    "\n",
    "    # Compute per-label F1 on hold-out\n",
    "    eval_trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=TrainingArguments(\n",
    "            output_dir=master_dir,\n",
    "            per_device_eval_batch_size=best.params[\"batch_size\"],\n",
    "        ),\n",
    "        tokenizer=best_tok,\n",
    "    )\n",
    "    preds_out = eval_trainer.predict(val_ds)\n",
    "    y_pred    = preds_out.predictions.argmax(-1)\n",
    "    y_true    = preds_out.label_ids\n",
    "    rpt = classification_report(y_true, y_pred, labels=list(range(7)), output_dict=True, zero_division=0)\n",
    "\n",
    "    # Compile summary dictionary with performance metrics and best parameters\n",
    "    summary = {\n",
    "        \"dataset\":              split_name,\n",
    "        \"model\":                model_key,\n",
    "        \"best_cv_macro_f1\":     study.best_trial.value,\n",
    "        \"best_params\":          json.dumps(best.params),\n",
    "        \"holdout_accuracy\":     accuracy_score(y_true, y_pred),\n",
    "        \"holdout_macro_f1\":     rpt[\"macro avg\"][\"f1-score\"],\n",
    "        \"holdout_weighted_f1\":  rpt[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "    for lbl, m in rpt.items():\n",
    "        if lbl.isdigit():\n",
    "            summary[f\"label_{lbl}_f1\"] = m[\"f1-score\"]\n",
    "\n",
    "    return summary, trials_df, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbe3395ab0be76ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execution\n",
    "Loop over every **train split × checkpoint** combination, run `run_search_transformer`, and append predictions for downstream comparison."
   ],
   "id": "b20712c7896df4e2"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for results and predictions\n",
    "all_results = []\n",
    "model_preds = {}\n",
    "\n",
    "# Loop through different training files and transformer checkpoints\n",
    "for train_csv in glob.glob(TRAIN_GLOB):\n",
    "    # Extract split_type and size from the filename\n",
    "    base       = os.path.splitext(os.path.basename(train_csv))[0]\n",
    "    parts      = base.split('_')       # e.g. [\"demo\",\"app\",\"updates\",\"train\",\"real\",\"2000\"]\n",
    "    split_type = parts[-2]             # \"real\" or \"equal\"\n",
    "    size       = parts[-1]             # \"2000\", etc.\n",
    "\n",
    "    for model_key, checkpoint in MODEL_CHECKPOINTS.items():\n",
    "        print(f\"[FT] {model_key} on {split_type} n={size}\")\n",
    "        summary, trials_df, y_pred = run_search_transformer(\n",
    "            model_key,\n",
    "            checkpoint,\n",
    "            train_csv,\n",
    "            VAL_PATH\n",
    "        )\n",
    "        all_results.append(summary)\n",
    "        key = f\"{model_key}_{split_type}_{size}\"\n",
    "        model_preds[key] = y_pred\n",
    "\n",
    "# Combine all summary metrics into a single DataFrame and export to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(os.path.join(TABLE_DIR, \"10000_transformers_optuna_summary.csv\"), index=False)\n",
    "\n",
    "# Merge PLM predictions into the existing output file\n",
    "val_with_preds = pd.read_csv(os.path.join(OUTPUT_DIR, '10000_validation_with_model_preds_NLP.csv'))\n",
    "for key, preds in model_preds.items():\n",
    "    val_with_preds[f\"{key}_pred\"] = preds\n",
    "val_with_preds.to_csv(\n",
    "    os.path.join(OUTPUT_DIR, '10000_validation_with_model_preds_NLP.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "722cb1298212501e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

