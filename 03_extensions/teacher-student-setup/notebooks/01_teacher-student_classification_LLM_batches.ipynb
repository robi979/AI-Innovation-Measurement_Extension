{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "We use the teacher model generated labels for progressively larger sets (n = 10,000 & n = 20,000) of additional release notes to train and tune a set of smaller LLM student models on these pseudo-labeled datasets. We evaluate them on the same expert-labeled holdout validation set (n =1,000) used throughout the study.\n",
    "For each provider (OpenAI & Mistral), a systematic procedure was implemented to generate, upload, and execute batch requests for both base and fine-tuned models, adhering to the respective provider’s API documentation."
   ],
   "id": "60c38c0f898000c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "5bc68bd5147202f5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "from anthropic import Anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "e9da44f648cc492d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, and Variables",
   "id": "5ff72c6357395e9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "API_KEY_DIR = os.path.expanduser(os.getenv(\"API_KEY_DIR\", 'F:\\RobinNO\\API_keys')) # insert path to .txt with API Key\n",
    "PROMPT_DIR = os.path.join(DEMO_PATH, 'LLM_API', 'prompt_templates')\n",
    "LLM_API = os.path.join(DEMO_PATH,'LLM_API')\n",
    "VAL_PATH = os.path.join(DEMO_PATH, 'training_validation_data', 'demo_app_updates_validation_real_1000.csv')\n",
    "\n",
    "OPENAI_BATCH_DIR = os.path.join(LLM_API, 'OpenAI_batches', 'raw')\n",
    "MISTRAL_BATCH_DIR = os.path.join(LLM_API, 'Mistral_batches', 'raw')\n",
    "ANTHROPIC_BATCH_DIR = os.path.join(LLM_API, 'Anthropic_batches')\n",
    "\n",
    "OPENAI_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'OpenAI_batches', \"results\")\n",
    "MISTRAL_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Mistral_batches', 'results')\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 94032\n",
    "\n",
    "# Max lines per batch (just redundancy, as batch per model size shouldn't exceed this)\n",
    "MAX_LINES_PER_BATCH = 50000\n",
    "\n",
    "# Load API keys\n",
    "with open(os.path.join(API_KEY_DIR, \"mistral_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: MISTRAL_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"openai_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: OPENAI_API_KEY = f.read().strip()\n",
    "\n",
    "# Define API-relevant URLs and clients\n",
    "MISTRAL_CLIENT =  Mistral(api_key=MISTRAL_API_KEY)\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "### Models for API-requests\n",
    "# OpenAI GPT models (fine-tuned set -> run (01) 4-2 notebook before or fine-tune models via developer platform, training data file in 'training_validation_data')\n",
    "GPT_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"GPT_fine-tuned.txt\")\n",
    "\n",
    "# Mistral models (fine-tuned set -> run (01) 4-2 notebook before or fine-tune models via developer platform, training data file in 'training_validation_data')\n",
    "MISTRAL_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"MISTRAL_fine-tuned.txt\")\n",
    "\n",
    "# Load prompt template\n",
    "DEFAULT_PROMPT_TEMPLATE = Path(PROMPT_DIR, \"updates_prompt_default.txt\").read_text(encoding=\"utf-8\").strip()\n",
    "DEFAULT_PROMPT_NAME = \"default\"\n",
    "\n",
    "# Other settings\n",
    "DEFAULT_TEMPERATURE = 0.0\n",
    "REPEATED_RUNS = 3"
   ],
   "id": "f36a47c54a96be45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "df = pd.read_csv(VAL_PATH)"
   ],
   "id": "753b56a08691a1b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI Batches\n",
    "We generate JSONL batch files for **default** and **fine-tuned** GPT models, upload them, and create 24 h batch-jobs via the OpenAI API."
   ],
   "id": "67a02679cc755c33"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per GPT model. Run either Default oder Fine-Tuned Models."
   ],
   "id": "4dc3ac0374dae609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fine-Tuned Models\n",
    "Note: Before running this code, run (01) 4-2 notebook before or fine-tune models via developer platform, training data file in `training_validation_data`."
   ],
   "id": "2504846431543400"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(GPT_FT_MODELS_PATH, encoding=\"utf-8\") as f:\n",
    "    FT_MODELS = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "for ft_model in FT_MODELS:\n",
    "    entries = []\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            update_text = (row.get(\"whats_new\", \"\") or \"\").strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp update text: {update_text}\"\n",
    "\n",
    "            custom_id = (\n",
    "                f\"{ft_model.replace('/', '-')}\"\n",
    "                f\"__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int(DEFAULT_TEMPERATURE * 10)}\"\n",
    "                f\"__run{run}\"\n",
    "                f\"__id{row['id']}\"\n",
    "            )\n",
    "\n",
    "            body = {\n",
    "                \"model\": ft_model,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_completion_tokens\": 1000,\n",
    "                \"seed\": SEED,\n",
    "                \"temperature\": DEFAULT_TEMPERATURE,\n",
    "            }\n",
    "\n",
    "            entries.append({\n",
    "                \"custom_id\": custom_id,\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": body\n",
    "            })\n",
    "\n",
    "    # Paginate entries to ≤ MAX_LINES_PER_BATCH lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    model_safe = re.sub(r'[:/\\\\\\s]+', '-', ft_model)\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(OPENAI_BATCH_DIR, batch_fname)\n",
    "\n",
    "        with open(batch_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {ft_model} to {batch_path} (lines {start + 1}–{end})\")"
   ],
   "id": "29eba8e906dd0bb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to OpenAI and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "2509eda64c727370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(OPENAI_BATCH_DIR, \"openai_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded_file_ids = []\n",
    "for path in batch_paths:\n",
    "    print(f\"Uploading {path}...\")\n",
    "    with open(path, 'rb') as f:\n",
    "        file_resp = OPENAI_CLIENT.files.create(\n",
    "            file=f,\n",
    "            purpose='batch'\n",
    "        )\n",
    "    print(f\"Uploaded: id={file_resp.id}, filename={file_resp.filename}, bytes={file_resp.bytes}\")\n",
    "    uploaded_file_ids.append(file_resp.id)\n",
    "\n",
    "print(\"All batch files uploaded.\\n\")"
   ],
   "id": "cb5a348208f56347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a 24 h chat-completion batch job for each uploaded file"
   ],
   "id": "ecbcf57f7ee3abd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Batch for each uploaded file\n",
    "for file_id in uploaded_file_ids:\n",
    "    batch = OPENAI_CLIENT.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"eval job for {file_id}\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created batch {batch.id} for {file_id}, status: {batch.status}\")"
   ],
   "id": "e38e61616ea89440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mistral Batches\n",
    "We generate JSONL batch files for default and fine-tuned Mistral models, upload them, and create batch-jobs via the Mistral API."
   ],
   "id": "5d9783de1d600d6e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per Mistral model. Run either Default or Fine-Tuned Models."
   ],
   "id": "d0852972cb497609"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "fc33628abd544a77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fine-Tuned Models\n",
    "Note: Before running this code, execute notebook 4-2 to generate fine-tuned models via fine-tuning API."
   ],
   "id": "639352aecbae1efc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(MISTRAL_FT_MODELS_PATH, encoding=\"utf-8\") as f:\n",
    "    raw_ft = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "for ft_full in raw_ft:\n",
    "    parts = ft_full.split(\":\")\n",
    "    if len(parts) < 5:\n",
    "        raise ValueError(f\"Bad line in MISTRAL_fine-tuned.txt: {ft_full}\")\n",
    "\n",
    "    # model_id is the actual model parameter for Mistral batch job later\n",
    "    model_id = \":\".join(parts[:4])  # e.g., ft:mistral-...:rev:date\n",
    "    suffix = parts[4]  # dataset token etc.\n",
    "\n",
    "    safe_ft = re.sub(r\"[:/\\\\\\s]+\", \"-\", ft_full)\n",
    "\n",
    "    entries = []\n",
    "\n",
    "    for run in range(1, REPEATED_RUNS + 1):\n",
    "        for _, row in df.iterrows():\n",
    "            txt = (row.whats_new or \"\").strip()\n",
    "            prompt = f\"{DEFAULT_PROMPT_TEMPLATE}\\n\\nApp update text: {txt}\"\n",
    "\n",
    "            cid = (\n",
    "                f\"{safe_ft}\"\n",
    "                f\"__{DEFAULT_PROMPT_NAME}\"\n",
    "                f\"__t{int(DEFAULT_TEMPERATURE * 10)}\"\n",
    "                f\"__run{run}\"\n",
    "                f\"__id{row.id}\"\n",
    "            )\n",
    "\n",
    "            body = {\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"random_seed\": SEED,\n",
    "                \"temperature\": DEFAULT_TEMPERATURE,\n",
    "            }\n",
    "\n",
    "            entries.append({\"custom_id\": cid, \"body\": body})\n",
    "\n",
    "    # Paginate entries to ≤ MAX_LINES_PER_BATCH lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        chunk = entries[part * MAX_LINES_PER_BATCH:(part + 1) * MAX_LINES_PER_BATCH]\n",
    "        idx = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"mistral_batch_{safe_ft}_{idx}.jsonl\"\n",
    "        path = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for e in chunk:\n",
    "                fout.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(chunk)} entries for {ft_full} → {path}\")"
   ],
   "id": "e24d33b972a7c2e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to Mistral and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "ea9157db8cd0dfca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(MISTRAL_BATCH_DIR, \"mistral_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded = []\n",
    "for path in batch_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    model = fname.split(\"_\")[2]\n",
    "\n",
    "    print(f\"Uploading {fname} (model={model})…\")\n",
    "    with open(path, 'rb') as f:\n",
    "        up = MISTRAL_CLIENT.files.upload(\n",
    "            file={\"file_name\": fname, \"content\": f},\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    print(f\"Uploaded: id={up.id}\")\n",
    "    uploaded.append({\"model\": model, \"file_id\": up.id})\n",
    "\n",
    "print(\"All Mistral batch files uploaded.\\n\")"
   ],
   "id": "e3a1aec05ffaddbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a batch job for each uploaded file."
   ],
   "id": "98b0e832e5bc9c8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a batch job for each uploaded file\n",
    "for item in uploaded:\n",
    "    file_id = item[\"file_id\"]\n",
    "    model = item[\"model\"]\n",
    "    # Parse model name for fine-tuned models\n",
    "    if model.startswith(\"ft-\"):\n",
    "        parts = model.split(\"-\")\n",
    "        # parts = [\"ft\",\"mistral\",\"small\",\"latest\",\"d1ef7e20\",\"20250527\",\"314de464\",\"equal_500\"]\n",
    "\n",
    "        prefix = parts[0]\n",
    "        model_name = \"-\".join(parts[1:4])   # \"mistral-small-latest\"\n",
    "        rev = parts[4]              # \"d1ef7e20\"\n",
    "        date = parts[5]              # \"20250527\"\n",
    "        sha = parts[6]              # \"314de464\"\n",
    "\n",
    "        real_model = f\"{prefix}:{model_name}:{rev}:{date}:{sha}\"\n",
    "        # \"ft:mistral-small-latest:d1ef7e20:20250527:314de464\"\n",
    "    else:\n",
    "        real_model = model\n",
    "\n",
    "    # Submit batch job to Mistral API\n",
    "    job = MISTRAL_CLIENT.batch.jobs.create(\n",
    "        input_files=[file_id],\n",
    "        model=real_model,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        metadata={\"description\": f\"classification batch for {real_model}\"}\n",
    "    )\n",
    "    print(f\"Created batch job {job.id} for file {file_id} (model={real_model}), status={job.status}\")"
   ],
   "id": "619ab0bbe99910b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process results\n",
    "We process JSONL result files and convert them to CSV format for analysis. Note: Download batch files from each provider's developer platform before."
   ],
   "id": "9057656827634055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation dataset and prepare ID columns for matching with batch results\n",
    "df = pd.read_csv(VAL_PATH, dtype={'id': str})\n",
    "df['id'] = df['id'].str.strip()"
   ],
   "id": "80455e4a9ccf4fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper Functions\n",
    "Utility functions for ID normalization and record parsing across different LLM providers."
   ],
   "id": "a5644509edfd497b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper: normalize ID string by removing 'id' prefix",
   "id": "348dcf96529d5ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_id_str(id_part):\n",
    "    \"\"\"Clean ID strings by removing 'id' prefix and whitespace\"\"\"\n",
    "    return id_part.replace('id', '').strip()"
   ],
   "id": "3cb06f4c42924d5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Provider-Specific Parsers\n",
    "Each LLM provider has different JSONL response formats requiring specialized parsing."
   ],
   "id": "9d73562abb7460f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_openai_record(record):\n",
    "    \"\"\"Parse OpenAI batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    parts = record.get('custom_id', '').split('__')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, col, content\n",
    "\n",
    "def parse_mistral_record(record):\n",
    "    \"\"\"Parse Mistral batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    parts = record.get('custom_id', '').split('__')\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "    model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, col, content"
   ],
   "id": "623486580d4244e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### File Processing Functions\n",
    "Core functions for processing JSONL files and parallel execution."
   ],
   "id": "8b8c4b49d9fc1653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def process_file(path, parser):\n",
    "    \"\"\"Process a single JSONL file using the specified parser\"\"\"\n",
    "    mapping = {}\n",
    "    print(f\"Processing {path.name}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed JSON lines\n",
    "                continue\n",
    "            # Parse record using provider-specific parser\n",
    "            parsed = parser(rec)\n",
    "            if parsed:\n",
    "                id_str, col, content = parsed\n",
    "                # Group by column, then by ID\n",
    "                mapping.setdefault(col, {})[id_str] = content\n",
    "\n",
    "    print(f\"  Mapped {len(mapping)} columns from {path.name}\")\n",
    "    return mapping\n",
    "\n",
    "def process_results_parallel(directory, parser, df, workers=10):\n",
    "    \"\"\"Process all JSONL files in a directory using parallel execution.\"\"\"\n",
    "    # Find all JSONL files in directory and subdirectories\n",
    "    files = list(Path(directory).rglob('*.jsonl'))\n",
    "    print(f\"Found {len(files)} files in {directory}\")\n",
    "\n",
    "    combined = {}\n",
    "\n",
    "    # Process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all file processing tasks\n",
    "        futures = {executor.submit(process_file, f, parser): f for f in files}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            file_map = future.result()\n",
    "            # Merge results from each file\n",
    "            for col, id_map in file_map.items():\n",
    "                combined.setdefault(col, {}).update(id_map)\n",
    "\n",
    "    # Add new columns to dataframe\n",
    "    for col, id_map in combined.items():\n",
    "        df[col] = df['id'].map(id_map)\n",
    "\n",
    "    return df.copy()"
   ],
   "id": "281215772d4ea7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process All Provider Results\n",
    "Execute processing for each LLM provider in sequence."
   ],
   "id": "3a794e90c24952d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = process_results_parallel(OPENAI_BATCH_RESULTS_DIR, parse_openai_record, df)\n",
    "df = process_results_parallel(MISTRAL_BATCH_RESULTS_DIR, parse_mistral_record, df)"
   ],
   "id": "b4d05371150991d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save final output\n",
    "df.to_csv(os.path.join(DEMO_PATH, 'output_data', '10000_ft_validation_with_model_preds_LLM.csv'), index=False)"
   ],
   "id": "b02481924e211f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean Results\n",
    "Standardize raw model outputs into valid single-label strings (1–7).\n",
    "Cleans and validates outputs from LLMs or API responses, ensuring only acceptable class labels are retained."
   ],
   "id": "e80cc1ad96b749f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Prep and Helper Function",
   "id": "5c450cfaf10ff0e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Columns to exclude from label cleaning\n",
    "EXCLUDE_COLS = [\n",
    "    'release_date', 'version_display', 'whats_new', 'body', 'update_classification', 'id', 'app_id', 'previous_version', 'previous_release_date', 'previous_id'\n",
    "]\n",
    "\n",
    "def extract_single_label(text):\n",
    "    \"\"\"\n",
    "    Normalize prediction output into a single class label string (1–7).\n",
    "    Returns '0' as a fallback for invalid predictions.\n",
    "\n",
    "    Handles:\n",
    "    - Integers or floats like 3, 3.0\n",
    "    - Strings like '3', ' (3) ', or even '3 ; 4' (keeps only last valid single label)\n",
    "    \"\"\"\n",
    "    if isinstance(text, float):\n",
    "        if pd.isna(text):\n",
    "            return '0'  # fallback\n",
    "        if text.is_integer() and 1 <= int(text) <= 7:\n",
    "            return str(int(text))\n",
    "        return '0'\n",
    "\n",
    "    if isinstance(text, int):\n",
    "        return str(text) if 1 <= text <= 7 else '0'\n",
    "\n",
    "    # Parse strings and try to extract digits in range 1–7\n",
    "    text = str(text)\n",
    "    pattern = r'\\(?\\s*([1-7](?:\\s*;\\s*[1-7])*)\\s*\\)?'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return '0'\n",
    "    last = matches[-1]\n",
    "    cleaned = [d for d in re.split(r'\\s*;\\s*', last.strip('; ')) if d.isdigit() and 1 <= int(d) <= 7]\n",
    "    return cleaned[-1] if cleaned else '0'"
   ],
   "id": "47980869480f866c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model Output File and Apply Cleaning",
   "id": "25631454efcbed60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(DEMO_PATH, 'output_data', '10000_ft_validation_with_model_preds_LLM.csv'))"
   ],
   "id": "bc8d4ba93c4e1215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify prediction columns (exclude inputs/metadata)\n",
    "model_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "# Apply label cleaning function to each model column\n",
    "for col in model_cols:\n",
    "    df[col] = df[col].apply(extract_single_label)\n",
    "\n",
    "# Save cleaned predictions to file\n",
    "save_path = os.path.join(DEMO_PATH, 'output_data', '10000_ft_validation_with_model_preds_LLM_cleaned.csv')\n",
    "df.to_csv(save_path, index=False)"
   ],
   "id": "419fe81344ffd1f2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
