{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "Fine-tuned models are trained on datasets (distribution and size) similar to those in notebook 3."
   ],
   "id": "f7a29443af8a7715"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "ba8c78698d3ef523"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "from openai import OpenAI\n",
    "from openai.types.fine_tuning import SupervisedMethod, SupervisedHyperparameters\n",
    "from mistralai import Mistral"
   ],
   "id": "530079752afaf018",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, Variables, and Models",
   "id": "872c558d4c064a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "API_KEY_DIR = os.path.expanduser(os.getenv(\"API_KEY_DIR\", 'PATH')) # insert path to .txt with API Key\n",
    "LLM_API = os.path.join(DEMO_PATH,'LLM_API')\n",
    "JSONL_FILES = glob.glob(os.path.join(LLM_API, 'demo_app_updates_train_*.jsonl'))\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 94032\n",
    "\n",
    "# Load API keys\n",
    "with open(os.path.join(API_KEY_DIR, \"mistral_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: MISTRAL_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"openai_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: OPENAI_API_KEY = f.read().strip()\n",
    "\n",
    "# Define API-relevant clients\n",
    "MISTRAL_CLIENT =  Mistral(api_key=MISTRAL_API_KEY)\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "### Models for API-requests\n",
    "# OpenAI GPT models\n",
    "GPT_MODELS = [\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-4o-2024-08-06\",\n",
    "    \"gpt-4.1-2025-04-14\",\n",
    "    \"gpt-4.1-mini-2025-04-14\",\n",
    "    \"gpt-4.1-nano-2025-04-14\",\n",
    "]\n",
    "\n",
    "# Mistral models (need to use \"-latest\" models for API)\n",
    "MISTRAL_MODELS = [\n",
    "    \"mistral-large-latest\",\n",
    "    \"mistral-small-latest\",\n",
    "    \"open-mistral-nemo\",\n",
    "    \"ministral-8b-latest\",\n",
    "]\n",
    "###\n",
    "\n",
    "# Define hyperparameters (let API decide, unless overridden)\n",
    "N_EPOCHS   = \"auto\"\n",
    "BATCH_SIZE = \"auto\"\n",
    "LR_MULT    = \"auto\"\n",
    "\n",
    "# Define parameters to fit API limitations\n",
    "FILE_BATCH_SIZE = 3     # How many files to process per model\n",
    "SKIP_BATCH = 3          # Set manually to choose which batch of files to process (0 = first 3, 1 = 4–6, etc)"
   ],
   "id": "69280ebee26e4d43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Fine-Tuning via API\n",
    "Upload JSONL files, launch fine-tuning jobs per model and training split, and summarize jobs in a JSON file."
   ],
   "id": "2ec366fbf55b9e9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### OpenAI Models",
   "id": "86e0ed1fac9ac53f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Upload each training file and capture OpenAI file IDs\n",
    "file_ids = {}\n",
    "for path in JSONL_FILES:\n",
    "    fname = os.path.basename(path)\n",
    "    print(f\"Uploading {fname}...\")\n",
    "    with open(path, 'rb') as fin:\n",
    "        resp = OPENAI_CLIENT.files.create(file=fin, purpose='fine-tune')\n",
    "    file_ids[fname] = resp.id\n",
    "    print(f\"-> {fname} uploaded as {resp.id}\")\n"
   ],
   "id": "6e34074cdfe45674",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pre-compute a list of (fname, file_id)\n",
    "all_files = list(file_ids.items())\n",
    "tasks_summary = []\n",
    "\n",
    "# Fine-tune each OpenAI model on each file in the current batch\n",
    "for model in GPT_MODELS:\n",
    "    # Pick out exactly FILE_BATCH_SIZE files starting at SKIP_BATCH * BATCH_SIZE\n",
    "    start = SKIP_BATCH * FILE_BATCH_SIZE\n",
    "    end   = start + FILE_BATCH_SIZE\n",
    "    batch = all_files[start:end]\n",
    "\n",
    "    for fname, training_file_id in batch:\n",
    "        # Parse dataset info from filename\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        parts = base.split('_')\n",
    "        split_type = parts[-2]\n",
    "        size       = parts[-1]\n",
    "\n",
    "        # Short model id for readable suffix (e.g. '3.5-turbo')\n",
    "        mparts = model.split('-')\n",
    "        model_short = '-'.join(mparts[1:3])\n",
    "\n",
    "        suffix = f\"{model_short}-{split_type}-{size}\"\n",
    "\n",
    "        # Prepare OpenAI API fine-tuning job config\n",
    "        tune_params = {\n",
    "            'training_file': training_file_id,\n",
    "            'model': model,\n",
    "            'method': {\n",
    "                \"type\": \"supervised\",\n",
    "                \"supervised\": SupervisedMethod(\n",
    "                    hyperparameters=SupervisedHyperparameters(\n",
    "                        n_epochs=N_EPOCHS,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        learning_rate_multiplier=LR_MULT\n",
    "                    )\n",
    "                )\n",
    "            },\n",
    "            'seed': SEED,\n",
    "            'suffix': suffix\n",
    "        }\n",
    "\n",
    "        print(f\"Creating fine-tune job for {fname} on {model} with suffix {suffix}...\")\n",
    "        ft_resp = OPENAI_CLIENT.fine_tuning.jobs.create(**tune_params)\n",
    "        print(f\"  → Job {ft_resp.id} created, status: {ft_resp.status}\")\n",
    "        tasks_summary.append({\n",
    "            'model': model,\n",
    "            'dataset': fname,\n",
    "            'job_id': ft_resp.id,\n",
    "            'suffix': suffix,\n",
    "            'status': ft_resp.status\n",
    "        })"
   ],
   "id": "ca4ce9cf875093c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save all fine-tuning job info to a summary file\n",
    "summary_path = os.path.join(LLM_API, 'fine_tune_jobs_summary.json')\n",
    "with open(summary_path, 'w', encoding='utf-8') as sumf:\n",
    "    json.dump(tasks_summary, sumf, indent=2)\n",
    "print(f\"All jobs created. Summary saved to {summary_path}\")"
   ],
   "id": "708f94b4ce979785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### MISTRAL Models",
   "id": "317567ecae2252fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Upload all training files for Mistral\n",
    "mistral_file_ids = {}\n",
    "for path in JSONL_FILES:\n",
    "    fname = os.path.basename(path)\n",
    "    print(f\"Uploading {fname} to Mistral…\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        resp = MISTRAL_CLIENT.files.upload(file={\n",
    "            \"file_name\": fname,\n",
    "            \"content\": f\n",
    "        })\n",
    "    mistral_file_ids[fname] = path, resp.id\n",
    "    print(f\" -> {fname} → {resp.id}\")"
   ],
   "id": "71789ae7448412a3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "DESIRED_EPOCHS = 3      # Number of epochs to approximate for each run\n",
    "\n",
    "# Turn dict into a list\n",
    "all_mistral_files = list(mistral_file_ids.items())\n",
    "mistral_jobs = []\n",
    "\n",
    "# Fine-tune each Mistral model on each file in the batch\n",
    "for model in MISTRAL_MODELS:\n",
    "    # Compute slice for the current batch (see above)\n",
    "    start = SKIP_BATCH * BATCH_SIZE\n",
    "    end   = start + BATCH_SIZE\n",
    "    batch = all_mistral_files[start:end]\n",
    "\n",
    "    for fname, (local_path, file_id) in batch:\n",
    "        # Compute training steps as function of file size to match epochs\n",
    "        size_bytes = os.path.getsize(local_path)\n",
    "        size_mb    = size_bytes / (1024 * 1024)\n",
    "        training_steps = max(1, int(DESIRED_EPOCHS * size_mb))\n",
    "\n",
    "        hyperparams = {\n",
    "            \"training_steps\":   training_steps,     # ~Epochs × MB\n",
    "            \"learning_rate\":    1e-4,               # Standard starting LR\n",
    "        }\n",
    "\n",
    "        print(\n",
    "            f\"Creating Mistral FT job on {model} with {fname}: \"\n",
    "            f\"{training_steps} steps (~{DESIRED_EPOCHS} epochs)…\"\n",
    "        )\n",
    "        job = MISTRAL_CLIENT.fine_tuning.jobs.create(\n",
    "            model=model,\n",
    "            training_files=[{\"file_id\": file_id, \"weight\": 1}],\n",
    "            hyperparameters=hyperparams,\n",
    "            auto_start=True\n",
    "        )\n",
    "\n",
    "        print(f\" → job {job.id}, status={job.status}\")\n",
    "        mistral_jobs.append({\n",
    "            \"model\":         model,\n",
    "            \"dataset\":       fname,\n",
    "            \"file_size_mb\":  round(size_mb, 2),\n",
    "            \"training_steps\":training_steps,\n",
    "            \"job_id\":        job.id,\n",
    "            \"status\":        job.status\n",
    "        })\n",
    "\n",
    "# Save all Mistral job info to a summary file\n",
    "with open(os.path.join(LLM_API, \"mistral_fine_tune_summary.json\"), \"w\") as out:\n",
    "    json.dump(mistral_jobs, out, indent=2)"
   ],
   "id": "2127934acc7b752d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
