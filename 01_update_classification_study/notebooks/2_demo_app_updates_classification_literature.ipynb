{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "To replicate prior work, we incorporated the preprocessing steps, dictionaries and filters described in Kircher & Foerderer (2023) and Agarwal & Kapoor (2023), and we applied version logic similar to that used in Wen & Zhu (2019).\n",
    "\n",
    "(1) Kircher & Foerderer (2023) classify an update as innovative (feature update) if the release note contains at least one of the keywords “new”, “added”, “upgrade”, or “major”.\n",
    "\n",
    "(2) Agarwal & Kapoor (2023) classify an update as innovative (new functionality) if the release note exceeds 200 characters or contains at least one of the keywords “introduce”, “feature”, “support”, “performance”, “improve”, “enable”, “update”, “enhance”, “modify”, “optimize”, “fast”, “adjust”, or “multitask”. To account for differences in text representation and ensure consistency in the keyword matching process, we applied several preprocessing steps. We converted each release note to lowercase, tokenized it into individual words, and each word was reduced to its stem. The keywords in each dictionary were processed similarly.\n",
    "\n",
    "(3) In Wen & Zhu (2019), the baseline treats every update release as an innovative update. A version-number rule is applied in robustness checks, classifying an update as innovative (major update) if the <major> field in the app’s version number (first digit) changed. When replicating this approach, we additionally controlled for changes in the <minor> field in the app’s version number (second digit)."
   ],
   "id": "4dd0285d08a2016c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "cb14367f986a2752"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ],
   "id": "ae8f0c4a89daf399",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, Variables, and Classifier Instances",
   "id": "f2d5234710bf2d47"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "VAL_PATH   = os.path.join(DEMO_PATH,'training_validation_data', 'demo_app_updates_validation_real_1000.csv')\n",
    "OUTPUT_DIR = os.path.join(DEMO_PATH,'output_data')\n",
    "\n",
    "# Relevant column names\n",
    "COL_VERSION      = 'version_display'\n",
    "COL_PREV_VERSION = 'previous_version'\n",
    "COL_WHATS_NEW    = 'whats_new'\n",
    "\n",
    "# Dictionaries from literature\n",
    "DICT_SOURCES = {\n",
    "    'KF23': {\n",
    "        'innovation': [\"new\", \"added\", \"upgrade\", \"major\"],\n",
    "        'maintenance': [\"bug\", \"minor\", \"crash\", \"error\"],\n",
    "    },\n",
    "    'AK23': {\n",
    "        'innovation': [\n",
    "            \"new\", \"introduce\", \"feature\", \"add\", \"support\", \"performance\",\n",
    "            \"improve\", \"upgrade\", \"enable\", \"update\", \"enhance\", \"modify\",\n",
    "            \"optimize\", \"fast\", \"adjust\", \"multitask\"\n",
    "        ],\n",
    "        'maintenance': [\n",
    "            \"bug\", \"fix\", \"issue\", \"crash\", \"problem\", \"error\", \"glitch\"\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Preprocess stemmer & keywords once\n",
    "ps = PorterStemmer()\n",
    "STEMMED_KEYWORDS = {\n",
    "    name: {\n",
    "        'innovation': set(ps.stem(w) for w in kws['innovation']),\n",
    "        'maintenance': set(ps.stem(w) for w in kws['maintenance'])\n",
    "    }\n",
    "    for name, kws in DICT_SOURCES.items()\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataframe\n",
    "df = pd.read_csv(VAL_PATH)"
   ],
   "id": "46785718b999949c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "This section includes the main functions used for classifying software updates based dictionary-based classifications following Kircher & Foerderer (2023) and Agarwal and Kapoor (2023) and version-based classification following Wen & Zhu (2018)."
   ],
   "id": "839db79b1759ee30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dictionary-based classification\n",
    "We use keyword matching within the release notes (`whats_new`) to categorize updates as \"innovation\" or \"maintenance.\""
   ],
   "id": "8abf026bfd5849ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_text(text: str):\n",
    "    \"\"\"Tokenize and stem a string of text.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    return [ps.stem(tok) for tok in word_tokenize(text.lower())]\n",
    "\n",
    "def classify_whats_new(df: pd.DataFrame, source: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Classifies each row's 'what's new' text as 'innovation' or 'maintenance'\n",
    "    for a given source, based on stemmed keyword matching and length rules\n",
    "    \"\"\"\n",
    "    stemmed = STEMMED_KEYWORDS[source]\n",
    "    df['_toks'] = df[COL_WHATS_NEW].apply(preprocess_text)\n",
    "\n",
    "    def classify(row):\n",
    "        \"\"\"\n",
    "        Classifies a single row as innovation/maintenance based on:\n",
    "        - If 'AK23', marks as innovation if text is >200 chars.\n",
    "        - Otherwise, marks as innovation/maintenance if any stemmed keyword found.\n",
    "        \"\"\"\n",
    "        tokens = set(row['_toks'])\n",
    "        # Innovation if: (AK23 and long text) OR (any innovation keyword match)\n",
    "        is_innov = (\n",
    "            (source == 'AK23' and isinstance(row[COL_WHATS_NEW], str)\n",
    "             and len(row[COL_WHATS_NEW]) > 200)\n",
    "            or bool(tokens & stemmed['innovation'])\n",
    "        )\n",
    "        # Maintenance if not innovation AND at least one maintenance keyword match\n",
    "        is_maint = not is_innov and bool(tokens & stemmed['maintenance'])\n",
    "        return pd.Series({\n",
    "            f'{source}_innovation':  int(is_innov),\n",
    "            f'{source}_maintenance': int(is_maint),\n",
    "        })\n",
    "    # Classify each row and collect results in a new DataFrame\n",
    "    out = df.apply(classify, axis=1)\n",
    "\n",
    "    # Remove the temporary token column\n",
    "    df.drop(columns=['_toks'], inplace=True)\n",
    "\n",
    "    return pd.concat([df, out], axis=1)"
   ],
   "id": "38165f0dd4b54263",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Version-based classification\n",
    "We analyze version numbers to identify whether a release is a major or minor update, based on differences between the current (`version_display`) and previous version (`previous_version`).\n"
   ],
   "id": "7af91dc66dfb9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clean_version(v: str) -> str:\n",
    "    \"\"\"Clean a version string, keeping only digits and periods.\"\"\"\n",
    "    if pd.isna(v) or isinstance(v, float):\n",
    "        return ''\n",
    "    return re.sub(r'[^0-9.]', '', str(v))\n",
    "\n",
    "def split_to_parts(s: str):\n",
    "    \"\"\"Split a version string like '3.2.0' into a list of integers [3,2,0].\"\"\"\n",
    "    if not s:\n",
    "        return []\n",
    "    return [int(x) for x in s.split('.') if x.isdigit()]\n",
    "\n",
    "def add_version_flags(df: pd.DataFrame,\n",
    "                      version_col: str = 'version_display',\n",
    "                      prev_col: str    = 'previous_version'\n",
    "                     ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add columns to indicate major and minor version bumps:\n",
    "      - first_digit: 1 if there's a major bump (or full release with no prev)\n",
    "      - second_digit: 1 if there's a minor bump (and no major bump)\n",
    "    Logic depends on whether previous version info is present.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Clean version strings for both columns\n",
    "    df['_v_clean']  = df[version_col].apply(clean_version)\n",
    "    df['_ov_clean'] = df[prev_col].apply(clean_version)\n",
    "\n",
    "    # Split cleaned version strings into integer parts\n",
    "    df['_v_parts']  = df['_v_clean'].apply(split_to_parts)\n",
    "    df['_ov_parts'] = df['_ov_clean'].apply(split_to_parts)\n",
    "\n",
    "    # Extract major and minor version numbers, defaulting to 0 if missing\n",
    "    df['_v_major'] = df['_v_parts'].str[0].fillna(0).astype(int)\n",
    "    df['_v_minor'] = df['_v_parts'].str[1].fillna(0).astype(int)\n",
    "    df['_ov_major'] = df['_ov_parts'].str[0].fillna(0).astype(int)\n",
    "    df['_ov_minor'] = df['_ov_parts'].str[1].fillna(0).astype(int)\n",
    "\n",
    "    # Determine which rows have a previous version\n",
    "    has_prev = df['_ov_parts'].str.len() > 0\n",
    "    no_prev  = ~has_prev\n",
    "\n",
    "    # Compute version bumps when prev exists\n",
    "    first_with  = has_prev & (df['_v_major'] != df['_ov_major'])\n",
    "    second_with = has_prev & ~first_with & (df['_v_minor'] != df['_ov_minor'])\n",
    "\n",
    "    # For rows with NO previous version:\n",
    "    first_no = no_prev & df['_v_parts'].apply(lambda pts: len(pts)>=1 and all(p==0 for p in pts[1:])) # first = “full” release: only major or trailing zeros\n",
    "    second_no = no_prev & ~first_no & df['_v_parts'].apply(lambda pts: len(pts)>=2) # second = has at least two segments (and no first bump)\n",
    "\n",
    "    # Assign final flag columns (first wins over second if both would be true)\n",
    "    df['first_digit']  = (first_with  | first_no).astype(int)\n",
    "    df['second_digit'] = (second_with | second_no).astype(int)\n",
    "\n",
    "    # Cleanup\n",
    "    df.drop(columns=[\n",
    "        '_v_clean','_ov_clean','_v_parts','_ov_parts',\n",
    "        '_v_major','_v_minor','_ov_major','_ov_minor'\n",
    "    ], inplace=True)\n",
    "\n",
    "    return df"
   ],
   "id": "d6d2654b900d5dbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Execution\n",
    "We apply both classification strategies to the validation dataset."
   ],
   "id": "80898e7db3481ba5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute flags once\n",
    "df = add_version_flags(df, version_col='version_display', prev_col='previous_version')\n",
    "\n",
    "# Loop over dictionaries\n",
    "for src in DICT_SOURCES:\n",
    "    df = classify_whats_new(df, src)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "df.to_csv(os.path.join(OUTPUT_DIR, 'validation_literature_classification.csv'), index=False)"
   ],
   "id": "5a2ba32985ffb88b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
