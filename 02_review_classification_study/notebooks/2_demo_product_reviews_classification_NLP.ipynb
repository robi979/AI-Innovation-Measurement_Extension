{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "For benchmarking classical ML models, deep learning approaches, and transformer-based pre-trained language models against our LLM applications, we trained each with the same annotated training data as our LLM fine-tuning, albeit without the prompts. To identify the best-performing model setup, we conducted several iterations with different embedding approaches and hyperparameter settings.\n",
    "For the main analysis in Section 4 of the paper, we used representative training sets consisting of n = 2,000 reviews and labels. For the additional analysis in Online Appendix D, we varied the training data distribution (representative and balanced) and size (n = 100, 250, 500, 1,000). To ensure compatibility with scikit-learn, labels were transformed into multi-hot vectors. A fixed hold-out validation set (n = 1,000) was reserved for final evaluation."
   ],
   "id": "3b11b14e88cf3f96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "4b232129f7a5185c"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas  as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import optuna\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer, f1_score, classification_report, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from glove_vectorizer import GloveVectorizer\n",
    "from skorch.classifier import NeuralNetClassifier\n",
    "from textcnn import TorchTokenizer, TextCNN\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from optuna.samplers import TPESampler"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, Variables, and Classifier Instances",
   "id": "c4676aed5d5d2963"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths \n",
    "TRAIN_GLOB =os.path.join(DEMO_PATH,'training_validation_data','demo_product_reviews_train*.csv')\n",
    "VAL_PATH   = os.path.join(DEMO_PATH,'training_validation_data', 'demo_product_reviews_validation_real_1000.csv')\n",
    "OUTPUT_DIR = os.path.join(DEMO_PATH,'output_data')\n",
    "TABLE_DIR  = os.path.join(DEMO_PATH,'tables')\n",
    "FIG_DIR    = os.path.join(DEMO_PATH,'figures')\n",
    "GLOVE_DIR = os.path.abspath(os.path.join(\"..\", \"embedding_files\"))\n",
    "MODELS_DIR = os.path.join(DEMO_PATH,'models')\n",
    "\n",
    "# Define parallelism structure\n",
    "INNER_JOBS = 5\n",
    "OUTER_JOBS = 10\n",
    "\n",
    "# CV & reproducibility\n",
    "RANDOM_STATE = 94032\n",
    "CV     = KFold(n_splits=3, shuffle=True, random_state=RANDOM_STATE)\n",
    "SCORER = make_scorer(f1_score, average=\"macro\")\n",
    "\n",
    "# Define classifier instances\n",
    "CLASSIFIERS = {\n",
    "    \"RandomForest\":        RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=INNER_JOBS),\n",
    "    \"LogisticRegression\":  LogisticRegression(random_state=RANDOM_STATE, n_jobs=INNER_JOBS, max_iter=1000),\n",
    "    \"KNeighbors\":          KNeighborsClassifier(n_jobs=INNER_JOBS),\n",
    "    \"NaiveBayes\":          MultinomialNB(),\n",
    "    \"SVM\":                 SVC(random_state=RANDOM_STATE, probability=True),\n",
    "    \"XGBoost\":             XGBClassifier(random_state=RANDOM_STATE, n_jobs=INNER_JOBS, eval_metric=\"mlogloss\"),\n",
    "}\n",
    "\n",
    "# Wrap each classifier in OneVsRest\n",
    "OVR_CLASSIFIERS = {\n",
    "    model_name: OneVsRestClassifier(est, n_jobs=INNER_JOBS)\n",
    "    for model_name, est in CLASSIFIERS.items()\n",
    "}\n",
    "\n",
    "# Define hyperparameter ranges for each classifier\n",
    "CLASSIFIER_PARAM_TEMPLATES = {\n",
    "    \"RandomForest\": {\n",
    "        \"clf__estimator__n_estimators\"      : randint(50, 501),    # Number of trees (50-500).  Vary forest size to trade off bias (few trees) vs. variance & runtime (many trees).\n",
    "        \"clf__estimator__max_depth\"         : randint(5, 101),     # Maximum tree depth (5-100).  Controls model complexity; shallow trees generalise, deep trees capture nuance.\n",
    "        \"clf__estimator__min_samples_split\" : randint(2, 21),      # Min samples to split (2-20).  Larger values prevent very small, noisy splits.\n",
    "        \"clf__estimator__min_samples_leaf\"  : randint(1, 11),      # Min samples at leaf (1-10).  Smooths predictions and combats over-fitting on rare n-grams.\n",
    "        \"clf__estimator__max_features\"      : uniform(0.1, 0.9),   # Feature subsampling (10-90 %).  Forces diversity among trees; useful when TF-IDF has many correlated features.\n",
    "        \"clf__estimator__criterion\"         : [\"gini\", \"entropy\"], # Split impurity measure.  Both are common; entropy can capture class imbalance nuances.\n",
    "        \"clf__estimator__bootstrap\"         : [True, False],       # Use bootstrap samples or full data.  Testing both may improve stability for sparse text.\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"clf__estimator__C\"      : uniform(0.01, 10),              # Inverse reg. strength (0.01-10).  Explores from strong regularisation (simple model) to weak (complex).\n",
    "        \"clf__estimator__penalty\": [\"l2\"],                         # L2 penalty (stable for multiclass TF-IDF).  Kept fixed per literature best-practice.\n",
    "        \"clf__estimator__solver\" : [\"lbfgs\", \"saga\"],              # Optimisers: LBFGS (dense) vs. SAGA (sparse, large data).  Choice may affect convergence speed.\n",
    "    },\n",
    "    \"KNeighbors\": {\n",
    "        \"clf__estimator__n_neighbors\": randint(1, 31),             # k (1-30).  Smaller k captures local nuance; larger k smooths decision boundaries.\n",
    "        \"clf__estimator__weights\"    : [\"uniform\", \"distance\"],    # Voting scheme.  Distance weighting often helps when neighbours at differing distances.\n",
    "        \"clf__estimator__leaf_size\"  : randint(10, 51),            # Ball-tree leaf size (10-50).  Impacts search speed vs. memory for high-dim. TF-IDF vectors.\n",
    "    },\n",
    "\n",
    "    # NaiveBayes hyperparameters:\n",
    "    \"NaiveBayes\": {\n",
    "        \"clf__estimator__alpha\"    : uniform(1e-6, 1.0),           # Additive smoothing (1e-6-1).  Controls how aggressively rare terms are down-weighted.\n",
    "        \"clf__estimator__fit_prior\": [True, False],                # Learn class priors or assume uniform.  Testing both handles potential label imbalance.\n",
    "    },\n",
    "\n",
    "    # SVM hyperparameters:\n",
    "    \"SVM\": {\n",
    "        \"clf__estimator__C\"     : uniform(0.1, 10),                # Soft-margin cost (0.1-10).  Balances margin width vs. mis-classification tolerance.\n",
    "        \"clf__estimator__kernel\": [\"linear\", \"rbf\", \"poly\"],       # Kernels: linear (fast for TF-IDF), RBF & poly for non-linear patterns.\n",
    "        \"clf__estimator__gamma\" : [\"scale\", \"auto\"],               # Kernel coefficient heuristics.  Both common; effect only for RBF/poly.\n",
    "        \"clf__estimator__degree\": randint(2, 6),                   # Polynomial degree (2-5).  Higher degree leads to more complex decision surface (poly kernel only).\n",
    "    },\n",
    "\n",
    "    # XGBoost hyperparameters:\n",
    "    \"XGBoost\": {\n",
    "        \"clf__estimator__n_estimators\"     : randint(50, 501),     # Boosting rounds (50-500).  More rounds improve fit but risk overfitting & longer training.\n",
    "        \"clf__estimator__max_depth\"        : randint(3, 11),       # Tree depth (3-10).  Shallow trees reduce overfitting on sparse high-dim. text features.\n",
    "        \"clf__estimator__learning_rate\"    : uniform(0.01, 0.3),   # Shrinkage (0.01-0.3).  Lower LR needs more trees but can yield better generalisation.\n",
    "        \"clf__estimator__subsample\"        : uniform(0.5, 0.5),    # Row subsampling centred at 0.5.  Encourages diversity among trees; mitigates overfitting.\n",
    "        \"clf__estimator__colsample_bytree\" : uniform(0.5, 0.5),    # Column subsampling (~50 %).  Helpful with large TF-IDF vocabularies to speed up training.\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e59d02d642d931af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Helper Function to parse multi-hot vectors\n",
    "def load_multilabel_csv(path, sep=',', dtype=None):\n",
    "    \"\"\"Load features and multi-label targets from a CSV file.\"\"\"\n",
    "    df = pd.read_csv(path, sep=sep)\n",
    "    X = df['review_text_plain'].values\n",
    "    y = np.vstack(df['multi_hot'].map(json.loads))\n",
    "    if dtype is not None:\n",
    "        y = y.astype(dtype)\n",
    "    return X, y"
   ],
   "id": "f990e4432f4c05de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Classical ML models",
   "id": "e4b7f12de9be4c65"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF Pipeline\n",
    "To identify the optimal model configuration for review classification, we systematically tuned feature representations and classifier hyperparameters using an automated and reproducible pipeline via the scikit-learn (Pedregosa et al., 2011) and XGBoost  (Chen & Guestrin, 2016) libraries.\n",
    "We used scikit-learn’s `RandomizedSearchCV` with 50 randomly sampled parameter sets per classifier for hyperparameter optimization. Feature representations and classifiers were tuned jointly. All classifiers were wrapped with `OneVsRestClassifier`, and multi-hot encoded targets were used as labels."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a981320557a33c3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define hyperparameter search space for the TF-IDF vectorizer\n",
    "TFIDF_PARAMS = {\n",
    "    \"tfidf__max_features\" : randint(2000, 10001),      # Vocabulary size (2 k – 10 k).  Controls richness of feature set vs. sparsity and runtime.\n",
    "    \"tfidf__ngram_range\"  : [(1, 1), (1, 2), (1, 3)],  # Unigrams, bigrams or trigrams.  Enables both single words and frequent phrases as features.\n",
    "    \"tfidf__min_df\"       : randint(1, 5),             # Min-doc frequency (1–4).  Prunes extremely rare tokens that add noise or inflate vocab.\n",
    "    \"tfidf__max_df\"       : uniform(0.6, 0.4),         # Max-doc frequency (0.6–1.0).  Filters very common terms that carry little discriminative power.\n",
    "}\n",
    "\n",
    "# Combine TF-IDF and model-specific hyperparameters for joint optimization in RandomizedSearchCV\n",
    "PARAM_DISTS = {\n",
    "    model_name: { **TFIDF_PARAMS, **CLASSIFIER_PARAM_TEMPLATES[model_name] }\n",
    "    for model_name in OVR_CLASSIFIERS\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44c4d36b937b141f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Evaluation\n",
    "\n",
    "The following function `run_search_tfidf` handles model training, hyperparameter search, and evaluation on a fixed validation set. Cross-validation results are visualized."
   ],
   "id": "9bcae837e43ab88b"
  },
  {
   "cell_type": "code",
   "source": [
    "def run_search_tfidf(clf_name, train_path, val_path):\n",
    "    \"\"\"\n",
    "    Trains and tunes a multi-label pipeline using TF-IDF features and the specified classifier.\n",
    "    \"\"\"\n",
    "    dataset_name = os.path.splitext(os.path.basename(train_path))[0]\n",
    "    fig_sub = os.path.join(FIG_DIR, clf_name)\n",
    "    os.makedirs(fig_sub, exist_ok=True)\n",
    "\n",
    "    # Load training and validation data (text + multi-hot labels)\n",
    "    X_train, y_train = load_multilabel_csv(train_path)\n",
    "    X_val,   y_val   = load_multilabel_csv(val_path)\n",
    "\n",
    "    # Build sklearn pipeline: TF-IDF vectorizer followed by multi-label classifier (One-vs-Rest)\n",
    "    pipe = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"clf\", OVR_CLASSIFIERS[clf_name]),\n",
    "    ])\n",
    "\n",
    "    # Perform randomized search over TF-IDF and classifier parameters using cross-validation\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        param_distributions=PARAM_DISTS[clf_name],\n",
    "        n_iter=50,\n",
    "        scoring=SCORER,\n",
    "        cv=CV,\n",
    "        n_jobs=OUTER_JOBS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1,\n",
    "        refit=True,\n",
    "        error_score=0.0\n",
    "    )\n",
    "    rs.fit(X_train, y_train)\n",
    "\n",
    "    # Extract best CV score and optimal parameter combination\n",
    "    best_cv = rs.best_score_\n",
    "    best_params = {k: (v.item() if hasattr(v, \"item\") else v)\n",
    "                   for k, v in rs.best_params_.items()}\n",
    "\n",
    "    # Evaluate best model on the hold-out validation set\n",
    "    y_pred = rs.predict(X_val)\n",
    "    report = classification_report(\n",
    "        y_val, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    hold_macro = report['macro avg']['f1-score']\n",
    "    hold_wgtd = report['weighted avg']['f1-score']\n",
    "    hold_acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Compile summary dictionary with performance metrics and best parameters\n",
    "    summary = {\n",
    "        'dataset': dataset_name,\n",
    "        'model': clf_name,\n",
    "        'best_cv_macro_f1': best_cv,\n",
    "        'best_params': json.dumps(best_params),\n",
    "        'holdout_macro_f1': hold_macro,\n",
    "        'holdout_weighted_f1': hold_wgtd,\n",
    "        'holdout_accuracy': hold_acc\n",
    "    }\n",
    "    for label, m in report.items():\n",
    "        if label.isdigit():\n",
    "            summary[f'label_{label}_f1'] = m['f1-score']\n",
    "\n",
    "    # Plot histogram of mean CV scores from the search results\n",
    "    cv_df = pd.DataFrame(rs.cv_results_)\n",
    "    cv_df['dataset'] = dataset_name\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.hist(cv_df['mean_test_score'], bins=10, color='grey', edgecolor='white')\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    ax.set_xlabel('Mean (CV) Macro Avg. F1-Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(fig_sub, f\"tfidf_{dataset_name}_histogram.jpg\"),\n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    return summary, cv_df, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "180e1f420dec88b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execution\n",
    "We loop over all available training splits and classifiers, optimizing each configuration. Results and predictions are saved for further analysis."
   ],
   "id": "92eb958cbb2c1d43"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for results and predictions\n",
    "all_summaries = []\n",
    "cv_results_by_model = {name: [] for name in OVR_CLASSIFIERS}\n",
    "model_preds = {}\n",
    "\n",
    "# Loop through different training files and classifiers\n",
    "for train_csv in glob.glob(TRAIN_GLOB):\n",
    "    base       = os.path.splitext(os.path.basename(train_csv))[0]\n",
    "    split_type = base.split('_')[-2]\n",
    "    size       = base.split('_')[-1]\n",
    "\n",
    "    for name in OVR_CLASSIFIERS:\n",
    "        print(f\"Running {name} on {base}\")\n",
    "        summary, cv_df, y_pred = run_search_tfidf(name, train_csv, VAL_PATH)\n",
    "        all_summaries.append(summary)\n",
    "        cv_results_by_model[name].append(cv_df)\n",
    "        # Store predictions for this model and dataset combination\n",
    "        key = f\"tfidf__{split_type}__{size}__{name}\"\n",
    "        model_preds[key] = y_pred\n",
    "\n",
    "# Combine all summary metrics into a single DataFrame and export to CSV\n",
    "combined_df = pd.DataFrame(all_summaries)\n",
    "combined_df.to_csv(os.path.join(TABLE_DIR, 'tfidf_all_ovr_models_all_datasets_summary.csv'),index=False)\n",
    "\n",
    "# Append model predictions to the validation set and export results\n",
    "val_with_preds = pd.read_csv(VAL_PATH)\n",
    "for key, preds in model_preds.items():\n",
    "    val_with_preds[f\"{key}_pred\"] = [json.dumps(row.tolist()) for row in preds]\n",
    "val_with_preds.to_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0b529f061eb4278",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Glove Pipeline\n",
    "To benchmark word-embedding features, we convert each release note into a dense vector using pre-trained GloVe models.\n",
    "For every embedding size, we keep the vectors fixed and tune only the downstream classifier hyperparameters (identical search spaces as in the TF-IDF pipeline)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "13a01fb743585ba1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Define available GloVe models\n",
    "GLOVE_FILES = {\n",
    "    name: os.path.join(GLOVE_DIR, fname)\n",
    "    for name, fname in {\n",
    "        \"6B-100d\":\"glove.6B.100d.txt\",\n",
    "        \"6B-300d\":\"glove.6B.300d.txt\",\n",
    "        \"42B-300d\":\"glove.42B.300d.txt\",\n",
    "        \"840B-300d\":\"glove.840B.300d.txt\"\n",
    "    }.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71425af57919553b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load validation set\n",
    "X_val_text, y_val = load_multilabel_csv(VAL_PATH)\n",
    "\n",
    "# Fit one GloveVectorizer per embedding file\n",
    "glove_vecs = {}\n",
    "for name, path in GLOVE_FILES.items():\n",
    "    print(f\"[{name}] loading embeddings…\")\n",
    "    vec = GloveVectorizer(glove_path=path)\n",
    "    vec.fit(X_val_text)  # just loads the file\n",
    "    glove_vecs[name] = vec\n",
    "\n",
    "# Transform validation text once per embedding\n",
    "X_val_glove = {\n",
    "    name: vec.transform(X_val_text)\n",
    "    for name, vec in glove_vecs.items()\n",
    "}\n",
    "\n",
    "# Precompute train features for each size & each embedding\n",
    "X_train_glove = {name: {} for name in glove_vecs}\n",
    "y_train_glove = {}\n",
    "\n",
    "for train_csv in glob.glob(TRAIN_GLOB):\n",
    "    print(train_csv)\n",
    "    base       = os.path.splitext(os.path.basename(train_csv))[0]\n",
    "    parts      = base.split('_')            # e.g. [\"demo\", \"...\", \"train\", \"real\", \"2000\"]\n",
    "    split_type = parts[-2]                  # \"real\" or \"equal\"\n",
    "    size       = int(parts[-1])             # \"2000\"\n",
    "    key        = (split_type, size)\n",
    "\n",
    "    X_train_texts, y_train_multi = load_multilabel_csv(train_csv)\n",
    "    y_train_glove[key] = y_train_multi\n",
    "\n",
    "    for name, vec in glove_vecs.items():\n",
    "        X_train_glove[name][key] = vec.transform(X_train_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dae9eb616557d73e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Evaluation\n",
    "The helper function `run_search_glove` mirrors `run_search_tfidf`:\n",
    "it trains a pipeline on pre-computed GloVe features, performs `RandomizedSearchCV`, evaluates on the hold-out validation set, and returns summary metrics plus predictions."
   ],
   "id": "11ca94aa0a71ff0f"
  },
  {
   "cell_type": "code",
   "source": [
    "def run_search_glove(clf_name, glove_name, Xg, yg, X_val, y_val, split_type, size):\n",
    "    \"\"\"\n",
    "    Trains and tunes a multi-label classifier on a fixed GloVe embedding.\n",
    "    \"\"\"\n",
    "    dataset_name = f\"{glove_name}_{split_type}_{size}\"\n",
    "    fig_sub = os.path.join(FIG_DIR, glove_name, clf_name)\n",
    "    os.makedirs(fig_sub, exist_ok=True)\n",
    "\n",
    "    # Build sklearn pipeline with classifier (no vectorizer)\n",
    "    pipe = Pipeline([('clf', OVR_CLASSIFIERS[clf_name])])\n",
    "\n",
    "     # Perform randomized search over classifier parameters using cross-validation\n",
    "    rs = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=CLASSIFIER_PARAM_TEMPLATES[clf_name],\n",
    "        n_iter=50,\n",
    "        scoring=SCORER,\n",
    "        cv=CV,\n",
    "        n_jobs=OUTER_JOBS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=1,\n",
    "        refit=True\n",
    "    )\n",
    "    rs.fit(Xg, yg)\n",
    "\n",
    "    # Extract best CV score and optimal parameter combination\n",
    "    best_cv = rs.best_score_\n",
    "    best_params = {k: (v.item() if hasattr(v, \"item\") else v)\n",
    "                   for k, v in rs.best_params_.items()}\n",
    "\n",
    "    # Evaluate best model on the hold-out validation set\n",
    "    y_pred = rs.predict(X_val)\n",
    "\n",
    "    # Generate classification report and summarize key hold-out metrics\n",
    "    report = classification_report(\n",
    "        y_val, y_pred,\n",
    "        output_dict=True,\n",
    "        zero_division=0\n",
    "    )\n",
    "    hold_macro = report['macro avg']['f1-score']\n",
    "    hold_wgtd = report['weighted avg']['f1-score']\n",
    "    hold_acc = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    # Compile summary dictionary with performance metrics and best parameters\n",
    "    summary = {\n",
    "        'dataset': dataset_name,\n",
    "        'model': clf_name,\n",
    "        'vectorizer': f\"glove-{glove_name}\",\n",
    "        'best_cv_macro_f1': best_cv,\n",
    "        'best_params': json.dumps(best_params),\n",
    "        'holdout_macro_f1': hold_macro,\n",
    "        'holdout_weighted_f1': hold_wgtd,\n",
    "        'holdout_accuracy': hold_acc\n",
    "    }\n",
    "    for label, m in report.items():\n",
    "        if label.isdigit():\n",
    "            summary[f'label_{label}_f1'] = m['f1-score']\n",
    "\n",
    "    # Plot histogram of mean CV scores from the search results\n",
    "    cv_df = pd.DataFrame(rs.cv_results_)\n",
    "    cv_df['dataset'] = dataset_name\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.hist(\n",
    "        cv_df['mean_test_score'],\n",
    "        bins=10,\n",
    "        color='grey',\n",
    "        edgecolor='white'\n",
    "    )\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    ax.set_xlabel('Mean CV Macro Avg. F1-Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.grid(False)\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(fig_sub, f\"glove_{dataset_name}_histogram.jpg\"),\n",
    "        dpi=300, bbox_inches='tight'\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "    return summary, cv_df, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2527f57db18d8011",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execution\n",
    "\n",
    "We iterate over every **training split × GloVe model × classifier** combination.\n",
    "For each configuration we run the search, store cross-validation results, and append hold-out predictions for later analysis.\n"
   ],
   "id": "45a6a0d5b29ffd49"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for results and predictions\n",
    "all_summaries = []\n",
    "cv_results_by_model = {name: [] for name in OVR_CLASSIFIERS}\n",
    "model_preds = {}\n",
    "\n",
    "all_summaries = []\n",
    "cv_results_by_model = {name: [] for name in OVR_CLASSIFIERS}\n",
    "model_preds = {}\n",
    "\n",
    "for glove_name, size_dict in X_train_glove.items():\n",
    "    for (split_type, size), Xg in sorted(size_dict.items()):\n",
    "        yg = y_train_glove[(split_type, size)]\n",
    "\n",
    "        for clf_name in OVR_CLASSIFIERS:\n",
    "            if clf_name == \"NaiveBayes\":\n",
    "                print(f\"→ Skipping NaiveBayes on glove={glove_name}, {split_type} n={size}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Running {clf_name} on glove={glove_name}, {split_type} n={size}\")\n",
    "            summary, cv_df, y_pred = run_search_glove(\n",
    "                clf_name,\n",
    "                glove_name,\n",
    "                Xg, yg,\n",
    "                X_val_glove[glove_name],\n",
    "                y_val,\n",
    "                split_type,\n",
    "                size\n",
    "            )\n",
    "            all_summaries.append(summary)\n",
    "            cv_results_by_model[clf_name].append(cv_df)\n",
    "            key = f\"{glove_name}_{split_type}_{size}__{clf_name}\"\n",
    "            model_preds[key] = y_pred\n",
    "\n",
    "# Combine summary metrics into a single DataFrame and export to CSV\n",
    "glove_df = pd.DataFrame(all_summaries)\n",
    "glove_df.to_csv(os.path.join(TABLE_DIR, 'glove_all_models_all_datasets_summary.csv'), index=False)\n",
    "\n",
    "# Merge GloVe predictions into the existing TF-IDF output file\n",
    "val_with_preds = pd.read_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv')) # Assuming tf-idf run before\n",
    "for key, preds in model_preds.items():\n",
    "    val_with_preds[f\"{key}_pred\"] = [json.dumps(row.tolist()) for row in preds]\n",
    "val_with_preds.to_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "130f5040408d333c",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Convolutional Neural Network\n",
    "We evaluate a convolutional neural network (TextCNN) that leverages the large 42 B-token, 300-d GloVe embeddings. For every training split, we tune CNN hyper-parameters with `RandomizedSearchCV` and evaluate on the fixed validation set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4713aca31c238a5"
  },
  {
   "cell_type": "code",
   "source": [
    "# Path to the 42 B-token GloVe file (300-d vectors)\n",
    "GLOVE_42B_PATH = GLOVE_FILES[\"42B-300d\"]  # Important: need to load GLOVE_FILES before\n",
    "\n",
    "# Define hyper-parameter search space for the TextCNN pipeline\n",
    "CNN_PARAM_DIST = {\n",
    "    \"tok__num_words\"           : randint(10_000, 30_001),  # Vocabulary size (10–30 k).  Balances lexical coverage against memory & training time.\n",
    "    \"tok__seq_len\"             : randint(100, 301),        # Max sequence length (100–300).  Truncates / pads texts; trades context for speed.\n",
    "    \"clf__module__n_filters\"   : randint(64, 257),         # Conv-filter count (64–256).  Controls capacity to capture local n-gram patterns.\n",
    "    \"clf__module__kernel_size\" : randint(3, 9),            # Kernel width (3–8).  Learns n-gram windows from tri-grams to octo-grams.\n",
    "    \"clf__module__dense_units\" : randint(32, 257),         # Dense layer width (32–256).  Governs representational power before softmax.\n",
    "    \"clf__module__dropout\"     : uniform(0.1, 0.4),        # Dropout rate (0.1–0.5).  Regularises network to mitigate over-fitting.\n",
    "    \"clf__module__trainable\"   : [False, True],            # Freeze vs. fine-tune embeddings.  Tests benefit of adapting GloVe vectors.\n",
    "    \"clf__batch_size\"          : randint(32, 129),         # Mini-batch size (32–128).  Larger batches speed training but need more GPU RAM.\n",
    "    \"clf__max_epochs\"          : randint(5, 16),           # Training epochs (5–15).  Allows early stopping to pick optimal training length.\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95229de6ff17707e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Embedding Matrix Helper\n",
    "Utility to construct a PyTorch tensor from the 42 B GloVe file for the current vocabulary."
   ],
   "id": "85ddf861a786599a"
  },
  {
   "cell_type": "code",
   "source": [
    "def glove42b_matrix(vocab, glove_path, dim=300):\n",
    "    \"\"\"Return embedding matrix aligned with `vocab`.\"\"\"\n",
    "    mat = np.random.normal(0, .05, (len(vocab), dim)).astype('float32')\n",
    "    with open(glove_path, encoding='utf-8') as fh:\n",
    "        for line in fh:\n",
    "            word, *vec = line.rstrip().split()\n",
    "            if word in vocab.stoi:\n",
    "                mat[vocab.stoi[word]] = np.array(vec, dtype='float32')\n",
    "    return torch.tensor(mat)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6725301a86e4d667",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Evaluation\n",
    "The helper `run_search_cnn` mirrors the TF-IDF and GloVe routines:\n",
    "it builds a `Pipeline(tok, TextCNN)`, performs random search, evaluates on the hold-out set, and returns metrics plus predictions."
   ],
   "id": "a2380f83f29ae3e5"
  },
  {
   "cell_type": "code",
   "source": [
    "def run_search_cnn(train_path, val_path, glove_path):\n",
    "    \"\"\" Train & tune TextCNN with fixed 42B GloVe embeddings.\"\"\"\n",
    "    dataset = os.path.splitext(os.path.basename(train_path))[0]\n",
    "    fig_sub = os.path.join(FIG_DIR, \"TextCNN42B\")\n",
    "    os.makedirs(fig_sub, exist_ok=True)\n",
    "\n",
    "    # Load training and validation data (text + multi-hot labels)\n",
    "    X_train, y_train = load_multilabel_csv(train_path, dtype=np.float32)\n",
    "    X_val, y_val = load_multilabel_csv(val_path, dtype=np.float32)\n",
    "\n",
    "    # Tokenize training text and build embedding matrix\n",
    "    tok = TorchTokenizer().fit(X_train)\n",
    "    emb = glove42b_matrix(tok.vocab, glove_path)\n",
    "\n",
    "    # Extract number of labels (9)\n",
    "    n_labels = y_train.shape[1]\n",
    "\n",
    "    # Wrap TextCNN in skorch and build sklearn pipeline\n",
    "    net = NeuralNetClassifier(\n",
    "        module=TextCNN,\n",
    "        module__vocab_size=len(tok.vocab),\n",
    "        module__emb_matrix=emb,\n",
    "        module__n_classes=n_labels,\n",
    "        criterion=nn.BCEWithLogitsLoss,\n",
    "        optimizer=torch.optim.Adam,\n",
    "        iterator_train__shuffle=True,\n",
    "        train_split=None,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    # Perform randomized search over CNN hyper-parameters\n",
    "    pipe = Pipeline([('tok', tok), ('clf', net)])\n",
    "    rs = RandomizedSearchCV(\n",
    "        pipe,\n",
    "        CNN_PARAM_DIST,\n",
    "        n_iter=30,\n",
    "        scoring=SCORER,\n",
    "        cv=CV,\n",
    "        n_jobs=OUTER_JOBS,\n",
    "        random_state=RANDOM_STATE,\n",
    "        verbose=2,\n",
    "        refit=True,\n",
    "        error_score='raise'\n",
    "    ).fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate best model on the hold-out validation set using direct forward + sigmoid\n",
    "    X_val_tensor = torch.tensor(tok.transform(X_val), dtype=torch.long)\n",
    "    model = rs.best_estimator_.named_steps['clf'].module_\n",
    "    device = rs.best_estimator_.named_steps['clf'].device\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Logits: (n_samples, n_labels)\n",
    "        logits = model(X_val_tensor.to(device))\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "    y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "    y_val_int = y_val.astype(int)\n",
    "    rpt = classification_report(y_val_int, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Compile summary dictionary with performance metrics & best params\n",
    "    summary = {\n",
    "        'dataset': dataset,\n",
    "        'model': \"TextCNN42B\",\n",
    "        'best_cv_macro_f1': rs.best_score_,\n",
    "        'best_params': json.dumps({k:(v.item() if hasattr(v,'item') else v)\n",
    "                                   for k, v in rs.best_params_.items()}),\n",
    "        'holdout_macro_f1': rpt['macro avg']['f1-score'],\n",
    "        'holdout_weighted_f1': rpt['weighted avg']['f1-score'],\n",
    "        'holdout_accuracy': accuracy_score(y_val_int, y_pred)\n",
    "    }\n",
    "    for lbl, m in rpt.items():\n",
    "        if str(lbl).isdigit():\n",
    "            summary[f'label_{lbl}_f1'] = m['f1-score']\n",
    "\n",
    "    # Plot histogram of mean CV scores from the search results\n",
    "    cv_df = pd.DataFrame(rs.cv_results_)\n",
    "    cv_df['dataset'] = dataset\n",
    "    fig, ax = plt.subplots(figsize=(8,5))\n",
    "    ax.hist(cv_df['mean_test_score'], bins=10, edgecolor='white')\n",
    "    ax.yaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    ax.set_xlabel('Mean CV Macro Avg. F1-Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    plt.savefig(os.path.join(fig_sub, f\"cnn_{dataset}_hist.jpg\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "        # Save the best model parameters for reuse\n",
    "    os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "    best_net = rs.best_estimator_.named_steps['clf']\n",
    "    model_path = os.path.join(MODELS_DIR, f\"TextCNN42B_{dataset}.pt\")\n",
    "    best_net.save_params(f_params=model_path)\n",
    "    print(f\"Saved best model to {model_path}\")\n",
    "\n",
    "    return summary, cv_df, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4e6b6ba501f142bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execution\n",
    "We loop over every training split, run `run_search_cnn`, and append predictions for downstream comparison."
   ],
   "id": "4091824b6a80ddf1"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for results and predictions\n",
    "cnn_summaries = []\n",
    "cnn_cv_results = []\n",
    "model_preds     = {}\n",
    "\n",
    "# Loop through different training files\n",
    "for train_csv in glob.glob(TRAIN_GLOB):\n",
    "    base = os.path.splitext(os.path.basename(train_csv))[0]\n",
    "    parts = base.split('_')  # parts == [\"demo\",\"app\",\"updates\",\"train\",\"real\",\"2000\"]\n",
    "    split_type = parts[-2]   # \"real\" or \"equal\"\n",
    "    size       = parts[-1]   # \"2000\", etc.\n",
    "    print(f\"[CNN] TextCNN42B on {train_csv}\")\n",
    "\n",
    "    summary, cv_df, y_pred = run_search_cnn(train_csv, VAL_PATH, GLOVE_42B_PATH)\n",
    "    cnn_summaries.append(summary)\n",
    "    cnn_cv_results.append(cv_df)\n",
    "    key = f\"TextCNN42B__{split_type}__{size}\"\n",
    "    model_preds[key] = y_pred\n",
    "\n",
    "# Combine all summary metrics into a single DataFrame and export to CSV\n",
    "cnn_df = pd.DataFrame(cnn_summaries)\n",
    "cnn_df.to_csv(\n",
    "    os.path.join(TABLE_DIR, \"cnn_textcnn42b_all_datasets_summary.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# Merge CNN predictions into the existing output file\n",
    "val_with_preds = pd.read_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv')) # Assuming other models run before\n",
    "for key, preds in model_preds.items():\n",
    "    val_with_preds[f\"{key}_pred\"] = [json.dumps(row.tolist()) for row in preds]\n",
    "val_with_preds.to_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "282fc52df875c45a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pre-Trained Language Models\n",
    "We fine-tune four transformer checkpoints (`BERT`, `RoBERTa`, `XLNet`, `ELECTRA`) on each train split.\n",
    "Hyper-parameters are tuned with **Optuna** (`NUM_TRIALS = 5`) and the best model is evaluated on the houldout validation set."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "236bc04a6d424677"
  },
  {
   "cell_type": "code",
   "source": [
    "# Hugging Face model checkpoints\n",
    "MODEL_CHECKPOINTS = {\n",
    "    \"xlnet\"   : \"xlnet-base-cased\",\n",
    "    \"roberta\" : \"roberta-base\",\n",
    "    \"electra\" : \"google/electra-base-discriminator\",\n",
    "    \"bert\"    : \"bert-base-cased\",\n",
    "}\n",
    "\n",
    "# Base TrainingArguments shared by all fine-tuning runs\n",
    "BASE_ARGS = {\n",
    "    \"eval_strategy\"           : \"epoch\",   # Evaluate once per epoch for clear learning curves.\n",
    "    \"save_strategy\"           : \"epoch\",   # Save a checkpoint after every epoch for rollback.\n",
    "    \"load_best_model_at_end\"  : True,      # Restore best epoch automatically.\n",
    "    \"metric_for_best_model\"   : \"f1_macro\",# Macro-F1 chosen to weight classes equally.\n",
    "    \"greater_is_better\"       : True,      # Higher F1 = better.\n",
    "    \"seed\"                    : RANDOM_STATE,\n",
    "    \"logging_steps\"           : 50,        # Frequent logging; low overhead on modern GPUs.\n",
    "    \"save_total_limit\"        : 1,         # Keep only the best checkpoint → disk-friendly.\n",
    "    \"disable_tqdm\"            : True,      # Cleaner notebook output.\n",
    "    \"report_to\"               : [],        # Disable WandB/MLflow unless explicitly enabled.\n",
    "}\n",
    "\n",
    "# Optuna trials per checkpoint\n",
    "NUM_TRIALS = 5"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d4c79873d0f77b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Helper: Load CSVs as HuggingFace Datasets\n",
    "Zero-bases the labels so they align with `transformers` expectations."
   ],
   "id": "9fca1466f9d8148e"
  },
  {
   "cell_type": "code",
   "source": [
    "def load_datasets_HF(train_csv, val_csv):\n",
    "    \"\"\"Return Hugging Face Datasets for train / validation splits.\"\"\"\n",
    "    df_tr = pd.read_csv(train_csv)\n",
    "    df_va = pd.read_csv(val_csv)\n",
    "\n",
    "    # Parse JSON multi_hot → list[int]\n",
    "    df_tr['labels'] = df_tr['multi_hot'].map(json.loads)\n",
    "    df_va['labels'] = df_va['multi_hot'].map(json.loads)\n",
    "\n",
    "    # Cast each list[int] → list[float]\n",
    "    df_tr['labels'] = df_tr['labels'].apply(lambda lst: [float(x) for x in lst])\n",
    "    df_va['labels'] = df_va['labels'].apply(lambda lst: [float(x) for x in lst])\n",
    "\n",
    "    # Build HF Datasets\n",
    "    train_ds = Dataset.from_pandas(df_tr[['body','labels']])\n",
    "    val_ds   = Dataset.from_pandas(df_va[['body','labels']])\n",
    "    return train_ds, val_ds"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ddf1549b0c9f75d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Model Training and Hyperparameter Search\n",
    "`run_search_transformer` performs an Optuna search, saves the best model, and returns metrics plus predictions."
   ],
   "id": "53392e835b552393"
  },
  {
   "cell_type": "code",
   "source": [
    "def run_search_transformer(model_key, checkpoint, train_csv, val_csv):\n",
    "    \"\"\"\n",
    "    Fine-tune a transformer checkpoint, tune HPs with Optuna, evaluate on hold-out.\n",
    "    Returns summary dict, Optuna trials DataFrame, and validation predictions.\n",
    "    \"\"\"\n",
    "    split_name = os.path.basename(train_csv).rsplit('.', 1)[0]\n",
    "    train_ds, val_ds = load_datasets_HF(train_csv, val_csv)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    # Tokenization\n",
    "    def preprocess(batch):\n",
    "        toks = tok(batch['body'], truncation=True, padding='max_length', max_length=512)\n",
    "        toks[\"labels\"] = batch[\"labels\"]\n",
    "        return toks\n",
    "\n",
    "    train_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "    val_ds   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    # Get number of labels (9)\n",
    "    n_labels = len(train_ds[0]['labels'])\n",
    "\n",
    "    # Optuna setup\n",
    "    sampler = TPESampler(seed=RANDOM_STATE)\n",
    "    study   = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "\n",
    "    def compute_metrics(p):\n",
    "        # Multi-label: sigmoid + threshold\n",
    "        logits = p.predictions\n",
    "        probs  = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "        y_pred = (probs >= 0.5).astype(int)\n",
    "        y_true = p.label_ids\n",
    "        return {\n",
    "            \"f1_macro\":    f1_score(y_true, y_pred, average=\"macro\"),\n",
    "            \"f1_weighted\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "            \"accuracy\":    accuracy_score(y_true, y_pred),\n",
    "        }\n",
    "\n",
    "    def objective(trial):\n",
    "        # Set hyperparameters\n",
    "        lr     = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)  # Learning rate (1e-5–5e-5).  Standard fine-tune range; log-scale search.\n",
    "        bs     = trial.suggest_categorical(\"batch_size\", [8, 16])            # Batch size 8/16.  16 fits 12–16 GB GPUs; 8 for memory-constrained cases.\n",
    "        epochs = trial.suggest_int(\"num_train_epochs\", 3, 5)                 # Training epochs (3–5).  Balances convergence vs. over-fitting.\n",
    "        wd     = trial.suggest_float(\"weight_decay\", 0.0, 0.01)              # L2 weight decay (0–0.01).  Light regularisation for stability.\n",
    "\n",
    "        trial_dir = os.path.join(MODELS_DIR, f\"{model_key}_{split_name}\", f\"trial_{trial.number}\")\n",
    "        os.makedirs(trial_dir, exist_ok=True)\n",
    "\n",
    "        # Combine into TrainingArguments\n",
    "        args = TrainingArguments(\n",
    "            output_dir=trial_dir,\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bs,\n",
    "            per_device_eval_batch_size=bs,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=wd,\n",
    "            **BASE_ARGS\n",
    "        )\n",
    "        # Load config with multi-label\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            checkpoint,\n",
    "            num_labels=n_labels,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, config=config)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=val_ds,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        trial.set_user_attr(\"best_ckpt\", trainer.state.best_model_checkpoint)\n",
    "\n",
    "        # Cleanup GPU mem\n",
    "        del trainer, model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return metrics[\"eval_f1_macro\"]\n",
    "\n",
    "    # Run Optuna\n",
    "    study.optimize(objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "    # Save all trial results + best flag\n",
    "    trials_df = study.trials_dataframe()\n",
    "    trials_df[\"is_best\"] = trials_df[\"number\"] == study.best_trial.number\n",
    "    trials_out = os.path.join(TABLE_DIR, f\"{model_key}_{split_name}_optuna_trials.csv\")\n",
    "    trials_df.to_csv(trials_out, index=False)\n",
    "    print(f\"Saved {len(trials_df)} trials to {trials_out} (best trial = {study.best_trial.number})\")\n",
    "\n",
    "    # Reload & save best model centrally\n",
    "    best_ckpt = study.best_trial.user_attrs[\"best_ckpt\"]\n",
    "    master_dir = os.path.join(MODELS_DIR, f\"{model_key}_{split_name}_best\")\n",
    "    os.makedirs(master_dir, exist_ok=True)\n",
    "    \n",
    "    best_cfg = AutoConfig.from_pretrained(best_ckpt)\n",
    "    best_model = AutoModelForSequenceClassification.from_pretrained(best_ckpt, config=best_cfg)\n",
    "    best_model.config.problem_type = \"multi_label_classification\"\n",
    "    best_tok   = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    best_model.save_pretrained(master_dir)\n",
    "    best_tok.save_pretrained(master_dir)\n",
    "\n",
    "    # Compute per-label F1 on hold-out\n",
    "    eval_trainer = Trainer(\n",
    "        model=best_model,\n",
    "        args=TrainingArguments(output_dir=master_dir, per_device_eval_batch_size=study.best_params[\"batch_size\"]),\n",
    "        tokenizer=best_tok,\n",
    "    )\n",
    "    preds_out = eval_trainer.predict(val_ds)\n",
    "    logits = preds_out.predictions\n",
    "    probs  = torch.sigmoid(torch.tensor(logits)).numpy()\n",
    "    y_pred  = (probs >= 0.5).astype(int)\n",
    "    y_true    = preds_out.label_ids\n",
    "    rpt = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "    # Compile summary dictionary with performance metrics and best parameters\n",
    "    summary = {\n",
    "        \"model\": model_key,\n",
    "        \"train_split\": split_name,\n",
    "        \"best_f1\": study.best_value,\n",
    "        \"best_params\":         json.dumps(study.best_trial.params),\n",
    "        \"holdout_accuracy\":     accuracy_score(y_true, y_pred),\n",
    "        \"holdout_macro_f1\":     rpt[\"macro avg\"][\"f1-score\"],\n",
    "        \"holdout_weighted_f1\":  rpt[\"weighted avg\"][\"f1-score\"],\n",
    "    }\n",
    "    for lbl, m in rpt.items():\n",
    "        if lbl.isdigit():\n",
    "            summary[f\"label_{lbl}_f1\"] = m[\"f1-score\"]\n",
    "\n",
    "    return summary, trials_df, y_pred"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d45dec0e8b4e8ac3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Execution\n",
    "Loop over every **train split × checkpoint** combination, run `run_search_transformer`, and append predictions for downstream comparison."
   ],
   "id": "65439250c290a2e3"
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize containers for results and predictions\n",
    "all_results = []\n",
    "model_preds = {}\n",
    "\n",
    "# Loop through different training files and transformer checkpoints\n",
    "for train_csv in glob.glob(TRAIN_GLOB):\n",
    "    # Extract split_type and size from the filename\n",
    "    base       = os.path.splitext(os.path.basename(train_csv))[0]\n",
    "    parts      = base.split('_')       # e.g. [\"demo\",\"app\",\"updates\",\"train\",\"real\",\"2000\"]\n",
    "    split_type = parts[-2]             # \"real\" or \"equal\"\n",
    "    size       = parts[-1]             # \"2000\", etc.\n",
    "\n",
    "    for model_key, checkpoint in MODEL_CHECKPOINTS.items():\n",
    "        print(f\"[ML-FT] {model_key} on {split_type} n={size}\")\n",
    "        summary, trials_df, y_pred = run_search_transformer(\n",
    "            model_key,\n",
    "            checkpoint,\n",
    "            train_csv,\n",
    "            VAL_PATH\n",
    "        )\n",
    "        all_results.append(summary)\n",
    "\n",
    "        # Build a key like \"roberta_real_2000\"\n",
    "        key = f\"{model_key}_{split_type}_{size}\"\n",
    "        model_preds[key] = y_pred\n",
    "\n",
    "# Combine all summary metrics into a single DataFrame and export to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(os.path.join(TABLE_DIR, \"transformers_optuna_ml_summary.csv\"), index=False)\n",
    "\n",
    "# Merge PLM predictions into the existing output file\n",
    "val_with_preds = pd.read_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv')) # Assuming other models run before\n",
    "for key, preds in model_preds.items():\n",
    "    val_with_preds[f\"{key}_pred\"] = [json.dumps(row.tolist()) for row in preds]\n",
    "val_with_preds.to_csv(os.path.join(OUTPUT_DIR, 'validation_ovr_with_model_preds_NLP.csv'), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca336340ba55a9b0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
