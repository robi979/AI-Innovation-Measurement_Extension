{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "To evaluate large language model (LLM) performance at scale, we leveraged batch inference APIs from all three model providers included in the experiments—OpenAI, Mistral, and Anthropic—across all design decision variations.\n",
    "For each provider, a systematic procedure was implemented to generate, upload, and execute batch requests for both base and fine-tuned models, adhering to the respective provider’s API documentation."
   ],
   "id": "730efb46c3f2200c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Imports\n",
    " See `requirements.txt` for full dependency versions"
   ],
   "id": "5ad8f49c7051a0fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "from anthropic import Anthropic\n",
    "from anthropic.types.message_create_params import MessageCreateParamsNonStreaming\n",
    "from anthropic.types.messages.batch_create_params import Request\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "e9da44f648cc492d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Global Paths, Directories, and Variables",
   "id": "e8b57b9ff80eae6c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Demo Study path\n",
    "DEMO_PATH   = os.path.abspath(os.path.join(\"..\"))\n",
    "\n",
    "# Define relevant paths\n",
    "API_KEY_DIR = os.path.expanduser(os.getenv(\"API_KEY_DIR\", 'PATH')) # insert path to .txt with API Key\n",
    "PROMPT_DIR = os.path.join(DEMO_PATH, 'LLM_API', 'prompt_templates')\n",
    "LLM_API = os.path.join(DEMO_PATH,'LLM_API')\n",
    "VAL_PATH   = os.path.join(DEMO_PATH,'training_validation_data', 'demo_product_reviews_validation_real_1000.csv')\n",
    "\n",
    "OPENAI_BATCH_DIR = os.path.join(LLM_API, 'OpenAI_batches', 'raw')\n",
    "MISTRAL_BATCH_DIR = os.path.join(LLM_API, 'Mistral_batches', 'raw')\n",
    "ANTHROPIC_BATCH_DIR = os.path.join(LLM_API, 'Anthropic_batches')\n",
    "\n",
    "OPENAI_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'OpenAI_batches', \"results\")\n",
    "MISTRAL_BATCH_RESULTS_DIR = os.path.join(LLM_API, 'Mistral_batches', 'results')\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 94032\n",
    "\n",
    "# Max lines per batch (just redundancy, as batch per model size shouldn't exceed this)\n",
    "MAX_LINES_PER_BATCH = 50000\n",
    "\n",
    "# Load API keys\n",
    "with open(os.path.join(API_KEY_DIR, \"anthropic_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: ANTHROPIC_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"mistral_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: MISTRAL_API_KEY = f.read().strip()\n",
    "with open(os.path.join(API_KEY_DIR, \"openai_api_key_ai-measurement.txt\"), encoding=\"utf-8\") as f: OPENAI_API_KEY = f.read().strip()\n",
    "\n",
    "# Define API-relevant URLs and clients\n",
    "MISTRAL_CLIENT =  Mistral(api_key=MISTRAL_API_KEY)\n",
    "OPENAI_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
    "ANTHROPIC_CLIENT = Anthropic(api_key=ANTHROPIC_API_KEY)\n",
    "\n",
    "### Models for API-requests\n",
    "# OpenAI GPT models (default set)\n",
    "GPT_MODELS = [\n",
    "    \"gpt-3.5-turbo-0125\",\n",
    "    \"gpt-4o-mini-2024-07-18\",\n",
    "    \"gpt-4o-2024-08-06\",\n",
    "    \"gpt-4.1-2025-04-14\",\n",
    "    \"gpt-4.1-mini-2025-04-14\",\n",
    "    \"gpt-4.1-nano-2025-04-14\",\n",
    "    \"o3-2025-04-16\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "]\n",
    "\n",
    "# Of those, the “o-series” that don't use temperature:\n",
    "GPT_REASONING_MODELS = [\n",
    "    \"o3-2025-04-16\",\n",
    "    \"o4-mini-2025-04-16\",\n",
    "]\n",
    "\n",
    "# OpenAI GPT models (fine-tuned set -> run 3-2 notebook before)\n",
    "GPT_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"GPT_fine-tuned.txt\")\n",
    "\n",
    "\n",
    "# Mistral models (default set)\n",
    "MISTRAL_MODELS = [\n",
    "    \"mistral-large-2411\",\n",
    "    \"mistral-medium-2505\",\n",
    "    \"mistral-small-2503\",\n",
    "    \"open-mistral-nemo-2407\",\n",
    "    \"ministral-8b-2410\",\n",
    "    \"ministral-3b-2410\",\n",
    "]\n",
    "\n",
    "# Mistral models (fine-tuned set -> run 3-2 notebook before)\n",
    "MISTRAL_FT_MODELS_PATH = os.path.join(LLM_API, \"fine-tuned_models\", \"MISTRAL_fine-tuned.txt\")\n",
    "\n",
    "\n",
    "# Anthropic Claude models (default set)\n",
    "CLAUDE_MODELS = [\n",
    "    \"claude-sonnet-4-20250514\",\n",
    "    \"claude-3-7-sonnet-20250219\",\n",
    "    \"claude-opus-4-20250514\",\n",
    "    \"claude-3-5-haiku-20241022\",\n",
    "    \"claude-3-5-sonnet-20241022\",\n",
    "    \"claude-3-haiku-20240307\",\n",
    "    \"claude-3-opus-20240229\",\n",
    "]   # note reasoning models can easily get very expensive\n",
    "\n",
    "# Load prompt templates\n",
    "PROMPT_FILES = {\n",
    "    \"default\":         \"updates_prompt_default.txt\",\n",
    "    \"few_shot\":        \"updates_prompt_few-shot.txt\",\n",
    "    \"automatic_cot\":   \"updates_prompt_automatic-cot.txt\",\n",
    "    \"manual_cot\":      \"updates_prompt_manual-cot.txt\",\n",
    "    \"contrastive_cot\": \"updates_prompt_contrastive-cot.txt\",\n",
    "}\n",
    "\n",
    "PROMPTS = {\n",
    "    key: open(os.path.join(PROMPT_DIR, fname), encoding=\"utf-8\").read().strip()\n",
    "    for key, fname in PROMPT_FILES.items()\n",
    "}\n",
    "\n",
    "# Other settings\n",
    "TEMPERATURE_RANGE = [0, 0.5, 1.0, 1.5]\n",
    "REPEATED_RUNS = 3"
   ],
   "id": "f36a47c54a96be45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation data\n",
    "df = pd.read_csv(VAL_PATH)"
   ],
   "id": "753b56a08691a1b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## OpenAI Batches\n",
    "We generate JSONL batch files for **default** and **fine-tuned** GPT models, upload them, and create 24 h batch-jobs via the OpenAI API."
   ],
   "id": "8ccc17a5392d0567"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per GPT model. Run either Default oder Fine-Tuned Models."
   ],
   "id": "16aee42d8ac2836a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "a1b1bb78ff0c24a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in GPT_MODELS:\n",
    "    # Initialize container for model’s entries\n",
    "    entries = []\n",
    "    # Iterate over every prompt template\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        # o-series ignores temperature; others get sweep on default prompt\n",
    "        if model in GPT_REASONING_MODELS:\n",
    "            temps = [None]\n",
    "        else:\n",
    "            temps = TEMPERATURE_RANGE if prompt_name == 'default' else [None]\n",
    "\n",
    "        for temp in temps:\n",
    "            for run in range(1, REPEATED_RUNS + 1):\n",
    "                for _, row in df.iterrows():\n",
    "                    # Compose user prompt\n",
    "                    review_text = (row.get('review_text_tagged', '') or '').strip()\n",
    "                    prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "                    # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                    custom_id = (\n",
    "                        f\"02__\"\n",
    "                        f\"{model}__{prompt_name}\"\n",
    "                        f\"__t{int((temp or 0) * 10)}__run{run}__review_id{row['review_id']}\"\n",
    "                    )\n",
    "                    body = {\n",
    "                        'model': model,\n",
    "                        'messages': [{'role': 'user', 'content': prompt}],\n",
    "                        'max_completion_tokens': 1000,\n",
    "                        'seed': SEED\n",
    "                    }\n",
    "                    if temp is not None:\n",
    "                        body['temperature'] = temp\n",
    "                    if model in GPT_REASONING_MODELS:\n",
    "                        body['reasoning_effort'] = 'low'\n",
    "\n",
    "                    entries.append({\n",
    "                        'custom_id': custom_id,\n",
    "                        'method': 'POST',\n",
    "                        'url': '/v1/chat/completions',\n",
    "                        'body': body\n",
    "                    })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        model_safe = model.replace('/', '-')\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"02_openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(LLM_API, batch_fname)\n",
    "\n",
    "        with open(batch_path, 'w', encoding='utf-8') as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {batch_path} (lines {start + 1}–{end})\")"
   ],
   "id": "c9640a9cd558565b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Fine-Tuned Models\n",
    "Note: Before running this code, run notebook 3-2 to generate fine-tuned models via fine-tuning API."
   ],
   "id": "2e13173564d403ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read every non-empty line as a model identifier\n",
    "with open(GPT_FT_MODELS_PATH, encoding=\"utf-8\") as f:\n",
    "    FT_MODELS = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Process for each fine-tuned model\n",
    "for ft_model in FT_MODELS:\n",
    "    # Parse out the \"type–size\" token (e.g. \"3-5-turbo-equal-2000\")\n",
    "    parts = ft_model.split(\":\")\n",
    "    type_size = parts[3] if len(parts) > 3 else \"\"\n",
    "    is_var_model = \"2000\" in type_size\n",
    "\n",
    "    # Initialize container for model's entries\n",
    "    entries = []\n",
    "\n",
    "    if is_var_model:\n",
    "        # Iterate over every prompt template\n",
    "        for prompt_name, prompt_template in PROMPTS.items():\n",
    "            # Only the \"default\" prompt gets a temperature sweep\n",
    "            temps = TEMPERATURE_RANGE if prompt_name == \"default\" else [None]\n",
    "            for temp in temps:\n",
    "                for run in range(1, REPEATED_RUNS + 1):\n",
    "                    for _, row in df.iterrows():\n",
    "                        # Compose user prompt\n",
    "                        review_text = (row.get(\"review_text_tagged\", \"\") or \"\").strip()\n",
    "                        prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "                        # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                        custom_id = (\n",
    "                            f\"{ft_model.replace('/', '-')}\"\n",
    "                            f\"__{prompt_name}\"\n",
    "                            f\"__t{int((temp or 0) * 10)}\"\n",
    "                            f\"__run{run}\"\n",
    "                            f\"__review_id{row['review_id']}\"\n",
    "                        )\n",
    "                        body = {\n",
    "                            \"model\": ft_model,\n",
    "                            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                            \"max_completion_tokens\": 1000,\n",
    "                            \"seed\": SEED,\n",
    "                        }\n",
    "                        if temp is not None:\n",
    "                            body[\"temperature\"] = temp\n",
    "                        entries.append({\n",
    "                            \"custom_id\": custom_id,\n",
    "                            \"method\": \"POST\",\n",
    "                            \"url\": \"/v1/chat/completions\",\n",
    "                            \"body\": body\n",
    "                        })\n",
    "    else:\n",
    "        # Only 3 runs, default prompt, no temp variation\n",
    "        prompt_template = PROMPTS[\"default\"]\n",
    "        for run in range(1, REPEATED_RUNS + 1):\n",
    "            for _, row in df.iterrows():\n",
    "                # Compose user prompt\n",
    "                review_text = (row.get(\"review_text_tagged\", \"\") or \"\").strip()\n",
    "                prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "                # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                custom_id = (\n",
    "                    f\"{ft_model.replace('/', '-')}\"\n",
    "                    f\"__default\"\n",
    "                    f\"__t0\"\n",
    "                    f\"__run{run}\"\n",
    "                    f\"__review_id{row['review_id']}\"\n",
    "                )\n",
    "                body = {\n",
    "                    \"model\": ft_model,\n",
    "                    \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                    \"max_completion_tokens\": 1000,\n",
    "                    \"seed\": SEED,\n",
    "                }\n",
    "                entries.append({\n",
    "                    \"custom_id\": custom_id,\n",
    "                    \"method\": \"POST\",\n",
    "                    \"url\": \"/v1/chat/completions\",\n",
    "                    \"body\": body\n",
    "                })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "    # Sanitize model name for filenames\n",
    "    model_safe = re.sub(r'[:/\\\\\\s]+', '-', ft_model)\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        batch_fname = f\"02_openai_batch_{model_safe}_{suffix}.jsonl\"\n",
    "        batch_path = os.path.join(LLM_API, batch_fname)\n",
    "\n",
    "        with open(batch_path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {ft_model} to {batch_path}\"\n",
    "              f\" (lines {start + 1}–{end})\")"
   ],
   "id": "29eba8e906dd0bb3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to OpenAI and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "2509eda64c727370"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(LLM_API, \"02_openai_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded_file_ids = []\n",
    "for path in batch_paths:\n",
    "    print(f\"Uploading {path}...\")\n",
    "    with open(path, 'rb') as f:\n",
    "        file_resp = OPENAI_CLIENT.files.create(\n",
    "            file=f,\n",
    "            purpose='batch'\n",
    "        )\n",
    "    print(f\"Uploaded: id={file_resp.id}, filename={file_resp.filename}, bytes={file_resp.bytes}\")\n",
    "    uploaded_file_ids.append(file_resp.id)\n",
    "\n",
    "print(\"All batch files uploaded.\\n\")"
   ],
   "id": "cb5a348208f56347",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a 24 h chat-completion batch job for each uploaded file"
   ],
   "id": "ecbcf57f7ee3abd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create a Batch for each uploaded file\n",
    "for file_id in uploaded_file_ids:\n",
    "    batch = OPENAI_CLIENT.batches.create(\n",
    "        input_file_id=file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"eval job for {file_id}\"\n",
    "        }\n",
    "    )\n",
    "    print(f\"Created batch {batch.id} for {file_id}, status: {batch.status}\")"
   ],
   "id": "e38e61616ea89440",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Mistral Batches\n",
    "We generate JSONL batch files for default and fine-tuned Mistral models, upload them, and create batch-jobs via the Mistral API."
   ],
   "id": "a8a09d5b57a8bf43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Creation\n",
    "We build one JSONL batch per Mistral model. Run either Default or Fine-Tuned Models."
   ],
   "id": "ff0280ad83091447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Default Models",
   "id": "e94a56337b86fc78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in MISTRAL_MODELS:\n",
    "    entries = []\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        # Only the \"default\" prompt gets a temperature sweep\n",
    "        temps = TEMPERATURE_RANGE if prompt_name == 'default' else [None]\n",
    "        for temp in temps:\n",
    "            for run in range(1, REPEATED_RUNS + 1):\n",
    "                for _, row in df.iterrows():\n",
    "                    # Compose user prompt\n",
    "                    review_text = (row.get('review_text_tagged', '') or '').strip()\n",
    "                    prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "                    # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                    custom_id = (\n",
    "                        f\"{model}__{prompt_name}\"\n",
    "                        f\"__t{int((temp or 0) * 10)}__run{run}__review_id{row['review_id']}\"\n",
    "                    )\n",
    "                    body = {\n",
    "                        \"max_tokens\": 1000,\n",
    "                        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                        **({\"temperature\": temp} if temp is not None else {}),\n",
    "                        \"random_seed\": SEED,\n",
    "                    }\n",
    "                    entries.append({\n",
    "                        \"custom_id\": custom_id,\n",
    "                        \"body\": body\n",
    "                    })\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "\n",
    "    for part in range(num_files):\n",
    "        start = part * MAX_LINES_PER_BATCH\n",
    "        end = min(start + MAX_LINES_PER_BATCH, total)\n",
    "        batch_lines = entries[start:end]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        suffix = f\"{part + 1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"02_mistral_batch_{model.replace('/', '-')}_{suffix}.jsonl\"\n",
    "        path = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, 'w', encoding='utf-8') as fout:\n",
    "            for entry in batch_lines:\n",
    "                fout.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "        print(f\"Wrote {len(batch_lines)} entries for {model} to {path}\")"
   ],
   "id": "ca5970ed467acea4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read every non-empty line as a model identifier\n",
    "with open(MISTRAL_FT_MODELS_PATH, encoding=\"utf-8\") as f:\n",
    "    raw_ft = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Process for each fine-tuned model\n",
    "for ft_full in raw_ft:\n",
    "     # Parse out the model components\n",
    "    parts = ft_full.split(\":\")\n",
    "    if len(parts) < 5:\n",
    "        raise ValueError(f\"Bad line in MISTRAL_fine-tuned.txt: {ft_full}\")\n",
    "    model_id = \":\".join(parts[:4])\n",
    "    suffix   = parts[4]\n",
    "    # Sanitize model name for filenames\n",
    "    safe_ft  = re.sub(r\"[:/\\\\\\s]+\", \"-\", ft_full)\n",
    "\n",
    "    # Initialize container for model's entries\n",
    "    entries = []\n",
    "    is_var  = \"real_2000\" in suffix\n",
    "\n",
    "    if is_var:\n",
    "         # Iterate over every prompt template\n",
    "        for prompt_name, prompt_template in PROMPTS.items():\n",
    "            # Only the \"default\" prompt gets a temperature sweep\n",
    "            temps = TEMPERATURE_RANGE if prompt_name == \"default\" else [None]\n",
    "            for temp in temps:\n",
    "                for run in range(1, REPEATED_RUNS + 1):\n",
    "                    for _, row in df.iterrows():\n",
    "                        # Compose user prompt\n",
    "                        review_text = (row.get('review_text_tagged', '') or '').strip()\n",
    "                        prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "                        # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                        custom_id = (\n",
    "                            f\"{safe_ft}\"\n",
    "                            f\"__{prompt_name}\"\n",
    "                            f\"__t{int((temp or 0)*10)}\"\n",
    "                            f\"__run{run}\"\n",
    "                            f\"__review_id{row.review_id}\"\n",
    "                        )\n",
    "                        body = {\n",
    "                            \"max_tokens\":   1000,\n",
    "                            \"messages\":    [{\"role\":\"user\",\"content\":prompt}],\n",
    "                            \"random_seed\": SEED,\n",
    "                            **({\"temperature\": temp} if temp is not None else {})\n",
    "                        }\n",
    "                        entries.append({\"custom_id\": custom_id, \"body\": body})\n",
    "\n",
    "    else:\n",
    "        # Only 3 runs, default prompt, no temp variation\n",
    "        template = PROMPTS[\"default\"]\n",
    "        for run in range(1, REPEATED_RUNS + 1):\n",
    "            for _, row in df.iterrows():\n",
    "                # Compose user prompt\n",
    "                review_text = (row.get('review_text_tagged', '') or '').strip()\n",
    "                prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "                # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                custom_id = (\n",
    "                    f\"{safe_ft}\"\n",
    "                    \"__default\"\n",
    "                    f\"__t0\"\n",
    "                    f\"__run{run}\"\n",
    "                    f\"__review_id{row.review_id}\"\n",
    "                )\n",
    "                body = {\n",
    "                    \"max_tokens\":   1000,\n",
    "                    \"messages\":    [{\"role\":\"user\",\"content\":prompt}],\n",
    "                    \"random_seed\": SEED\n",
    "                }\n",
    "                entries.append({\"custom_id\": custom_id, \"body\": body})\n",
    "\n",
    "    # Paginate entries to ≤ 50,000 lines per file\n",
    "    total     = len(entries)\n",
    "    num_files = math.ceil(total / MAX_LINES_PER_BATCH)\n",
    "    for part in range(num_files):\n",
    "        chunk = entries[part*MAX_LINES_PER_BATCH:(part+1)*MAX_LINES_PER_BATCH]\n",
    "\n",
    "        # Sanitize model name for filenames\n",
    "        idx   = f\"{part+1:02d}\" if num_files > 1 else \"01\"\n",
    "        fname = f\"02_mistral_batch_{safe_ft}_{idx}.jsonl\"\n",
    "        path  = os.path.join(MISTRAL_BATCH_DIR, fname)\n",
    "\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as fout:\n",
    "            for e in chunk:\n",
    "                fout.write(json.dumps(e, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Wrote {len(chunk)} entries for {ft_full} → {path}\")"
   ],
   "id": "e24d33b972a7c2e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Upload Batches\n",
    "We upload every JSONL batch to Mistral and capture file IDs. Run either batch creation for default models or fine-tuned models before."
   ],
   "id": "8be7d84213aeb213"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Path to batch files\n",
    "batch_paths = sorted(glob.glob(os.path.join(MISTRAL_BATCH_DIR, \"02_mistral_batch_*.jsonl\")))\n",
    "\n",
    "# Upload each batch file and collect its file ID\n",
    "uploaded = []\n",
    "for path in batch_paths:\n",
    "    fname = os.path.basename(path)\n",
    "    model = fname.split(\"_\")[3]\n",
    "\n",
    "    print(f\"Uploading {fname} (model={model})…\")\n",
    "    with open(path, 'rb') as f:\n",
    "        up = MISTRAL_CLIENT.files.upload(\n",
    "            file={\"file_name\": fname, \"content\": f},\n",
    "            purpose=\"batch\"\n",
    "        )\n",
    "    print(f\"Uploaded: id={up.id}\")\n",
    "    uploaded.append({\"model\": model, \"file_id\": up.id})\n",
    "\n",
    "print(\"All Mistral batch files uploaded.\\n\")"
   ],
   "id": "e3a1aec05ffaddbd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Batch Jobs\n",
    "We create a batch job for each uploaded file."
   ],
   "id": "3311b674c2bc7d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for item in uploaded:\n",
    "    file_id = item[\"file_id\"]\n",
    "    model = item[\"model\"]\n",
    "\n",
    "    # Parse model name for fine-tuned models\n",
    "    if model.startswith(\"ft-\"):\n",
    "        parts = model.split(\"-\") # parts = [\"ft\", \"mistral\", \"small\", \"latest\", \"d1ef7e20\", \"20250527\", \"314de464\", \"equal_500\"]\n",
    "\n",
    "        prefix     = parts[0]                        # \"ft\"\n",
    "        model_name = \"-\".join(parts[1:4])            # \"mistral-small-latest\"\n",
    "        rev        = parts[4]                        # \"d1ef7e20\"\n",
    "        date       = parts[5]                        # \"20250527\"\n",
    "        sha        = parts[6]                        # \"314de464\"\n",
    "\n",
    "        real_model = f\"{prefix}:{model_name}:{rev}:{date}:{sha}\"\n",
    "        # \"ft:mistral-small-latest:d1ef7e20:20250527:314de464\"\n",
    "    else:\n",
    "        real_model = model\n",
    "\n",
    "    # Submit batch job to Mistral API\n",
    "    job = MISTRAL_CLIENT.batch.jobs.create(\n",
    "        input_files=[file_id],\n",
    "        model=real_model,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        metadata={\"description\": f\"classification batch for {real_model}\"}\n",
    "    )\n",
    "    print(f\"Created batch job {job.id} for file {file_id} (model={real_model}), status={job.status}\")"
   ],
   "id": "57d40fa23299dfb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Anthropic Batches\n",
    "We create batch requests for Claude models and process JSONL results into CSV format."
   ],
   "id": "c434387bb42d2e80"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Batch Requests\n",
    "We build batch requests for each Claude model with various prompts and temperature settings."
   ],
   "id": "a37637b387b43779"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Shorten model names as custom_id max is 64 characters\n",
    "def short_model_name(full_model: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts full Claude model names to shortened versions for custom_id usage.\n",
    "    # e.g. \"claude-3-7-sonnet-20250219\" → [\"claude\",\"3\",\"7\",\"sonnet\",\"20250219\"]\n",
    "    \"\"\"\n",
    "    parts = full_model.split(\"-\")\n",
    "    # Drop the first (\"claude\") and last (date) parts, then re-join\n",
    "    return \"-\".join(parts[1:-1])"
   ],
   "id": "72513c6c6f273617",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Process for each model\n",
    "for model in CLAUDE_MODELS:\n",
    "    short_model = short_model_name(model)\n",
    "    batch_requests = []\n",
    "\n",
    "    # Iterate through all prompt templates\n",
    "    for prompt_name, prompt_template in PROMPTS.items():\n",
    "        # Only the \"default\" prompt gets a temperature sweep\n",
    "        base_temps = TEMPERATURE_RANGE if prompt_name == \"default\" else [None]\n",
    "        temps = [t for t in base_temps if t is None or t <= 1.0]\n",
    "\n",
    "        for temp in temps:\n",
    "            for run in range(1, REPEATED_RUNS + 1):\n",
    "                for _, row in df.iterrows():\n",
    "                    # Compose user prompt\n",
    "                    review_text = (row.get(\"review_text_tagged\", \"\") or \"\").strip()\n",
    "                    prompt = f\"{prompt_template}\\n\\nApp review text: {review_text}\"\n",
    "\n",
    "                    # Unique ID encodes model/prompt/temp/run/row.id\n",
    "                    raw_id = (\n",
    "                        f\"{short_model}_{prompt_name}\"\n",
    "                        f\"_t{int((temp or 0) * 10)}_r{run}_review_i{row['review_id']}\"\n",
    "                    )\n",
    "                    # Ensure ≤64 chars for API limits (redundancy)\n",
    "                    custom_id = (\n",
    "                        raw_id[:64]\n",
    "                        if len(raw_id) <= 64\n",
    "                        else raw_id[:50]\n",
    "                    )\n",
    "\n",
    "                    # Build request parameters\n",
    "                    params = MessageCreateParamsNonStreaming(\n",
    "                        model=model,\n",
    "                        max_tokens=1000,\n",
    "                        temperature=(temp or 0),\n",
    "                        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    )\n",
    "                    batch_requests.append(Request(custom_id=custom_id, params=params))\n",
    "\n",
    "    # Submit the complete batch for model\n",
    "    message_batch = ANTHROPIC_CLIENT.messages.batches.create(\n",
    "        requests=batch_requests\n",
    "    )\n",
    "    print(\n",
    "        f\"Created batch for {model} \"\n",
    "        f\"with {len(batch_requests)} requests: \"\n",
    "        f\"{message_batch.id} (status={message_batch.processing_status})\"\n",
    "    )"
   ],
   "id": "d7f0f8b8b516af0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Process results\n",
    "We process JSONL result files and convert them to CSV format for analysis. Note: Download batch files from each provider's developer platform before."
   ],
   "id": "9057656827634055"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load validation dataset and prepare ID columns for matching with batch results\n",
    "df = pd.read_csv(VAL_PATH, dtype={'review_id': str})\n",
    "df['review_id'] = df['review_id'].astype(str)\n",
    "df['id'] = df['id'].astype(str)"
   ],
   "id": "80455e4a9ccf4fc8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# List of batch IDs with a special case (see extra handling below)\n",
    "special_batches = [\n",
    "    \"650d5d12-19f1-42df-b41d-a364f276ed9c\",\n",
    "    \"85a4baa4-e1ac-4685-abed-18a6485668cd\",\n",
    "    \"1fa3043f-54c6-4589-92fa-5964133f1ee1\",\n",
    "    \"4de6b936-a554-4104-b66b-642a6e1d4132\",\n",
    "    \"5a63b46c-5a21-43bc-8a59-b3da30657d6b\",\n",
    "    \"1b01c69d-d09e-4473-8b33-a2fd9aa239c8\"\n",
    "]"
   ],
   "id": "9b66086516e186ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Helper Functions\n",
    "Utility functions for ID normalization and record parsing across different LLM providers."
   ],
   "id": "ca0fe059b14033f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Helper: normalize ID string by removing 'id' prefix",
   "id": "6fad967e16c4e168"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_id_str(id_part):\n",
    "    \"\"\"Clean ID strings by removing 'id' prefix and whitespace\"\"\"\n",
    "    return id_part.replace('review_id', '').strip()"
   ],
   "id": "8bf0319443a1dd5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Provider-Specific Parsers\n",
    "Each LLM provider has different JSONL response formats requiring specialized parsing."
   ],
   "id": "6eaaeebae4d8ed55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def parse_openai_record(record):\n",
    "    \"\"\"Parse OpenAI batch response record\"\"\"\n",
    "    # Split custom_id into components\n",
    "    custom_id = record.get('custom_id', '')\n",
    "    parts     = custom_id.split('__')\n",
    "\n",
    "    if custom_id.startswith(\"02__\"):\n",
    "        if len(parts) < 6:\n",
    "            return None\n",
    "        model, prompt_type, temp, run, id_part = parts[-5:]\n",
    "    else:\n",
    "        if len(parts) < 5:\n",
    "            return None\n",
    "        model, prompt_type, temp, run, id_part = parts[:5]\n",
    "    id_str = normalize_id_str(id_part)\n",
    "    orig_id = record.get('id', '')\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    try:\n",
    "        content = record['response']['body']['choices'][0]['message']['content']\n",
    "    except Exception:\n",
    "        return None\n",
    "    return id_str, orig_id, col, content\n",
    "\n",
    "def parse_mistral_record(record):\n",
    "    \"\"\"Parse Mistral batch response record\"\"\"\n",
    "    # Same as parse_openai_record\n",
    "    return parse_openai_record(record)\n",
    "\n",
    "def parse_anthropic_record(record):\n",
    "    \"\"\"Parse Anthropic batch response record\"\"\"\n",
    "    # Split custom_id into components (single underscore separator)\n",
    "    custom_id = record.get('custom_id', '')\n",
    "    parts     = custom_id.split('_')\n",
    "\n",
    "    if len(parts) < 5:\n",
    "        return None\n",
    "\n",
    "    # Extract components from different positions due to Anthropic format\n",
    "    id_part = parts[-1]\n",
    "    run = parts[-2]\n",
    "    temp = parts[-3]\n",
    "    model = parts[0]\n",
    "    prompt_type = '_'.join(parts[1:-3])\n",
    "    id_str = normalize_id_str(id_part.lstrip('review_i'))\n",
    "    orig_id = record.get('id', '')\n",
    "\n",
    "    # Create standardized column name\n",
    "    col = f\"{model}__{prompt_type}__{temp}__{run}\".replace(':','_').replace('-','_').replace('.','_')\n",
    "\n",
    "    # Extract response content\n",
    "    content = ''\n",
    "    try:\n",
    "        content = record.get('result', {}).get('message', {})['content'][0]['text']\n",
    "    except Exception:\n",
    "        # If content missing or malformed, leave empty\n",
    "        pass\n",
    "    return id_str, orig_id, col, content\n",
    "\n",
    "def process_file(path, parser):\n",
    "    \"\"\"Process a single JSONL file using the specified parser\"\"\"\n",
    "    mapping = {}\n",
    "    batch_is_special = any(batch in str(path) for batch in special_batches)\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed JSON lines\n",
    "                continue\n",
    "            # Parse record using provider-specific parser\n",
    "            parsed = parser(rec)\n",
    "            if parsed:\n",
    "                id_str, orig_id, col, content = parsed\n",
    "                if col not in mapping:\n",
    "                    mapping[col] = {}\n",
    "                # Group by column, then by ID\n",
    "                mapping[col][id_str] = (content, orig_id)\n",
    "    return mapping, batch_is_special\n",
    "\n",
    "def process_results_parallel(directory, parser, df, workers=8):\n",
    "    \"\"\"Process all JSONL files in a directory using parallel execution.\"\"\"\n",
    "    # Find all JSONL files in directory and subdirectories\n",
    "    files = list(Path(directory).rglob('*.jsonl'))\n",
    "\n",
    "    combined = {}\n",
    "    special_batch_cols = set()\n",
    "\n",
    "    # Process files in parallel\n",
    "    with ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        # Submit all file processing tasks\n",
    "        futures = {executor.submit(process_file, f, parser): f for f in files}\n",
    "\n",
    "        # Collect results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            file_map, batch_is_special = future.result()\n",
    "             # Merge results from each file\n",
    "            for col, id_map in file_map.items():\n",
    "                if col not in combined:\n",
    "                    combined[col] = {}\n",
    "                combined[col].update(id_map)\n",
    "                if batch_is_special:\n",
    "                    special_batch_cols.add(col)\n",
    "\n",
    "    # Add new columns to dataframe\n",
    "    for col, id_map in combined.items():\n",
    "        def lookup(row):\n",
    "            row_id = str(row['id']).strip()\n",
    "            row_review_id = str(row['review_id']).strip()\n",
    "            # For special batches: match only by id (which is digits, same as id_str)\n",
    "            if col in special_batch_cols:\n",
    "                if row_id in id_map:\n",
    "                    return id_map[row_id][0]\n",
    "                else:\n",
    "                    return ''\n",
    "            # For regular: try review_id, then id\n",
    "            if row_review_id in id_map:\n",
    "                return id_map[row_review_id][0]\n",
    "            if row_id in id_map:\n",
    "                return id_map[row_id][0]\n",
    "            return ''\n",
    "        df[col] = df.apply(lookup, axis=1)\n",
    "    return df.copy()\n"
   ],
   "id": "281215772d4ea7c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Process All Provider Results\n",
    "Execute processing for each LLM provider in sequence."
   ],
   "id": "6a9cfc7b5d681508"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = process_results_parallel(OPENAI_BATCH_RESULTS_DIR, parse_openai_record, df)\n",
    "df = process_results_parallel(MISTRAL_BATCH_RESULTS_DIR, parse_mistral_record, df)\n",
    "df = process_results_parallel(ANTHROPIC_BATCH_DIR, parse_anthropic_record, df)"
   ],
   "id": "b4d05371150991d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save final output\n",
    "df.to_csv(os.path.join(DEMO_PATH, 'output_data','validation_with_model_preds_LLM.csv'), index=False)"
   ],
   "id": "b02481924e211f74",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean Results\n",
    "Standardize raw model outputs into consistent multi-label strings.\n",
    "This includes string cleanup, numeric type handling, and fallback for invalid predictions."
   ],
   "id": "6f85736880b4e3f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning Prep and Helper Function",
   "id": "527b0af345bbff8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Columns to Exclude from Cleaning\n",
    "EXCLUDE_COLS = [\n",
    "    'review_id', 'user_id', 'title', 'body', 'label', 'id', 'app_id',\n",
    "    'review_text_plain', 'review_text_tagged', 'split_labels','sorted_labels','multi_hot'\n",
    "]\n",
    "\n",
    "def extract_pred_group(text):\n",
    "    \"\"\"\n",
    "    Normalize prediction output into semicolon-separated label string.\n",
    "    Returns '9' if no valid label is found (fallback class).\n",
    "\n",
    "    - Accepts int, float, or string input.\n",
    "    - Parses predictions like '2;5', '(3 ; 6)', etc.\n",
    "    - Ignores non-digit content or out-of-range values (labels ∈ [0, 8]).\n",
    "    \"\"\"\n",
    "    # Handle numeric types first\n",
    "    if isinstance(text, float):\n",
    "        # If it's NaN, treat as no valid label\n",
    "        if pd.isna(text):\n",
    "            return '9'\n",
    "        # If it's a float that represents an int (e.g. 2.0), convert to int string\n",
    "        if text.is_integer() and 0 <= int(text) <= 8:\n",
    "            return str(int(text))\n",
    "        # If not in range or not an int, treat as no valid label\n",
    "        return '9'\n",
    "    if isinstance(text, int):\n",
    "        if 0 <= text <= 8:\n",
    "            return str(text)\n",
    "        return '9'\n",
    "\n",
    "    # Handle string or other types\n",
    "    text = str(text)\n",
    "    pattern = r'\\(?\\s*([0-8](?:\\s*;\\s*[0-8])*)\\s*\\)?'\n",
    "    matches = re.findall(pattern, text)\n",
    "    if not matches:\n",
    "        return '9'\n",
    "    last = matches[-1]\n",
    "    cleaned_digits = [d for d in re.split(r'\\s*;\\s*', last.strip('; ')) if d.isdigit() and 0 <= int(d) <= 8]\n",
    "    if not cleaned_digits:\n",
    "        return '9'\n",
    "    return ';'.join(cleaned_digits)"
   ],
   "id": "b97981e48e91f1af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load Model Output File and Apply Cleaning",
   "id": "38397bb02fa7a2d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load CSV\n",
    "df = pd.read_csv(os.path.join(DEMO_PATH, 'output_data', 'validation_with_model_preds_LLM.csv'))"
   ],
   "id": "2dc67f8ad631928b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Identify prediction columns (exclude metadata & raw input)\n",
    "model_cols = [c for c in df.columns if c not in EXCLUDE_COLS]\n",
    "\n",
    "# Apply standardization function to all model columns\n",
    "for col in model_cols:\n",
    "    df[col] = df[col].apply(extract_pred_group)\n",
    "\n",
    "# Save cleaned results\n",
    "save_path = os.path.join(DEMO_PATH, 'validation_with_model_preds_LLM_cleaned.csv')\n",
    "df.to_csv(save_path, index=False)\n",
    "print(f\"Saved cleaned file to {save_path}\")"
   ],
   "id": "53f441aec320e11",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
